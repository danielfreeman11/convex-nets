\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:Intro}{{1}{2}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Topology of Level Sets}{2}{section.2}}
\newlabel{emp_risk_min}{{1}{2}{Topology of Level Sets}{equation.2.1}{}}
\newlabel{risk_min}{{2}{2}{Topology of Level Sets}{equation.2.2}{}}
\citation{linearcase}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Linear Case}{3}{subsection.2.1}}
\newlabel{linearcase}{{4}{3}{The Linear Case}{equation.2.4}{}}
\newlabel{proplinear}{{2.2}{3}{}{theorem.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Half-Rectified Nonlinear Case}{4}{subsection.2.2}}
\newlabel{relucase}{{5}{4}{Half-Rectified Nonlinear Case}{equation.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Geometry of Level Sets}{4}{section.3}}
\newlabel{sec:QuanNoncon}{{3}{4}{Geometry of Level Sets}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Greedy Algorithm}{4}{subsection.3.1}}
\newlabel{sec:GreedyAlg}{{3.1}{4}{The Greedy Algorithm}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A cartoon of the algorithm. $a):$ The initial two models with approximately the same loss, $L_0$. $b):$ The interpolated loss curve, in red, and its global maximum, occuring at $t=t^*$. $c):$ The interpolated model $\Theta (\theta _i, \theta _j, t^*)$ is added and labeled $\theta _{i,j}$. $d):$ Stochastic gradient descent is performed on the interpolated model until its loss is below $\alpha L_0$. $e):$ New interpolated loss curves are calculated between the models, pairwise on a chain. $f):$ As in step $c)$, a new model is inserted at the maxima of the interpolated loss curve between $\theta _i$ and $\theta _{i,j}$. $g):$ As in step $d)$, gradient descent is performed until the model has low enough loss.}}{5}{figure.1}}
\newlabel{fig:AlgorithmFigure}{{1}{5}{A cartoon of the algorithm. $a):$ The initial two models with approximately the same loss, $L_0$. $b):$ The interpolated loss curve, in red, and its global maximum, occuring at $t=t^*$. $c):$ The interpolated model $\Theta (\theta _i, \theta _j, t^*)$ is added and labeled $\theta _{i,j}$. $d):$ Stochastic gradient descent is performed on the interpolated model until its loss is below $\alpha L_0$. $e):$ New interpolated loss curves are calculated between the models, pairwise on a chain. $f):$ As in step $c)$, a new model is inserted at the maxima of the interpolated loss curve between $\theta _i$ and $\theta _{i,j}$. $g):$ As in step $d)$, gradient descent is performed until the model has low enough loss}{figure.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Greedy Dynamic String Sampling}}{6}{algorithm.1}}
\newlabel{euclid}{{1}{6}{The Greedy Algorithm}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Failure Conditions and Practicalities}{6}{subsection.3.2}}
\newlabel{sec:Fail}{{3.2}{6}{Failure Conditions and Practicalities}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{6}{section.4}}
\newlabel{sec:NumExp}{{4}{6}{Numerical Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Polynomial Regression}{6}{subsection.4.1}}
\newlabel{sec:PolyFuncs}{{4.1}{6}{Polynomial Regression}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Convolutional Neural Networks}{7}{subsection.4.2}}
\newlabel{sec:CNN}{{4.2}{7}{Convolutional Neural Networks}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{7}{section.5}}
\newlabel{sec:Discussion}{{5}{7}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Constrained Dynamic String Sampling}{7}{section.6}}
\newlabel{sec:ConstrainedAlg}{{6}{7}{Constrained Dynamic String Sampling}{section.6}{}}
\bibdata{iclr2017_conference}
\bibstyle{iclr2017_conference}
