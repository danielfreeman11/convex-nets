\section{Constrained Dynamic String Sampling}
  \label{sec:ConstrainedAlg}
  
  While the algorithm presented in Sec. \ref{sec:GreedyAlg} is fast for sufficiently smooth families of loss surfaces with few saddle points, here we present a slightly modified version which, while slower, provides more control over the convergence of the string.  We did not use the algorithm presented in this section for our numerical studies.  
  
  Instead of training intermediate models via full SGD to a desired accuracy as in step $8$ of the algorithm, intermediate models are be subject to a constraint that ensures they are ``close'' to the neighboring models on the string.  Specifically, intermediate models are constrained to the unique hyperplane in weightspace equidistant from its two neighbors.  This can be further modified by additional regularization terms to control the ``springy-ness'' of the string.  These heuristics could be chosen to try to more faithfully sample the geodesic between two models.  
  
  In practice, for a given model on the string, $\theta_i$, these two regularizations augment the standard loss by: $\tilde{F}(\theta) = F(\theta)+\zeta(\|\theta_{i-1} - \theta_i\|+\|\theta_{i+1} - \theta_i\|) + \kappa \|\frac{(\theta_{i-1} - \theta_{i+1})/2}{\|(\theta_{i-1} - \theta_{i+1})/2\|} \cdot \frac{(\theta_i - (\theta_{i-1} - \theta_{i+1})/2)}{\| (\theta_i - (\theta_{i-1} - \theta_{i+1})/2)\|}\|$.  The $\zeta$ regularization term controls the ``springy-ness'' of the weightstring, and the $\kappa$ regularization term controls how far off the hyperplane a new model can deviate.  
  
  Because adapting DSS to use this constraint is straightforward, here we will describe an alternative ``breadth-first'' approach wherein models are trained in parallel until convergence.  This alternative approach has the advantage that it will indicate a disconnection between two models ``sooner'' in training.  The precise geometry of the loss surface will dictate which approach to use in practice.
  
  Given two random models $\sigma_i$ and $\sigma_j$ where $|\sigma_i - \sigma_j| < L_0$, we aim to follow the evolution of the family of models connecting $\sigma_i$ to $\sigma_j$.  Intuitively, almost every continuous path in the space of random models connecting $\sigma_i$ to $\sigma_j$ has, on average, the same (high) loss.  For simplicity, we choose to initialize the string to the linear segment interpolating between these two models.  If this entire segment is evolved via gradient descent, the segment will either evolve into a string which is entirely contained in a basin of the loss surface, or some number of points will become fixed at a higher loss.  These fixed points are difficult to detect directly, but will be indirectly detected by the persistence of a large interpolated loss between two adjacent models on the string.
  
  The algorithm proceeds as follows:
  
  (0.) Initialize model string to have two models, $\sigma_i$ and $\sigma_j$.
  
  1. Begin training all models to the desired loss, keeping the instantaneous loss, $L_0(t)$, of all models being trained approximately constant.
  
  2. If the pairwise interpolated loss between $\sigma_n$ and $\sigma_{n+1}$ exceeds $L_0(t)$, insert a new model at the maximum of the interpolated loss (or halfway) between these two models.
  
  3. Repeat steps (1) and (2) until all models (and interpolated errors) are below a threshold loss $L_0(t_{\rm{final}}):=L_0$, or until a chosen failure condition (see \ref{sec:Fail}).


