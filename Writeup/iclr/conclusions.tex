%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:Discussion}
%%%%%%%%%%%%%%%%%%%%%%

We have addressed the problem of characterizing the loss surface of neural networks from the perspective
of gradient descent algorithms. We explored two angles -- topological and geometrical aspects -- that build on top of each other. 


On the one hand, we have presented new theoretical results that quantify 
the amount of uphill climbing that is required in order to progress to lower energy configurations in 
single hidden-layer ReLU networks, and proved that this amount converges to zero with overparametrization under mild conditions. On the other hand, we have introduced a dynamic programming algorithm that efficiently approximates geodesics within each level set, providing a tool that not only verifies the connectedness of level sets, but also estimates the geometric regularity of these sets. Thanks to this information, we can quantify how `non-convex' an optimization problem is, and verify that the optimization of quintessential deep learning tasks -- CIFAR-10 and MNIST classification using CNNs, and next word prediction using LSTMs -- behaves in a nearly convex fashion up until they reach high accuracy levels.

That said, there are some limitations to our framework. In particular, we do not address saddle-point issues that can greatly affect the actual convergence of gradient descent methods. There are also a number of open questions; amongst those, in the near future we shall concentrate on:
\begin{itemize}
\item \emph{Extending Theorem \ref{maintheo} to the multilayer case}. We believe this is within reach, since the main analytic tool we use is that small changes in the parameters result in small changes in the covariance structure of the features. That remains the case in the multilayer case. 
\item \emph{Empirical versus Oracle Risk}. A big limitation of our theory is that right now it does not inform us on the differences between optimizing the empirical risk versus the oracle risk. Understanding the impact of generalization error and stochastic gradient in the ability to do small uphill climbs is an open line of research.
\item \emph{Influence of symmetry groups}.  Under appropriate conditions, the presence of discrete symmetry groups does not prevent the loss from being connected, but at the expense of increasing the capacity. An important open question is whether one can improve the asymptotic properties by relaxing connectedness to being connected up to discrete symmetry. 
\item \emph{Improving numerics with Hyperplane method}. Our current numerical experiments employ a greedy (albeit faster) algorithm to discover connected components and estimate geodesics. We plan to perform experiments using the less greedy algorithm described in Appendix \ref{sec:ConstrainedAlg}. 
\end{itemize}
 

%\begin{itemize}
%\item Future: Generalization Error Question. 
%
%\end{itemize}
