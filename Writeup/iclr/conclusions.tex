%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:Discussion}
%%%%%%%%%%%%%%%%%%%%%%

We have addressed the problem of characterizing the loss surface of neural networks from the perspective
on gradient descent algorithms from two angles that build on top of each other. 

On the one hand, we have presented new theoretical results that quantify 
the amount of uphill climbing that is required to progress to lower energy configurations in 
single layer ReLU networks, and proved that this amount converges to zero with overparametrization under mild conditions. On the other hand, we have introduced a dynamic programming algorithm that efficiently approximates geodesics within each level set, providing a tool that not only verifies the connectedness of level sets, but also estimates the geometric regularity of these sets. Thanks to this information, we can quantify how `non-convex' an optimization problem is, and verify that the optimization of quintessential deep learning tasks -- Cifar-10 classification using CNNs, character-level text modeling using LSTMs -- behaves in a nearly convex fashion up until they reach high accuracy levels.

That said, there is still a large number of limitations and open questions related to our framework. Amongst those, in the near future we shall concentrate on:
\begin{itemize}
\item \emph{Extending Theorem \ref{maintheo} to the multilayer case}. We believe this is within reach, since the main analytic tool we use is that small changes in the parameters result in small changes in the covariance structure of the features. That remains the case in the multilayer case. 
\item \emph{Empirical versus Oracle Risk}. A big limitation of our theory is that right now it does not inform us on the differences between optimizing the empirical risk versus the oracle risk. Understanding the impact of generalization error and stochastic gradient in the ability to do small uphill climbs is a major open line of research.
\item \emph{Influence of symmetry groups}. Our current model shows that under appropriate conditions, the presence of discrete symmetry groups does not prevent the loss from being connected, but does so at the expense of increasing the capacity. An important open question is whether one can significantly improve the asymptotic properties by relaxing connectedness to being connected up to discrete symmetry. 
\item \emph{Improving numerics with Hyperplane method}. Our current numerical experiments employ a greedy (albeit faster) algorithm to discover connected compoenents and estimate geodesics. We will update experiments using the less greedy algorithm described in Appendix \ref{sec:ConstrainedAlg}. 
\end{itemize}
 

%\begin{itemize}
%\item Future: Generalization Error Question. 
%
%\end{itemize}
