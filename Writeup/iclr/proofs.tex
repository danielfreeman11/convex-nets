\section{Proofs}

\subsection{Proof of Proposition \ref{connectedminima}}

Suppose that $\theta_1$ is a local minima and $\theta_2$ is a global minima, 
but $F(\theta_1) > F(\theta_2)$. If $\lambda = F(\theta_1)$, then clearly 
$\theta_1$ and $\theta_2$ both belong to $\Omega_F(\lambda)$. Suppose 
now that $\Omega_F(\lambda)$ 
is connected. Then we could find a smooth (i.e. continuous and differentiable) path $\gamma(t)$ 
with $\gamma(0) = \theta_1$, $\gamma(1)= \theta_2$ and $F(\gamma(t)) \leq \lambda = F(\theta_1)$.
But this contradicts the strict local minima status of $\theta_1$, and therefore $\Omega_F(\lambda)$ cannot be connected $\square$.

%In particular, as $t \to 0$, noting $g(t) = F(\gamma(t))$, we have
%\begin{eqnarray*}
%g(t) &=& \sum_{k=0}^K g^{(k)}(0) \frac{t^k}{k!} + o(t^{K}) \\
%&=& g^{(K)}(0) t^{K}{K!} + o(t^K)~,
%\end{eqnarray*}
%which shows that $F(\gamma(t)) \leq F(\theta_1)$ for all $t$ is incompatible with $g^{(K)}>0$ for $K$ even, and therefore $\Omega_F(\lambda)$ cannot be connected $\square$.

%
%\begin{eqnarray*}
%F(\gamma(t)) &=& F(\theta_1) + t \langle \nabla F(\theta_1) , \dot{\gamma}(0) \rangle + \frac{t^2}{2} \left(\dot{\gamma}(0)^T H F(\theta_1) \dot{\gamma}(0) + \langle \nabla F(\theta_1), \ddot{\gamma}(0) \rangle \right) + o(t^2) \\
%&=& F(\theta_1) +   \frac{t^2}{2}  \dot{\gamma}(0)^T H F(\theta_1) \dot{\gamma}(0)  + o(t^2) ~,
%\end{eqnarray*}
%which shows that $F(\gamma(t)) \leq F(\theta_1)$ for all $t$ is incompatible with $H(\theta_1) \succeq 0$ and therefore $\Omega_F(\lambda)$ cannot be connected $\square$.


\subsection{Proof of Proposition \ref{proplinear}}

Let us first consider the case with $\kappa =0$.
We proceed by induction over the number of layers $K$. 
For $K=1$, the loss $F(\theta)$ is convex. Let  $\theta^A$, $\theta^B$ be two arbitrary points 
in a level set $\Omega_\lambda$. Thus $F(\theta^A) \leq \lambda$ and $F(\theta^B) \leq \lambda$. By definition
of convexity, a linear path is sufficient in that case to connect $\theta^A$ and $\theta^B$:
$$F( (1-t) \theta^A + t \theta^B) \leq (1-t) F(\theta^A) + t F(\theta^B) \leq \lambda~.$$
Suppose the result is true for $K-1$. Let $\theta^A = (W_1^A, \dots, W^A_K)$ and 
 $\theta^B = (W_1^B, \dots, W^B_K)$ with $F(\theta^A) \leq \lambda$, $F(\theta^B) \leq \lambda$.
 Since $n_j \geq \min(n_1, n_K)$ for $j=2 \dots K-1$, we can find $k^*=\{1, K-1\}$ such that
 $n_{k^*} \geq \min(n_{k^*-1}, n_{k^*+1})$.
For each $W_1, \dots, W_K$, we denote $\tilde{W}_j = W_j$ for $j \neq k^*, k^*- 1$ and
$\tilde{W}_{k^*} = W_{k^*-1} W_{k^*}$. 
%For each $W_1, \dots, W_K$, we denote $\tilde{W}_j = W_j$ for $j < K-1$ and
%$\tilde{W}_{K-1} = W_K W_{K-1}$. 
By induction hypothesis, the 
loss expressed in terms of $\tilde{\theta} = (\tilde{W}_1, \dots, \tilde{W}_{K-1})$ is connected 
between $\tilde{\theta}^A$ and $\tilde{\theta}^B$. Let $\tilde{W}_{k^*}(t)$ the corresponding 
linear path projected in the  layer $k^*$. 
We need to produce a path in the variables $W_{k^*-1}(t)$, $W_{k^*}(t)$ 
such that:
\begin{itemize}
\item[i] $W_{k^*-1}(0) = W_{k^*-1}^A$, $W_{k^*-1}(1) = W_{k^*-1}^B$, 
\item[ii] $W_{k^*}(0) = W_{k^*}^A$, $W_{k^*}(1) = W_{k^*}^B$,
\item[iii] $W_{k^*}(t) W_{k^*-1}(t) = \tilde{W}_{k^*-1}(t) $ for $t \in (0,1)$. 
%\item[iv] $\| W_{k^*-1}(t) \|^2 + \| W_{k^*}(t) \|^2 \leq (1-t) ( \| W \|_{k^*-1}^2(0) + \| W \|_{k^*}^2(0)  ) + t (\| W \|_{k^*-1}^2(1) + \| W \|_{k^*}^2(1))$.
\end{itemize} 
We construct it as follows. Let 
$$W_{k^*}(t) = t W_{k^*}^B + (1-t) W_{k^*}^A + t (1-t) V~,$$
$$W_{k^*-1}(t) = W_{k^*}(t)^\dagger \tilde{W}_{k^*-1}(t)  ~,$$
where $W_{k^*}(t)^\dagger = ( W_{k^*}(t)^T W_{k^*}(t))^{-1} W_{k^*}(t)^T$ denotes the pseudoinverse 
and $V$ is a $n_{k^*-1} \times n_{k^*}$ matrix drawn from a iid distribution. 
Conditions (i) and (ii) are immediate from the definition, and condition (iii) results from the fact that 
$$W_{k^*}(t) W_{k^*}(t)^\dagger  = {\bf I}_{N_k^*}~,$$
since $W_k^*(t)$ has full rank for all $t \in (0,1)$. 

Finally, let us prove that the result is also true when $K=2$ and $\kappa>0$.
We construct the path using the variational properties of atomic norms \cite{bach2013convex}. 
When we pick the ridge regression regularization, the corresponding atomic norm is the 
nuclear norm:
$$\| X \|_{*} = \min_{UV^T = X} \frac{1}{2}( \| U \|^2 + \| V \|^2)~.$$
The path is constructed by exploiting the convexity of the variational norm $\| X\|_{*}$. 
Let $\theta^A = (W_1^A, W_2^A)$ and $\theta^B=(W_1^B, W_2^B)$, and we 
define $\tilde{W} = W_1 W_2$. Since $\tilde{W}^{\{A,B\}} = W_1^{\{A,B\}} W_2^{\{A,B\}} $, 
it results that
\begin{equation}
\label{colcol}
\| \tilde{W}^{\{A,B\}} \|_* \leq \frac{1}{2} ( \| W_1^{\{A,B\}}\|^2 + \| W_2^{\{A,B\}}\|^2)~.
\end{equation}
From (\ref{colcol}) it results that the loss $\Forr(W_1, W_2)$ can be minored by another loss 
 expressed in terms of $\tilde{W}$ of the form 
$$\E \{ | Y - \tilde{W} X |^2 \} + 2\kappa \| \tilde{W} \|_*~,$$ 
which is convex with respect to $\tilde{W}$. Thus a linear path in $\tilde{W}$ from 
$\tilde{W}^A$ to $\tilde{W}^B$ is guaranteed to be below $\Forr(\theta^{\{A,B\}})$. 
Let us define 
$$\forall~t~,~W_{1}(t), W_{2}(t) = \arg\min_{UV^T= \tilde{W}(t)} (\| U\|^2 + \|V \|^2)~. $$ 
One can verify that we can first consider a path $(\beta^A_1(s), \beta^A_2(s))$ 
from $(W_1^{A}, W_2^{A})$ to $(W_{1}(0), W_{2}(0)$ such that 
$$\forall~s~\beta_1(s) \beta_2(s) = \tilde{W}^A \text{ and } \| \beta_1(s) \|^2 + \| \beta_2(s) \|^2 \text{ decreases} ~,$$
and similarly for $(W_1^{B}, W_2^{B})$ to $(W_{1}(1), W_{2}(1)$.
The path $(\beta_{\{1,2\}}^A(s), W_{\{1,2\}}(t), \beta_{\{1,2\}}^B(s))$ satisfies (i-iii) by definition. We also verify that 
\begin{eqnarray*}
\| W_{1}(t) \|^2 + \| W_{2}(t) \|^2 &=& 2 \| \tilde{W}(t)\|_{*}   \\
&\leq & 2 (1-t) \|  \tilde{W}(0)\|_{*} + 2 t  \|  \tilde{W}(1)\|_{*} \\ 
&\leq & (1-t) ( \| W \|_{1}^2(0) + \| W \|_{2}^2(0)  ) + t (\| W \|_{1}^2(1) + \| W \|_{2}^2(1))~.
\end{eqnarray*}
Finally, we verify that the paths we have just created, when applied to $\theta^A$ arbitrary and 
$\theta^B = \theta^*$ a global minimum, are strictly decreasing, again by induction. 
For $K=1$, this is again an immediate consequence of convexity. 
For $K>1$, our inductive construction guarantees that for any $0<t<1$, 
the path $\theta(t) = (W_k(t))_{k \leq K}$ satisfies $\Forr(\theta(t)) < \Forr(\theta^A)$. 
 This concludes the proof $\square$. 

%
%We need to produce a path in the variables $W_{K-1}(t)$, $W_K(t)$ 
%such that:
%\begin{itemize}
%\item[i] $W_{K-1}(0) = W_{K-1}^A$, $W_{K-1}(1) = W_{K-1}^B$, 
%\item[ii] $W_{K}(0) = W_{K}^A$, $W_{K}(1) = W_{K}^B$,
%\item[iii] $W_{K}(t) W_{K-1}(t) = \tilde{W}_{K-1}(t) = (1-t) W_{K}^A W_{K-1}^A + t W_{K}^B W_{K-1}^B $ for $t \in (0,1)$. 
%\item[iv] $\| W_{K-1}(t) \|^2 + \| W_{K}(t) \|^2 \leq (1-t) ( \| W \|_{K-1}^2(0) + \| W \|_{K}^2(0)  ) + t (\| W \|_{K-1}^2(1) + \| W \|_{K}^2(1))$.
%\end{itemize} 
%We construct the path using the variational properties of atomic norms \cite{bach2013convex}. 
%When we pick the ridge regression regularization, the corresponding atomic norm is the 
%nuclear norm:
%$$\| X \|_{*} = \min_{UV^T = X} \frac{1}{2}( \| U \|^2 + \| V \|^2)~.$$
%The path is constructed by exploiting the convexity of the variational norm $\| X\|_{*}$. 
%Let us define 
%$$\forall~t~,~W_{K-1}(t), W_{K}(t) = \arg\min_{UV^T= \tilde{W}_{K-1}(t)} (\| U\|^2 + \|V \|^2)~. $$ 
%This path satisfies (i-iii) by definition. We also verify that 
%\begin{eqnarray*}
%\| W_{K-1}(t) \|^2 + \| W_{K}(t) \|^2 &=& 2 \| \tilde{W}_{K-1}(t)\|_{*}   \\
%&\leq & 2 (1-t) \|  \tilde{W}_{K-1}(0)\|_{*} + 2 t  \|  \tilde{W}_{K-1}(1)\|_{*} \\ 
%&\leq & (1-t) ( \| W \|_{K-1}^2(0) + \| W \|_{K}^2(0)  ) + t (\| W \|_{K-1}^2(1) + \| W \|_{K}^2(1))~.
%\end{eqnarray*}
%which concludes the proof $\square$. 

%We construct it as follows. Let 
%$$W_{K}(t) = t W_{K}^B + (1-t) W_{K}^A + t (1-t) V~,$$
%$$W_{K-1}(t) = W_{K}(t)^\dagger \tilde{W}_{K-1}(t)  ~,$$
%where $W_{K}(t)^\dagger = ( W_{K}(t)^T W_{K}(t))^{-1} W_{K}(t)^T$ denotes the pseudoinverse 
%and $V$ is a $n_{K-1} \times n_{K}$ matrix drawn from a iid distribution. 
%Conditions (i) and (ii) are immediate from the definition, and condition (iii) results from the fact that 
%$$W_{K}(t) W_{K}(t)^\dagger  = {\bf I}_{N_K}~,$$
%since $W_K(t)$ has full rank for all $t \in (0,1)$. 
%$\square$.



\subsection{Proof of Proposition \ref{localdistprop}}

Let 
$$A(w_1, w_2) = \{ x \in \R^n; \, \langle x, w_1 \rangle \geq 0\,,\, \langle x, w_2 \rangle \geq 0\}~.$$
By definition, we have 
\begin{eqnarray}
\label{col1}
\langle w_1, w_2 \rangle_Z &=& \E \{ \max(0, \langle X, w_1 \rangle ) \max(0, \langle X, w_2 \rangle ) \} \\
&=& \int_{A(w_1, w_2)} \langle x, w_1 \rangle  \langle x, w_2 \rangle dP(x)~, \\
&=& \int_{Q({A}(w_1, w_2))}  \langle Q(x), w_1 \rangle  \langle Q(x), w_2 \rangle (d\bar{P}(Q(x)))~,  
\end{eqnarray}
where $Q$ is the orthogonal projection onto the space spanned by $w_1$ and $w_2$ and
 $d\bar{P}(x)=d\bar{P}(x_1, x_2)$ is the marginal density on that subspace. 
 Since this projection does not interfere with the rest of the proof, we abuse notation by dropping the $Q$ and still referring to $dP(x)$ as the probability density.

Now, let $r = \frac{1}{2}\| w_1 + w_2 \| = \frac{1 + \cos(\alpha)}{2}$ and $d = \frac{w_2 - w_1}{2}$.
By construction we have 
$$w_1 = r w_m - d~,~ w_2 = r w_m + d~,$$
and thus 
\begin{equation}
\label{col2}
\langle x, w_1 \rangle  \langle x, w_2 \rangle = r^2 | \langle x, w_m \rangle |^2 - | \langle x, d \rangle |^2~.
\end{equation}
By denoting $C(w_m) = \{ x \in \R^n;\, \langle x, w_m \rangle \geq 0 \}$, 
observe that $A(w_1, w_2 ) \subseteq C(w_m)$. Let us denote by $B = C(w_m) \setminus A(w_1, w_2) $ the disjoint complement. It results that 
\begin{alignat}{3}
\langle w_1, w_2 \rangle_Z &= \int_{A(w_1, w_2)} &&\langle x, w_1 \rangle  \langle x, w_2 \rangle dP(x) \nonumber \\
&= \int_{C(w_m)} &&[r^2 | \langle x, w_m \rangle |^2 - | \langle x, d \rangle |^2 ] dP(x) - \nonumber \\
& &&r^2 \int_B  | \langle x, w_m \rangle |^2 dP(x) + \int_B  | \langle x, d \rangle |^2  dP(x) \nonumber \\ 
&= &&r^2 \| w_m \|_Z^2 - \underbrace{ r^2 \int_B  | \langle x, w_m \rangle |^2 dP(x)}_{E_1} - \underbrace{\int_{A(w_1, w_2)} | \langle x, d \rangle |^2  dP(x) }_{E_2}~.
\end{alignat} 
We conclude by bounding each error term $E_1$ and $E_2$ separately:
\begin{equation}
\label{col3}
0 \leq E_1 \leq r^2 |\sin(\alpha)|^2 \int_B \| x \|^2 dP(x) \leq r^2 |\sin(\alpha)|^2 2 \| \Sigma_X\|~,
\end{equation}
since every point in $B$ by definition has angle greater than $\pi/2 - \alpha$ from $w_m$. Also,
\begin{equation}
\label{col4}
0 \leq E_2 \leq \|d \|^2 \int_{A(w_1, w_2)} \| x \|^2 dP(x) \leq \frac{1 - \cos(\alpha)}{2} 2 \| \Sigma_X \|
\end{equation}
by direct application of Cauchy-Schwartz. The proof is completed by plugging the bounds from (\ref{col3}) and (\ref{col4}) into (\ref{col5})  $\square$.


\subsection{Proof of Theorem \ref{maintheo}}

Consider a generic $\alpha$ and $l \leq m$. A path from $\theta^A$ to $\theta^B$ will be constructed 
by concatenating the following paths:
\begin{enumerate}
\item from $\theta^A$ to $\theta_{lA}$, the 
best linear predictor using the same first layer as $\theta^A$, 
\item from $\theta_{lA}$ to $\theta_{sA}$, the best $(m-l)$-term approximation using perturbed 
atoms from $\theta^A$,
\item from $\theta_{sA}$ to $\theta^*$ the oracle $l$ term approximation,  
\item from $\theta^*$ to $\theta_{sB}$, the best $(m-l)$-term approximation using perturbed 
atoms from $\theta^B$,
\item from $\theta_{sB}$ to $\theta_{lB}$, the 
best linear predictor using the same first layer as $\theta^B$, 
\item from $\theta_{lB}$ to $\theta^{B}$.
\end{enumerate}
The proof will study the increase in the loss along each subpath and aggregate 
the resulting increase into a common bound. 

Subpaths (1) and (6) only involve changing the parameters of the second layer 
while leaving the first-layer weights fixed, which define a convex loss. Therefore a linear path is sufficient to guarantee that 
the loss along that path will be upper bounded by $\lambda$ on the first end 
and $\delta_{W_1^A}(m,0,m)$ on the other end. 

Concerning subpaths (3) and (4), we notice that they can also be constructed using only parameters of the second layer, 
by observing that one can fit into a single $n \times m$ parameter matrix both the 
$(m-l)$-term approximation and the oracle $l$-term approximation. 
Indeed, let us describe subpath (3) in detail ( subpath (4) is constructed analogously by replacing the role of $\theta_{sA}$ 
with $\theta_{sB}$). Let $\tilde{W}_A$ the first-layer parameter matrix associated with the 
$m-l$-sparse solution $\theta_{sA}$, and let $\gamma_A$ denote its second layer coefficients, which is a $m$-dimensional vector
with at most $m-l$ non-zero coefficients. 
Let $W_{*}$ be the first-layer matrix of the $l$-term oracle approximation, and $\gamma_{*}$ the corresponding second-layer coefficients. 
Since there are only $m-l$ columns of $\tilde{W}_A$ that are used, corresponding to the support of $\gamma_A$, we can 
consider a path $\bar{\theta}$ that replaces the remaining $l$ columns with those from $W_{*}$ while keeping the second-layer vector $\gamma_A$ fixed. Since the modified columns correspond to zeros in $\gamma_A$, such paths have constant loss. 
Call $\bar{W}$ the resulting first-layer matrix, containing both the active $m-l$ active columns of $\tilde{W}_A$ and the $l$ columns of $W_{*}$ in the positions determined by the zeros of $\gamma_A$. 
Now we can consider the linear subpath that interpolates between $\gamma_A$ and $\gamma_{*}$ while keeping the first layer fixed at $\bar{W}$. 
Since again this is a linear subpath that only moves second-layer coefficients, it is non-increasing thanks to the convexity of the loss while fixing the first layer. We easily verify that at the end of this linear subpath we are using the oracle $l$-term approximation, which has loss $e(l)$, and therefore 
subpath (3) incurs in a loss that is bounded by its extremal values $\delta_{W_1^A}(m-l, \alpha, m)$ and $e(l)$. 


Finally, we need to show how to construct the subpaths (2) and (5), which are the most delicate step since they cannot be bounded using
convexity arguments as above. 
Let $\tilde{W}_A$ be the resulting perturbed first-layer parameter matrix 
with $m-l$ sparse coefficients $\gamma_A$.
Let us consider an auxiliary regression of the form 
$$\overline{W} = [ W^A ; \tilde{W}_A] ~\in \R^{n \times 2m}~.$$
and regression parameters 
$$\overline{\beta}_1 = [ \beta_1; 0]~,~\overline{\beta}_2 = [0; \gamma_A]~.$$
Clearly 
$$\E\{ | Y - \overline{\beta}_1 \overline{W} |^2 \} + \kappa \| \overline{\beta}_1 \|_1 = \E\{ | Y - \beta_1 W^A |^2 \} + \kappa \| {\beta}_1 \|_1 $$ 
%and $\| \overline{\beta}_1 \|_1 = \| \beta_1 \|_1$,
 and similarly for $\overline{\beta}_2$. By convexity, the augmented linear path $\eta(t) =(1- t) \overline{\beta}_1 + t \overline{\beta}_2$ thus satisfies 
$$\forall~t~,\overline{L}(t) = \E\{ | Y - \eta(t) \overline{W} |^2 \} + \kappa \| \eta(t) \|_1 \leq \max(\overline{L}(0), \overline{L}(1))~. $$
Let us now approximate this augmented linear path with a path in terms of first and second layer weights. 
We consider
$$\eta_1(t) = (1-t) W^A + t \tilde{W}_A~,\text{ and}~\eta_2(t) = (1- t) {\beta}_1 + t \gamma_A~.$$
We have that 
\begin{alignat}{3}
\label{bub1}
\Forr(\{ \eta_1(t), \eta_2(t) \}) &= &&\ \E \{ | Y - \eta_2(t) Z(\eta_1(t) ) |^2 \} + \kappa \| \eta_2(t) \|_1  \\ 
&\leq  &&\ \E \{ | Y - \eta_2(t) Z(\eta_1(t) ) |^2 \} + \kappa(  ( 1-t) \| {\beta}_1\|_1 + t \| \gamma_A \|_1 ) \nonumber \\
& = &&\ \overline{L}(t) + \E \{ | Y - \eta_2(t) Z(\eta_1(t) ) |^2 \} \nonumber \\
& - &&\ \E \{ | Y - (1-t) \beta_1 Z(W^A) - t \gamma_A Z(\tilde{W}_A) |^2 \} ~.
\end{alignat}
Finally, we verify that
{\small 
\begin{eqnarray}
\label{bub2}
& \left | \E \{ | Y - \eta_2(t) Z(\eta_1(t) ) |^2 \}  - \E \{ | Y - (1-t) \beta_1 Z(W^A) - t \gamma_A Z(\tilde{W}_A) |^2 \} \right|  \leq\\
& \leq 4  \alpha \max(\E |Y|^2, \sqrt{\E|Y^2|}) \| \Sigma_X \| ( \kappa^{-1/2} + \alpha \sqrt{\E|Y^2|} \kappa^{-1}) + o(\alpha^2)~. \nonumber
\end{eqnarray}}
Indeed, from Proposition \ref{localdistprop}, and using the fact that 
$$\forall~i\leq M,\, t \in [0,1]~,~\left| \angle( (1-t)w^A_i + t \tilde{w}^A_i ; w^A_i) \right| \leq \alpha~,~ \left| \angle( (1-t)w^A_i + t \tilde{w}^A_i ; \tilde{w}^A_i) \right| \leq \alpha $$
we can write 
$$(1-t) \beta_{1,i} z(w^A_i) + t \gamma_{A,i} z(\tilde{w}^A_i) \stackrel{d}{=} \eta_2(t)_i z(\eta_1(t)_i) + n_i ~,$$
with $\E\{ |n_i |^2 \} \leq 4 |\eta_2(t)_i|^2 \| \Sigma_X \| \alpha^2 + O(\alpha^4)~$ and $\E |n_i| \leq 2 |\eta_2(t)_i| \alpha \sqrt{\| \Sigma_X\|}$ using concavity of the moments.
Thus 
\begin{eqnarray*}
&& \left | \E \{ | Y - \eta_2(t) Z(\eta_1(t) ) |^2 \}  - \E \{ | Y - (1-t) \beta_1 Z(W^A) - t \gamma_A Z(\tilde{W}_A) |^2 \} \right| \\
 &\leq& 2\E \left\{  \sum_i (Y - \eta_2(t) Z(\eta_1(t) )) n_i  \right\} + \E \left\{ | \sum_i n_i |^2 \right\} \\
 &\leq & 4\left(\alpha \sqrt{\E|Y^2|} \| \Sigma_X\|  \| \eta_2 \| + \alpha^2 (\| \eta_2 \|_1)^2  \| \Sigma_X \| \right) \\
 &\leq & 4 \alpha \max(1, \sqrt{\E|Y^2|}) \| \Sigma_X \| ( \| \eta_2 \|_1 + \alpha \| \eta_2 \|_1^2) + o(\alpha^2) \\
 & \leq & 4 \alpha \max(\sqrt{\E|Y^2|}, {\E|Y^2|}) \| \Sigma_X \|  ( \kappa^{-1} + \alpha \sqrt{\E|Y^2|} \kappa^{-2}) + o(\alpha^2)~,
\end{eqnarray*}
 which proves (\ref{bub2}). 
 
We have just constructed a path from $\theta^A$ to $\theta^B$, in which all subpaths except (2) and (5) have energy maximized at the extrema due to convexity, given respectively by $\lambda$, $\delta_{W_A^1}(m, 0, m)$, $\delta_{W_A^1}(m-l, \alpha, m)$, $e(l)$, $\delta_{W_B^1}(m-l, \alpha, m)$, and 
$\delta_{W_B^1}(m, 0, m)$.  For the two subpaths (2) and (5), (\ref{bub2}) shows that it is sufficient to add the corresponding upper bound to the linear subpath, which is of the form $C \alpha + o(\alpha^2)$ where $C$ is an explicit constant independent of $\theta$. Since $l$ and $\alpha$ are arbitrary, we are free to pick the infimum, which concludes the proof. $\square$
 
%8 \sqrt{\E|Y^2} \} \max( \| \beta \|^2, \| \gamma_A \|^2) \alpha^2 \| \Sigma_X \| 



% and  $\| \beta \| \leq \kappa^{-1}$  $\square$.

\subsection{Proof of Corollary \ref{maincoro}}

Let us consider a generic first layer weight matrix $W \in \R^{n \times m}$. Without loss of generality, we can assume that $\| w_k \|=1$ for all $k$, since increasing the norm of $\|w_k\|$ 
within the unit ball has no penalty in the loss, and we can compensate this scaling in the second layer
thanks to the homogeneity of the half-rectification. Since this results in an attenuation of these second layer weights, 
they too are guaranteed not to increase the loss. 

From \cite{vershynin2010introduction} [Lemma 5.2] we verify that the covering number $\mathcal{N}(S^{n-1}, \epsilon)$ of the Euclidean unit sphere $S^{n-1}$ 
satisfies 
$$\mathcal{N}(S^{n-1}, \epsilon) \leq \left( 1 + \frac{2}{\epsilon} \right)^n~,$$
which means that we can cover the unit sphere with an $\epsilon$-net of size $\mathcal{N}(S^{n-1}, \epsilon) $. 

Let $0< \eta < n^{-1} ( 1 + n^{-1})^{-1}$, and let us pick, for each $m$, $\epsilon_m = m^{\frac{\eta-1}{n} }$. 
Let us consider its corresponding $\epsilon$-net of size 
$$u_m = \mathcal{N}(S^{n-1}, \epsilon_m) \simeq \left( 1 + \frac{2}{\epsilon_m} \right)^n \simeq m^{1-\eta} ~.$$
Since we have $m$ vectors in the unit sphere, it results from the pigeonhole principle that at least one 
element of the net will be associated with at least $v_m = m u_m^{-1} \simeq m^{\eta}$ vectors; in other words, 
we are guaranteed to find amongst our weight vector $W$ a collection $Q_m$ of $v_m \simeq m^\eta$ vectors that are all 
at an angle at most $2\epsilon_m$ apart. 
Let us now apply Theorem \ref{maintheo} by picking $n=v_m$ and $\alpha = \epsilon_m$. 
We need to see that the terms involved in the bound all converge to $0$ as $m \to \infty$. 

The contribution of the oracle error $e(v_m) - e(m)$ goes to zero as $m\to \infty$ by the 
fact that $\lim_{m \to \infty} e(m)$ exists (it is a decreasing, positive sequence) and that $v_m \to \infty$.

Let us now verify that $\delta(m - v_m, \epsilon_m, m)$ also converges to zero.
We are going to prune the first layer by removing one by one the vectors in $Q_m$.
Removing one of these vectors at a time incurs in an error of the order of $\epsilon_m$. 
Indeed, let $w_k$ be one of such vectors and let $\beta'$ be the solution of 
$$\min_{\beta'} E(\beta') = \min_{\beta'=(\beta_f;\beta_k) \in \R^{k}} \E \{ |Y - \beta_f^T Z(W_{-k}) - \beta_k z(w_k)|^2 \} + \kappa ( \| \beta_f \|_1 + |\beta_k| )~, $$
where $W_{-k}$ is a shorthand for the matrix containing the rest of the vectors that have not been discarded yet.
  Removing the vector $w_k$ from the first layer
 increases the loss by a factor that is upper bounded by $E(\beta_p) - E(\beta)$, where 
$$(\beta_p)_j = \left\{
\begin{array}{rl}
 \beta'_j  & \text{ for  } j < k -1 ~,\\
 \beta'_{k-1} + \beta'_{k } & \text{ otherwise.}
\end{array}\right.~, $$
 since now $\beta_p$ is a feasible solution for the pruned first layer.

Let us finally bound $E(\beta_p) - E(\beta)$. 

Since $\angle(w_k,w_{k-1}) \leq \epsilon_m $, 
it results from Proposition \ref{localdistprop} that 
$$z(w_k) \stackrel{d}{=}  z(w_{k-1}) + n~,$$
with $\E\{|n|^2\} \leq C\alpha^2$ for some constant $C$ independent of $m$. 
By redefining $p_1 = Y - \beta_p^T Z(W_{-k}) - \frac{1}{2} n$ and 
$p_2 = \frac{1}{2}n$, we have 
\begin{alignat*}{3}
&\E \{ | Y - \beta_p^T Z(W_{-k}) |^2 \} - \E \{ | Y - {\beta'}^T Z(W_{-k}) - \beta_k z(w_k) |^2 \} \\
=\ &\E\{ | p_1 + p_2 |^2 \} - \E \{ |p_1 - p_2 |^2 \} \\
=\ &4\E \{ |p_1 p_2| \} \\
\leq\ &\sqrt{ \E\left\{ \left|Y - \beta_p^T Z(W_{-k}) - \frac{1}{2} n \right|^2 \right\} } \sqrt{ \E \{ |n|^2 \} } \\
\leq\  &(C+\alpha) \alpha \simeq \epsilon_m~,
\end{alignat*}
where $C$ only depends on $\E\{|Y|^2\}$. We also verify that $\| \beta_p\|_1 \leq \| \beta'\|_1$.

It results that removing $|Q_m|$ of such vectors incurs an increase of the loss at most 
$|Q_m| \epsilon_m \simeq m^\eta m^{\frac{\eta-1}{n} } = m^{\eta + \frac{\eta-1}{n}}$. 
Since we picked $\eta$ such that $\eta + \frac{\eta-1}{n} <0$, this term converges to zero. The proof is finished. $\square$

%
%It results that if we choose $\epsilon$
%
%Use pigeonhole principle to control how many directions are within an angle smaller than $\epsilon$. 
%
%
%




