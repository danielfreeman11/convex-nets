

%%%%%%%%%%%%%%%%%%%%%%
\section{Geometry of Level Sets}
%\section{Quantifying Nonconvexity}
\label{sec:QuanNoncon}

\subsection{The Greedy Algorithm}
\label{sec:GreedyAlg}
%%%%%%%%%%%%%%%%%%%%%%
 
 The intuition behind our main result is that, for smooth enough loss functions and for sufficient overparameterization, it should be ``easy'' to connect two equally powerful models---i.e., two models with $F_o{\theta^{A,B}} \leq \lambda$.  A sensible measure of this ease-of-connectedness is the normalized length of the geodesic connecting one model to the other: $|\gamma_{A,B}(t)| / |\theta_A - \theta_B|$.  This length represents approximately how far of an excursion one must make in the space of models relative to the euclidean distance between a pair of models.  Thus, convex models have a geodesic length of $1$, because the geodesic is simply linear interpolation between models, while more non-convex models have geodesic lengths strictly larger than $1$.
 
 Because calculating the exact geodesic is difficult, we approximate the geodesic paths via a dynamic programming approach we call Dynamic String Sampling.  We comment on alternative algorithms in Appendix \ref{sec:ConstrainedAlg}.
 
 For a pair of models with network parameters $\theta_i$, $\theta_j$, each with $F_e(\theta)$ below a threshold $L_0$, we aim to efficienly generate paths in the space of weights where the empirical loss along the path remains below $L_0$.  These paths are continuous curves belonging to $\Omega_F(\lambda)$--that is, the level sets of the loss function of interest.

\begin{algorithm}
\caption{Greedy Dynamic String Sampling}\label{euclid}
\begin{algorithmic}[1]
\State $\text{$L_0$} \gets \text{Threshold below which path will be found}$
\State $\text{$\Phi_1$} \gets \text{randomly initialize } $$\theta_1$$ \text{, train } $$\Phi (x_i\;\theta_1)$$ \text{ to $L_0$}$
\State $\text{$\Phi_2$} \gets \text{randomly initialize } $$\theta_2$$ \text{, train } $$\Phi (x_i\;\theta_2)$$ \text{ to $L_0$}$

\State $\text{BeadList} \gets $$(\Phi_1,\Phi_2)$
\State $\text{Depth} \gets 0$ 

\Procedure{FindConnection}{$\Phi_1,\Phi_2$}
\State $\text{$t^*$} \gets \text{t such that } $$\frac{d \gamma(\theta_1, \theta_2, t)}{dt} \bigg|_{t} = 0$$  \text{ OR } $$t = 0.5$$ $
\State $\text{$\Phi_3$} \gets \text{train } $$\Phi(x_i; t^*\theta_1 + (1-t^*)\theta_2)$$ \text{ to $L_0$}$
\State $\text{BeadList} \gets \text{insert}$$(\Phi_3$$\text{, after } $$\Phi_1$$\text{, BeadList)}$
\State $\text{$MaxError_1$} \gets \text{$max_t$}$$(F_e(t\theta_3 + (1-t)\theta_1))$$ $
\State $\text{$MaxError_2$} \gets \text{$max_t$}$$(F_e(t\theta_2 + (1-t)\theta_3))$$ $
\If {$\text{$MaxError_1$} > \text{$L_0$ }} \text{ }\Return \text{ FindConnection}$$(\Phi_1,\Phi_3)$$ $
\EndIf
\If {$\text{$MaxError_2$} > \text{$L_0$ }} \text{ }\Return \text{ FindConnection}$$(\Phi_3,\Phi_2)$$ $
\EndIf
\State $\text{Depth} \gets \text{Depth$+1$}$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}
 
  The algorithm recursively builds a string of models in the space of weights which continuously connect $\theta_i$ to $\theta_j$.  Models are added and trained until the pairwise linearly interpolated loss, i.e. $\rm{max}_t F_e(t\theta_i\ +\ (1-t)\theta_j)$ for $t\in(0,1)$, is below the threshold, $L_0$, for every pair of neighboring models on the string.  We provide a cartoon of the algorithm in Appendix \ref{AlgCartoon}.
 
  
  \subsection{Failure Conditions and Practicalities}
  \label{sec:Fail}
  
  While the algorithm presented will faithfully certify two models are connected if the algorithm converges, it is worth emphasizing that the algorithm does not guarantee that two models are disconnected if the algorithm fails to converge.  In general, the problem of determining if two models are connected can be made arbitrarily difficult by choice of a particularly pathological geometry for the loss function, so we are constrained to heuristic arguments for determining when to stop running the algorithm.  Thankfully, in practice, loss function geometries for problems of interest are not intractably difficult to explore.  We comment more on diagnosing disconnections more carefully in Appendix \ref{sec:disconnect}.
  
  Further, if the $\rm{\mathbf{MaxError}}$ exceeds $L_0$ for every new recursive branch as the algorithm progresses, the worst case runtime scales as $O(\rm{exp}(\rm{\mathbf{Depth}}))$.  Empirically, we find that the number of new models added at each depth does grow, but eventually saturates, and falls for a wide variety of models and architectures, so that the typical runtime is closer to $O(\rm{poly}(\rm{\mathbf{Depth}}))$---at least up until a critical value of $L_0$.
  
  To aid convergence, either of the choices in line $7$ of the algorithm works in practice---choosing $t^*$ at a local maximum can provide a modest increase in algorithm runtime, but can be unstable if the the calculated interpolated loss is particularly flat or noisy.  $t^*=.5$ is more stable, but slower.  Finally, we find that training $\Phi_3$ to $\alpha L_0$ for $\alpha < 1$ in line $8$ of the algorithm tends to aid convergence without noticeably impacting our numerics.  We provide further implementation details in \ref{sec:NumExp}.
 
