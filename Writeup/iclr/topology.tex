\section{Topology of Level Sets}

Let $P$ be a probability measure on a product space $\mathcal{X} \times \mathcal{Y}$, 
where we assume $\mathcal{X}$ and $\mathcal{Y}$ are Euclidean vector spaces for simplicity.
Let $\{ (x_i, y_i)\}_i$ be an iid sample of size $L$ drawn from $P$ defining the training set.
We consider the classic empirical risk minimization of the form
\begin{equation}
\label{emp_risk_min}
\Fem(\theta) = \frac{1}{L} \sum_{l=1}^L \| \Phi(x_i;\theta) - y_i \|^2 + \kappa \mathcal{R}(\theta)~,
\end{equation}
where $\Phi(x ; \theta)$ encapsulates the feature representation 
that uses parameters $\theta \in \R^S$ and $\mathcal{R}(\theta)$ is a regularization term. 
 In a deep neural network, $\theta$
contains the weights and biases used in all layers.
For convenience, in our analysis we will also use the oracle risk minimization:
\begin{equation}
\label{risk_min}
\Forr(\theta) = \E_{(X,Y) \sim P} \| \Phi(X;\theta) - Y \|^2 + \kappa \mathcal{R}(\theta)~.
\end{equation}
Our setup considers the case where $\mathcal{R}$ 
consists on either $\ell_1$ or $\ell_2$ norms, as we shall describe below.
They correspond to well-known sparse and ridge regularization respectively.
%
% on all layers: $\mathcal{R}(\theta) = \| \theta \|_1$, 
%which provides a sparse regularization prior, and the Ridge regression case  
%$\mathcal{R}(\theta) = \| \theta\|_2^2$ (also referred as weight decay). 

\subsection{Poor local minima characterization from topological connectedness}

We define the level set of $F(\theta)$ as 
\begin{equation}
\Omega_F(\lambda) = \{ \theta \in \R^S~;~F(\theta) \leq \lambda \}~. 
\end{equation}

The first question we study is the structure of critical points of $\Fem(\theta)$ and $\Forr(\theta)$
when $\Phi$ is a multilayer neural network. In particular, we are interested to know whether
$\Fem$ has local minima which are not global minima. This question is answered by 
knowing whether $\Omega_F(\lambda)$ is connected at each energy level $\lambda$:

\begin{proposition}
\label{connectedminima}
If $\Omega_F(\lambda)$ is connected for all $\lambda$ then every local minima of $F(\theta)$ is a global minima. 
\end{proposition}

This proposition shows that a sufficient condition to prevent the existence of poor local minima is having connected level sets, but this condition is not necessary: one can have isolated local minima lying 
at the same energy level. This can be the case in systems that are defined up to 
a discrete symmetry group, such as multilayer neural networks. However, as we shall see next, this case puts the system in a brittle position, since one needs to be able to account for all the local minima (and there can be exponentially many of them as the parameter dimensionality increases) and verify that their energy is indeed equal. 

\subsection{The Linear Case}

We first consider the 
particularly simple case where 
$F$ is a multilayer network defined by
\begin{equation}
\label{linearcase}
\Phi(x;\theta) = W_K \dots W_1 x~,~\theta = (W_1, \dots, W_K)~.
\end{equation}
and the ridge regression $\mathcal{R}(\theta) =\| \theta \|^2$. This model defines a non-convex (and non-concave) loss $\Fem(\theta)$.
When $\kappa = 0$, it has been shown in \cite{saxe2013exact} and \cite{kawaguchi2016deep} that in this case, 
every local minima is a global minima.  
We provide here an alternative proof of that result that uses
a somewhat simpler argument and allows for $\kappa > 0$. 

%For that purpose, let $W_1, W_2, \dots, W_K$ be weight matrices of sizes 
%$n_k \times n_{k+1}$, $k < K$. Assume first that $n_j \geq \min(n_1, n_K)$ for $j=2 \dots K-1$.
%and let us define the following multilinear regression problem:
%\begin{equation}
%\label{multilinloss}
%L_0(W_1, \dots, W_K) = \sum_i \| W_K, \dots W_1 x_i - y_i \|^2~,
%\end{equation}
%where $\{ (x_i, y_i)\,; x_i \in \mathbb{R}^{n_1}, y_i \in \mathbb{R}^{n_K} \}_i$ is a given 
%training set. 

\begin{proposition}
\label{proplinear}
Let $W_1, W_2, \dots, W_K$ be weight matrices of sizes 
$n_k \times n_{k+1}$, $k < K$, and let $\Fem(\theta)$, $\Forr(\theta)$ 
denote the risk minimizations using $\Phi$ as in (\ref{linearcase}) and ridge regression.
Assume that $n_j \geq \min(n_1, n_K)$ for $j=2 \dots K-1$.
Then $\Omega_{\Fem}(\lambda)$ is connected for all $\lambda$, as well as $\Omega_{\Forr}$, 
and therefore there are no poor local minima.  
\end{proposition}
This result can be extended to treat other regularization terms other than the ridge term 
by using more general atomic norms \cite{bach2013convex}. But we now move our interest to the nonlinear case, which is more relevant to our purposes. 

\subsection{Half-Rectified Nonlinear Case}

We now study the setting given by 
\begin{equation}
\label{relucase}
\Phi(x;\theta) = W_K \rho W_{K-1} \rho \dots \rho W_1 x~,~\theta = (W_1, \dots, W_K)~,
\end{equation}
where $\rho(z) = \max(0 ,z)$. 
The biases can be implemented by replacing the input vector $x$ 
with $\overline{x}=(x, 1)$ and by rebranding each parameter matrix as 
$$\overline{W}_i = \left( 
\begin{array}{c|c}
W_i & b_i \\
\hline 
0 & 1 
\end{array}
\right)~,$$
where $b_i$ contains the biases for each layer.	
For simplicity, we continue to use $W_i$ and $x$ in the following.

\subsubsection{Nonlinear models are generally disconnected}
\label{disconnect}

One may wonder whether the same phenomena of global connectedness also holds 
in the half-rectified case. A simple motivating counterexample shows that this is not the case in 
general. Consider a simple setup with $X \in \R^2$ drawn from a mixture of two Gaussians $\mathcal{N}_{-1}$ 
and $\mathcal{N}_{1}$, and let $Y = (X-\mu_Z) \cdot Z $ , where $Z$ is the (hidden) mixture component taking $\{1,-1\}$ values.  Let 
$\hat{Y} = \Phi(X; \{ W_1, W_2\} )$ be a single-hidden layer ReLU network, with two hidden units. 
%Since the model is homogeneous, one can think about $W_1$ as encoding two unitary vectors without loss of generality. 
Let $\theta^A$ be a configuration that bisects the two mixture components, 
and let $\theta^B$ the same configuration, but swapping the bisectrices. 
One can verify that they can both achieve arbitrarily small risk by letting the covariance of the mixture components go to $0$. 
However, any path that connects $\theta^A$ to $\theta^B$ 
must necessarily pass through a point in which $W_1$ has rank $1$, which leads to an estimator with risk at least $1/2$.  

In fact, it is easy to see that this counter-example can be extended to any generic half-rectified architecture, if one is 
allowed to adversarially design a data distribution. For any given $\Phi(X; \theta)$ with arbitrary architecture and current parameters 
$\theta = (W_i)$, let $\mathcal{P}_\theta=\{ \mathcal{A}_1, \dots, \mathcal{A}_S\}$ be the underlying tessellation of the input space given by our current choice of parameters; that is, $\Phi(X; \theta)$ is piece-wise linear and $\mathcal{P}_\theta$ contains those pieces. Now let 
$X$ be any arbitrary distribution with density $p(x) > 0$ for all $x \in \R^n$, for example a Gaussian, and let %$Y ~|~ \{X \in \mathcal{A}_s\} = s$. 
%It is the indicator function corresponding to the tesselation $\mathcal{P}$. 
$Y ~|~X ~\stackrel{d}{=} \Phi(X ; \theta)$~. Since $\Phi$ is invariant under a subgroup of permutations $\theta_\sigma$ of its hidden layers, it is easy to see that one can find two parameter values $\theta_A = \theta$ and $\theta_B = \theta_\sigma$ such that $\Forr(\theta_A) = \Forr(\theta_B) = 0$, but any continuous path $\gamma(t)$ from $\theta_A$ to $\theta_B$ will have a different tessellation and therefore won't satisfy $\Forr( \gamma(t) ) = 0$. 
Moreover, one can build on this counter-example to show that not only the level sets are disconnected, but also that there exist poor local minima. Let $\theta'$ be a different set of parameters, and $Y' ~|~X \stackrel{d}{=} \Phi(X; \theta')$ be a different target distribution. Now consider the data distribution given by the mixture
$$X ~|~p(x) ~~,~z \sim \text{Bernoulli}(\pi)~,~Y ~|~X,z \stackrel{d}{=} z \Phi(X;\theta) + (1-z) \Phi(X; \theta')~.$$
By adjusting the mixture component $\pi$ we can clearly change the risk at $\theta$ and $\theta'$ and make them different, but we preserve the status of local minima of $\theta$ and $\theta'$. 
 
This illustrates an intrinsic difficulty in the optimization landscape if one is after \emph{universal} 
guarantees that do not depend upon the data distribution. This difficulty is non-existent in the linear case 
and not easy to exploit in mean-field approaches such as \cite{choromanska2015loss}, 
and shows that in general 
we should not expect to obtain connected level sets. However, 
connectedness can be recovered if one is willing to accept a small increase 
of energy and make some assumptions on the complexity of the regression task.
 Our main result shows that the amount by which the energy is 
allowed to increase is upper bounded by a quantity that trades-off model overparametrization 
and smoothness in the data distribution.

For that purpose, we start with a characterization of the oracle loss, and for simplicity let us assume 
$Y \in \R$ and let us first consider the case with a single hidden layer and $\ell_1$ regularization:
$\mathcal{R}(\theta) = \| \theta\|_1$.

\subsubsection{Preliminaries}
 Before proving our main result, we need to introduce  preliminary notation and results. 
We first describe the case with a single hidden layer of size $m$. 

We define
\begin{equation}
\label{bla2}
e(m) = \min_{W_1 \in \R^{m \times n}, \|W_1(i) \|_2 \leq 1, W_2 \in \R^m} \E\{ | \Phi(X; \theta) - Y|^2 \} + \kappa  \| W_2 \|_1~.
\end{equation}
to be the oracle risk using $m$ hidden units with norm $\leq 1$ and using sparse regression. 
It is a well known result by Hornik and Cybenko that a single hidden layer 
is a universal approximator under very mild assumptions, i.e. $\lim_{m \to \infty} e(m) = 0$.
This result merely states that our statistical setup is consistent, and it should not be 
surprising to the reader familiar with classic approximation theory.
 A more interesting question is the rate at which $e(m)$ decays, which depends 
on the smoothness of the joint density $(X, Y) \sim P$ relative to the nonlinear activation 
family we have chosen.

For convenience, we redefine $W = W_1$ and $\beta = W_2$ and
 $Z(W) = \max(0, W X)$. We also write $z(w) = \max(0, \langle w, X \rangle)$ where $(X, Y) \sim P$ and $w \in \R^N$ is any deterministic vector.
Let $\Sigma_X = \E_{P} XX^T \in \R^{N \times N}$ be the covariance operator of the random input $X$. We assume $\| \Sigma_X \| < \infty$. 

 A fundamental property that will be essential to our analysis is that, despite 
the fact that $Z$ is nonlinear, the quantity $[ w_1, w_2 ]_Z := \E_P \{ z(w_1) z(w_2) \} $ 
is locally equivalent to the linear metric $\langle w_1, w_2 \rangle_X = \E_P \{ w_1^T X X^T w_2 \} = \langle w_1, \Sigma_X w_2 \rangle$, and that the linearization error decreases with the angle between $w_1$ and $w_2$. Without loss of generality, we assume here that $\|w_1 \| = \| w_2 \| = 1$, and we write $\| w \|_Z^2 = \E \{ | z(w) |^2 \} $.
\begin{proposition}
\label{localdistprop}
Let $\alpha = \cos^{-1}( \langle w_1, w_2 \rangle )$ be the angle between unitary vectors $w_1$ and $w_2$ and let $w_m =  \frac{w_1 + w_2}{\| w_1 + w_2 \|}$ be their unitary bisector. 
Then
\begin{equation}
\label{localdisteq}
 \frac{1 + \cos \alpha}{2}  \| w_m  \|_Z^2 - 2 \| \Sigma_X \| \left( \frac{1-\cos \alpha}{2} + \sin^2 \alpha \right) \leq [ w_1, w_2 ]_Z \leq \frac{1+\cos \alpha}{2}  \| w_m  \|_Z^2 ~.
\end{equation}
\end{proposition} 
The term $\| \Sigma_X \| $ is overly pessimistic: we can replace it by the energy of $X$ projected into the subspace spanned by $w_1$ and $w_2$ (which is bounded by $2 \| \Sigma_X \|$). 
When $\alpha$ is small, a Taylor expansion of the trigonometric terms reveals that 
\begin{eqnarray*}
\frac{2}{3 \| \Sigma_X \|} \langle w_1, w_2 \rangle &=& \frac{2}{3 \| \Sigma_X \|} \cos \alpha = \frac{2}{3\| \Sigma_X \|}(1 - \frac{\alpha^2}{2} + O(\alpha^4)) \\ 
&\leq& ( 1 - \alpha^2/4)\| w_m \|_Z^2 - \| \Sigma_X \|( \alpha^2/4 + \alpha^2) + O(\alpha^4) \\
&\leq & [ w_1, w_2 ]_Z + O(\alpha^4) ~,
\end{eqnarray*}
and similarly 
$$[ w_1, w_2 ]_Z \leq \langle w_1, w_2 \rangle \| w_m \|_Z^2 \leq \| \Sigma_X\| \langle w_1, w_2 \rangle~.$$
The local behavior of parameters $w_1, w_2$ on our regression problem is thus equivalent to that of having a linear layer, provided $w_1$ and $w_2$ are sufficiently close to each other.
This result can be seen as a \emph{spoiler} of what is coming: increasing the hidden layer dimensionality $m$ will increase the chances to encounter pairs of vectors $w_1, w_2$ with small angle; and with it some hope of approximating the previous linear behavior thanks to the small linearization error. 

In order to control the connectedness, we need a last definition. Given a 
hidden layer of size $m$ with current parameters $W \in \R^{n \times m}$, we define a
``robust compressibility" factor as 
\begin{equation}
\label{compress}
\delta_W(n, \alpha; m) = \min_{ \|\gamma \|_0 \leq n, \sup_i |\angle(\tilde{w}_i, w_i)| \leq \alpha} \E \{| Y - \gamma Z(\tilde{W}) |^2 + \kappa \| \gamma \|_1  \}~,~(n \leq m)~.
\end{equation} 
This quantity thus measures how easily one can compress the current hidden layer representation, 
by keeping only a subset of $n$ its units, but allowing these units to move by a small amount controlled by $\alpha$. It is a form 
of $n$-width similar to Kolmogorov width \cite{donoho2006compressed} and is also related to robust sparse coding from \cite{tang2013compressed, ekanadham2011recovery}.


\subsubsection{Main result}

Our main result considers now a non-asymptotic scenario given by some fixed
size $m$ of the hidden layer. Given two parameter values $\theta^A = (W_1^A, W_2^A) \in \mathcal{W}$ 
and $\theta^B= (W_1^B, W_2^B)$ with $\Forr(\theta^{\{A,B\} } ) \leq \lambda$, 
we show that there exists a continuous path 
$\gamma: [0,1] \to \mathcal{W}$ connecting $\theta^A$ and $\theta^B$ 
such that its oracle risk is uniformly bounded by $\max(\lambda, \epsilon)$, where $\epsilon$ 
decreases with model overparametrization. 
\begin{theorem}
\label{maintheo}
For any $\theta^A, \theta^B \in \mathcal{W}$ and $\lambda \in \R$ satisfying $\Forr(\theta^{\{A,B\}}) \leq \lambda$, there exists a continuous path $\gamma: [0,1] \to \mathcal{W}$ such that
$\gamma(0) = \theta^A$, $\gamma(1) = \theta^B$ and
\begin{equation}
\Forr( \gamma(t) )  \leq \max( \lambda, \epsilon)~,\text{ with}
\end{equation}
\begin{equation}
\epsilon = \inf_{n, \alpha} \left(\max \left\{ e(n), \delta_{W_1^A}(m, 0; m ) , \delta_{W_1^A}(m-n, \alpha; m ) ,  \delta_{W_1^B}(m, 0; m ) , \delta_{W_1^B}(m-n, \alpha; m ) \right\} + C_1 \alpha  + O(\alpha^2) \right)~,
\end{equation}
%with 
%$$f(\alpha, M) = C_1 \alpha  + C_2 M \alpha^2 + O(\alpha^2)~,$$
%$$f(\alpha) = ~,$$
where $C_1$ is an absolute constant depending only on $\kappa$ and $P$.
\end{theorem}
Some remarks are in order. First, our regularization term is currently a mix between $\ell_2$ norm constraints on the first layer and $\ell_1$ norm constraints on the second layer. We believe this is an artifact of our proof technique, and we conjecture that more general regularizations yield similar results. Next, this result uses the data distribution through the oracle bound $e(m)$ and the covariance term. The 
extension to empirical risk is accomplished by replacing the probability measure $P$ by the empirical measure $\hat{P} = \frac{1}{L} \sum_l \delta\left( (x,y) - (x_l, y_l)\right) $. However, our asymptotic analysis has to be carefully reexamined to take into account and avoid the trivial regime when $M$ outgrows $L$.  
%\item Finally, another essential question is the extension of this result to several layers
%\end{itemize}
A consequence of Theorem \ref{maintheo} is that as $m$ increases, the model becomes asymptotically connected, as proven in the following corollary.
\begin{corollary}
\label{maincoro}
As $m$ increases, the energy gap $\epsilon$ goes to zero and therefore the level sets become connected at all energy levels.
%Here we use the fact that $\delta(\lambda m; \epsilon(m); m) \to 0$ as $m \to \infty$. (find the rate).
\end{corollary}
This is consistent with the overparametrization results from \cite{safran2015quality,shamir2} and the general common knowledge amongst deep learning practitioners. Our next sections explore this question, and refine it by considering not only topological properties but also some rough geometrical measure of the level sets.








