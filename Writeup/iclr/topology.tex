\section{Topology of Level Sets}

Let $P$ be a probability measure on a product space $\mathcal{X} \times \mathcal{Y}$, 
where we assume $\mathcal{X}$ and $\mathcal{Y}$ are Euclidean vector spaces for simplicity.
Let $\{ (x_i, y_i)\}_i$ be an iid sample of size $L$ drawn from $P$ defining the training set.
We consider the classic empirical risk minimization of the form
\begin{equation}
\label{emp_risk_min}
\Fem(\theta) = \frac{1}{L} \sum_{l=1}^L \| \Phi(x_i;\theta) - y_i \|^2~,
\end{equation}
where $\Phi(x ; \theta)$ encapsulates the feature representation 
that uses parameters $\theta \in \R^S$. In a deep neural network, this parameter
contains the weights and biases used in all layers.
For convenience, in our analysis we will also use the oracle risk minimization:
\begin{equation}
\label{risk_min}
\Forr(\theta) = \E_{(X,Y) \sim P} \| \Phi(X;\theta) - Y \|^2~,
\end{equation}

\subsection{Poor local minima characterization from topological conectedness}

We define the level set of $F(\theta)$ as 
\begin{equation}
\Omega_F(\lambda) = \{ \theta \in \R^S~;~F(\theta) \leq \lambda \}~. 
\end{equation}

The first question we study is the structure of critical points of $\Fem(\theta)$ and $\Forr(\theta)$
when $\Phi$ is a multilayer neural network. In particular, we are interested to know whether
$\Fem$ has local minima which are not global minima. This question is answered by 
knowing whether $\Omega_F(\lambda)$ is connected at each energy level $\lambda$:

\begin{proposition}
If $\Omega_F(\lambda)$ is connected for all $\lambda$ then every local minima of $F(\theta)$ is a global minima. 
\end{proposition}
{\it Proof:} Suppose that $\theta_1$ is a local minima and $\theta_2$ is a global minima, 
but $F(\theta_1) > F(\theta_2)$. If $\lambda = F(\theta_1)$, then clearly 
$\theta_1$ and $\theta_2$ both belong to $\Omega_F(\lambda)$. Suppose 
now that $\Omega_F(\lambda)$ 
is connected. Then we could find a smooth (i.e. continuous and differentiable) path $\gamma(t)$ 
with $\gamma(0) = \theta_1$, $\gamma(1)= \theta_2$ and $F(\gamma(t)) \leq \lambda = F(\theta_1)$.
In particular, as $t \to 0$, we have
\begin{eqnarray*}
F(\gamma(t)) &=& F(\theta_1) + t \langle \nabla F(\theta_1) , \dot{\gamma}(0) \rangle + \frac{t^2}{2} \left(\dot{\gamma}(0)^T H F(\theta_1) \dot{\gamma}(0) + \langle \nabla F(\theta_1), \ddot{\gamma}(0) \rangle \right) + o(t^2) \\
&=& F(\theta_1) +   \frac{t^2}{2}  \dot{\gamma}(0)^T H F(\theta_1) \dot{\gamma}(0)  + o(t^2) ~,
\end{eqnarray*}
which shows that $F(\gamma(t)) \leq F(\theta_1)$ for all $t$ is incompatible with $H(\theta_1) \succeq 0$ and therefore $\Omega_F(\lambda)$ cannot be connected $\square$.


\subsection{The Linear Case}

A particularly simple but insightful case is 
when $F$ is a multilayer network defined by
\begin{equation}
\label{linearcase}
\Phi(x;\theta) = W_K \dots W_1 x~,~\theta = (W_1, \dots, W_K)~.
\end{equation}
This model defines a non-convex (and non-concave) loss $\Fem(\theta)$.
It has been shown in \cite{ganguli} and \cite{linearcase} (concurrently with our work) that in this case, 
every local minima is a global minima.  
For completeness, we provide here an alternative proof of that result.

%For that purpose, let $W_1, W_2, \dots, W_K$ be weight matrices of sizes 
%$n_k \times n_{k+1}$, $k < K$. Assume first that $n_j \geq \min(n_1, n_K)$ for $j=2 \dots K-1$.
%and let us define the following multilinear regression problem:
%\begin{equation}
%\label{multilinloss}
%L_0(W_1, \dots, W_K) = \sum_i \| W_K, \dots W_1 x_i - y_i \|^2~,
%\end{equation}
%where $\{ (x_i, y_i)\,; x_i \in \mathbb{R}^{n_1}, y_i \in \mathbb{R}^{n_K} \}_i$ is a given 
%training set. 

We have the following result.
\begin{proposition}
\label{proplinear}
Let $W_1, W_2, \dots, W_K$ be weight matrices of sizes 
$n_k \times n_{k+1}$, $k < K$, and let $\Fem(\theta)$, $\Forr(\theta)$ 
denote the risk minimizations using $\Phi$ as in (\ref{linearcase}).
Assume that $n_j \geq \min(n_1, n_K)$ for $j=2 \dots K-1$ [TODO I think this is not necessary].
Then $\Omega_{\Fem}(\lambda)$ is connected for all $\lambda$, as well as $\Omega_{\Forr}$, 
and therefore there are no poor local minima.  
\end{proposition}
{\it Proof:} We proceed by induction over the number of layers $K$. 
For $K=1$, the loss $F(\theta)$ is convex. Let  $\theta_1$, $\theta_2$ be two arbitrary points 
in a level set $\Omega_\lambda$. Thus $L(\theta_1) \leq \lambda$ and $L(\theta_2) \leq \lambda$. We have
$$L( t \theta_1 + (1-t) \theta_2) \leq t L(\theta_1) + (1-t) L(\theta_2) \leq \lambda~,$$
and thus a linear path is sufficient in that case to connect $\theta_1$ and $\theta_2$.

Suppose the result is true for $K-1$. Let $\theta_1 = (W_1^1, \dots, W^1_K)$ and 
 $\theta_2 = (W_1^2, \dots, W^2_K)$ with $L(\theta_1) \leq \lambda$, $L(\theta_2) \leq \lambda$.
For each $W_1, \dots, W_K$, we denote $\tilde{W}_j = W_j$ for $j < K-1$ and
$\tilde{W}_{K-1} = W_K W_{K-1}$. By induction hypothesis, the 
loss expressed in terms of $\tilde{\theta} = (\tilde{W}_1, \dots, \tilde{W}_{K-1})$ is connected 
between $\tilde{\theta}_1$ and $\tilde{\theta_2}$. Let $\tilde{W}_{K-1}(t)$ the corresponding 
path projected in the last layer. We just need to produce a path in the variables $W_{K-1}(t)$, $W_K(t)$ 
such that (i) $W_{K-1}(0) = W_{K-1}^1$, $W_{K-1}(1) = W_{K-1}^2$, 
(ii) $W_{K}(0) = W_{K}^1$, $W_{K}(1) = W_{K}^2$, and 
(iii) $W_{K}(t) W_{K-1}(t) = \tilde{W}_{K-1}(t)$ for $t \in (0,1)$. 
We construct it as follows. Let 
$$W_{K}(t) = t W_{K}^2 + (1-t) W_{K}^1 + t (1-t) V~,$$
$$W_{K-1}(t) = W_{K}(t)^\dagger \tilde{W}_{K-1}(t)  ~,$$
where $W_{K}(t)^\dagger = ( W_{K}(t)^T W_{K}(t))^{-1} W_{K}(t)^T$ denotes the pseudoinverse 
and $V$ is a $n_{K-1} \times n_{K}$ matrix drawn from a iid distribution. 
Conditions (i) and (ii) are immediate from the definition, and condition (iii) results from the fact that 
$$W_{K}(t) W_{K}(t)^\dagger  = {\bf I}_{N_K}~,$$
since $W_K(t)$ has full rank for all $t \in (0,1)$. 
$\square$.

\subsection{Half-Rectified Nonlinear Case}

We now study the setting given by 
\begin{equation}
\label{relucase}
\Phi(x;\theta) = W_K \rho W_{K-1} \rho \dots \rho W_1 x~,~\theta = (W_1, \dots, W_K)~,
\end{equation}
where $\rho(z) = \max(0 ,z)$. 
The biases can be implemented by replacing the input vector $x$ 
with $\overline{x}=(x, 1)$ and by rebranding each parameter matrix as 
$$\overline{W}_i = \left( 
\begin{array}{c|c}
W_i & b_i \\
\hline 
0 & 1 
\end{array}
\right)~,$$
where $b_i$ contains the biases for each layer.	
For simplicity, we continue to use $W_i$ and $x$ in the following.

\subsubsection{Nonlinear models are generally disconnected}

One may wonder whether the same phenomena of global connectedness also holds 
in the half-rectified case. A simple motivating counterexample shows that this is not the case in 
general. Consider a simple setup with $X \in \R^2$ drawn from a mixture of two Gaussians $\mathcal{N}_{-1}$ 
and $\mathcal{N}_{1}$, and let $Y = (X-\mu_Z) \cdot Z $ , where $Z$ is the (hidden) mixture component taking $\{1,-1\}$ values.  Let 
$\hat{Y} = \Phi(X; \{ W_1, W_2\} )$ be a single-hidden layer ReLU network, with two hidden units, 
illustrated in Figure ??. 
%Since the model is homogeneous, one can think about $W_1$ as encoding two unitary vectors without loss of generality. 
Let $\theta^A$ be a configuration that bisects the two mixture components, 
and let $\theta^B$ the same configuration, but swapping the bisectrices. 
One can verify that they can both achieve arbitrarily small risk by letting the covariance of the mixture components go to $0$. 
However, any path that connects $\theta^A$ to $\theta^B$ 
must necessarily pass through a point in which $W_1$ has rank $1$, which leads to an estimator with risk at least $1/2$.  

In fact, it is easy to see that this counter-example can be extended to any generic half-rectified architecture, if one is 
allowed to adversarially design a data distribution. For any given $\Phi(X; \theta)$ with arbitrary architecture and current parameters 
$\theta = (W_i)$, let $\mathcal{P}_\theta=\{ \mathcal{A}_1, \dots, \mathcal{A}_S\}$ be the underlying tesselation of the input space given by our current choice of parameters; that is, $\Phi(X; \theta)$ is piece-wise linear and $\mathcal{P}_\theta$ contains those pieces. Now let 
$X$ be any arbitrary distribution with density $p(x) > 0$ for all $x \in \R^n$, for example a Gaussian, and let %$Y ~|~ \{X \in \mathcal{A}_s\} = s$. 
%It is the indicator function corresponding to the tesselation $\mathcal{P}$. 
$Y ~|~X = \Phi(X ; \theta)$~. Since $\Phi$ is invariant under permutations $\theta_\sigma$ of its hidden layers, it is easy to see that one can find two parameter values $\theta_A = \theta$ and $\theta_B = \theta_\sigma$ such that $\Forr(\theta_A) = \Forr(\theta_B) = 0$, but any continuous path $\gamma(t)$ from $\theta_A$ to $\theta_B$ will have a different tesselation and therefore won't satisfy $\Forr( \gamma(t) ) = 0$. 
 
This illustrates an intrinsic difficulty in the optimization landscape if one is after \emph{universal} 
guarantees that do not depend upon the data distribution. This difficulty is non-existent in the linear case 
and not easy to exploit in mean-field approaches such as \cite{choromaska}, but 
is easily detected as soon as one considers a non-linear model, and shows that in general 
we should not expect to obtain connected level sets. However, 
connectedness can be recovered if one is willing to accept a small increase 
of energy. Our main result shows that the amount by which the energy is 
allowed to increase is upper bounded by a quantity that trades-off model overparametrization 
and smoothness in the data distribution.

For that purpose, we start with a characterization of the oracle loss, and for simplicity let us assume 
$Y \in \R$ and let us first consider the case with a single hidden layer. 

\subsubsection{Preliminaries}
 Before proving our main result, we need to introduce  preliminary notation and results. 
We first describe the case with a single hidden layer of size $m$. 

We define
\begin{equation}
\label{bla2}
e(m) = \min_{W_1 \in \R^{m \times n}, W_2 \in \R^m} \E\{ | \Phi(X; \theta) - Y|^2 \} + \kappa \| W_2 \|^2~.
\end{equation}
to be the oracle risk using $m$ hidden units with Ridge regression. 
It is a well known result by Hornik and Cybenko that a single hidden layer 
is a universal approximator under very mild assumptions, i.e. $\lim_{m \to \infty} e(m) = 0$.
This result merely states that our statistical setup is consistent, and it should not be 
surprising to the reader familiar with classic approximation theory.
 A more interesting question is the rate at which $e(m)$ decays, which depends 
on the smoothness of the joint density $(X, Y) \sim P$ relative to the nonlinear activation 
family we have chosen.

For convenience, we redefine $W = W_1$ and $\beta = W_2$ and
 $Z(W) = \max(0, W X)$. We also write $z(w) = \max(0, \langle w, X \rangle)$ where $(X, Y) \sim P$ and $w \in \R^N$ is any deterministic vector.
Let $\Sigma_X = \E_{P} XX^T \in \R^{N \times N}$ be the covariance operator of the random input $X$. We assume $\| \Sigma_X \| < \infty$. 

 A fundamental property that will be essential to our analysis is that, despite 
the fact that $Z$ is nonlinear, the ``pseudo-metric" $\langle w_1, w_2 \rangle_Z := \E_P \{ z(w_1) z(w_2) \} $ 
is locally equivalent to the linear metric $\langle w_1, w_2 \rangle_X = \E_P \{ w_1^T X X^T w_2 \} = \langle w_1, \Sigma_X w_2 \rangle$, and that the linearization error decreases with the angle between $w_1$ and $w_2$. Without loss of generality, we assume here that $\|w_1 \| = \| w_2 \| = 1$, and we write $\| w \|_Z^2 = \E \{ | z(w) |^2 \} $.
\begin{proposition}
\label{localdistprop}
Let $\alpha = \cos^{-1}( \langle w_1, w_2 \rangle )$ be the angle between unitary vectors $w_1$ and $w_2$ and let $w_m =  \frac{w_1 + w_2}{\| w_1 + w_2 \|}$ be their unitary bisector. 
Then
\begin{equation}
\label{localdisteq}
 \frac{1 + \cos \alpha}{2}  \| w_m  \|_Z^2 - 2 \| \Sigma_X \| \left( \frac{1-\cos \alpha}{2} + \sin^2 \alpha \right) \leq \langle w_1, w_2 \rangle_Z \leq \frac{1+\cos \alpha}{2}  \| w_m  \|_Z^2 ~.
\end{equation}
\end{proposition} 
The term $\| \Sigma_X \| $ is overly pessimistic: we can replace it by the energy of $X$ projected into the subspace spanned by $w_1$ and $w_2$ (which is bounded by $2 \| \Sigma_X \|$). 
When $\alpha$ is small, a Taylor expansion of the trigonometric terms reveals that 
\begin{eqnarray*}
\frac{2}{3 \| \Sigma_X \|} \langle w_1, w_2 \rangle &=& \frac{2}{3 \| \Sigma_X \|} \cos \alpha = \frac{2}{3\| \Sigma_X \|}(1 - \frac{\alpha^2}{2} + O(\alpha^4)) \\ 
&\leq& ( 1 - \alpha^2/4)\| w_m \|_Z^2 - \| \Sigma_X \|( \alpha^2/4 + \alpha^2) + O(\alpha^4) \\
&\leq & \langle w_1, w_2 \rangle_Z + O(\alpha^4) ~,
\end{eqnarray*}
and similarly 
$$\langle w_1, w_2 \rangle_Z \leq \langle w_1, w_2 \rangle \| w_m \|_Z^2~.$$
The local behavior of parameters $w_1, w_2$ on our regression problem is thus equivalent to that of having a linear layer, provided $w_1$ and $w_2$ are sufficiently close to each other.
This result can be seen as a spoiler that increasing the hidden layer dimensionality $M$ will increase the chances to encounter pairs of vectors $w_1, w_2$ with small angle; and therefore some hope of approximating the previous linear behavior thanks to the small linearization error. 

In order to control the connectedness, we will also require another quantity. Given a 
hidden layer of size $m$ with current parameters $W \in \R^{m \times n}$, we define a
``robust compressibility" factor as 
\begin{equation}
\label{compress}
\delta_W(n, \alpha; m) = \min_{ \|\gamma \|_0 \leq n, \sup_i |\angle(\tilde{w}_i, w_i)| \leq \alpha} \E \{| Y - \gamma Z(\tilde{W}) |^2 + \kappa \| \gamma \|^2 \}~,~(n \leq m)~.
\end{equation} 
This quantity thus measures how easily one can compress the current hidden layer representation, 
by keeping only a subset of its units, but allowing these units to rotate by a small amount. It is a form 
of $n$-width similar to Kolmogorov width \cite{donoho}. 


\subsubsection{Main result}

Our main result considers now a non-asymptotic scenario given by some fixed
size $M$ of the hidden layer. Given two parameter values $\theta^A = (W_1^A, W_2^A) \in \mathcal{W}$ 
and $\theta^B= (W_1^B, W_2^B)$ with $\Forr(\theta^{\{A,B\} } ) \leq \lambda$, 
we show that there exists a continuous path 
$\gamma: [0,1] \to \mathcal{W}$ connecting $\theta^A$ and $\theta^B$ 
such that its oracle risk is uniformly bounded by $\max(\lambda, \epsilon)$, where $\epsilon$ 
decreases with model overparametrization. 
\begin{theorem}
\label{maintheo}
There exists a continuous path $\gamma: [0,1] \to \mathcal{W}$ such that
$\gamma(0) = \theta^A$, $\gamma(1) = \theta^B$ and
\begin{equation}
\Forr( \gamma(t) )  \leq \max( \lambda, \epsilon)~,\text{ with}
\end{equation}
\begin{equation}
\epsilon = \inf_{n, \alpha} \left(\max \left\{ e(n), \delta_{W_1^A}(M, 0; M ) , \delta_{W_1^A}(M-n, \alpha; M ) ,  \delta_{W_1^B}(M, 0; M ) , \delta_{W_1^B}(M-n, \alpha; M ) \right\} + \E(|Y|^2) \kappa^{-1} \alpha \right)~.
\end{equation}
\end{theorem}

\begin{corollary}
\label{maincoro}
If $M$ increases, the energy gap $\epsilon$ goes to zero.
Here we use the fact that $\delta(\lambda m; \epsilon(m); m) \to 0$ as $m \to \infty$. (find the rate).
\end{corollary}

\textbf{Remarks:}
\begin{itemize}
\item Ridge regression term.
\item Extension to empirical risk. It is straightforward just change the metric. 
\item Extension to several layers.
\end{itemize}

