%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:Intro}
%%%%%%%%%%%%%%%%%%%%%%

%\begin{itemize}
%\item Context of the problem
%\item Related work: Spin glass, recent results from Shamir. Gradient Descent converges to minimizers (Jordan Recht et al). 
%\item Topology of the level sets. Main result on connectedness of level sets.
%\item Geometry of the level sets. Algorithm to estimate the geodesics along level sets. Measure of curvature of these sets. 
%\end{itemize}
%
%------- 


Optimization is a critical component in deep learning, governing its success in different areas of computer vision, speech processing and natural language processing. The prevalent optimization strategy is Stochastic Gradient Descent, invented by Robbins and Munro in the 50s. The empirical performance of SGD on these models is better than one could expect in generic, arbitrary non-convex loss surfaces, often aided by modifications yielding significant speedups \cite{duchi2011adaptive, hinton2012lecture, ioffe2015batch, kingma2014adam}. This raises a number of theoretical questions as to why neural network optimization does not suffer in practice from poor local minima. 

%Exploration
%Topology: theoretical model. 
The loss surface of deep neural networks has recently attracted interest 
in the optimization and machine learning communities as a paradigmatic example of 
a hard, high-dimensional, non-convex problem. 
Recent work has explored models from statistical physics such as spin glasses \cite{choromanska2015loss}, 
in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model. Other authors have advocated 
that the real danger in high-dimensional setups are saddle points 
rather than poor local minima \cite{dauphin2014identifying}, although 
recent results rigorously establish that gradient descent does not 
get stuck on saddle points \cite{lee2016gradient} but merely slowed down. 
Other notable recent contributions are \cite{kawaguchi2016deep}, which further develops the spin-glass 
connection from \cite{choromanska2015loss}, and \cite{soudry2016no}, that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data). Lastly, the work \cite{safran2015quality} studies some topological 
properties of homogeneous nonlinear networks and shows how overparametrization acts upon these properties, and the pioneering \cite{shamir2} studied the distribution-specific hardness of optimizing non-convex objectives.

In this work, we do not make any linearity assumption and study conditions 
on the data distribution and model architecture that prevent the existence 
of bad local minima. 
The loss surface $F(\theta)$ of a given model can be expressed in terms of its level sets $\Omega_\lambda$, which contain for each energy level $\lambda$ all parameters $\theta$ yielding a loss smaller or equal than $\lambda$. A first question we address concerns the topology of these level sets, i.e. under which conditions they are connected. Connected level sets imply that one can always find a descent direction at each energy level, and therefore that no poor local minima can exist. In absence of nonlinearities, deep (linear) networks have connected level sets \cite{kawaguchi2016deep}. We first generalize this result to include ridge regression (in the two layer case) and provide an alternative, more direct proof of the general case. We then move to the half-rectified case and show that the topology is intrinsically different and clearly dependent on the interplay between data distribution and model architecture. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.

%Geometry: exploring loss surface
Beyond the question of whether the loss contains poor local minima or not, the immediate follow-up question that determines the convergence of algorithms in practice is the local conditioning of the loss surface. It is thus related not to the topology but to the shape or geometry of the level sets. As the energy level decays, one expects the level sets to exhibit more complex irregular structures, which correspond to regions where $F(\theta)$ has small curvature. In order to verify this intuition, we introduce an efficient algorithm to estimate the geometric regularity of these level sets by approximating geodesics of each level set starting at two random boundary points. Our algorithm uses dynamic programming and can be efficiently deployed to study mid-scale CNN architectures on MNIST, CIFAR-10 and RNN models on Penn Treebank next word prediction. 
Our empirical results show that these models have a nearly convex behavior up until their lowest test errors, with a single connected component that becomes more elongated as the energy decays. 
The rest of the paper is structured as follows. Section 2 presents our theoretical results on the topological connectedness of multilayer networks. Section 3 presents our path discovery algorithm and Section 4 covers the numerical experiments.













