%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:Intro}
%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item Context of the problem
\item Related work: Spin glass, recent results from Shamir. Gradient Descent converges to minimizers (Jordan Recht et al). 
\item Topology of the level sets. Main result on connectedness of level sets.
\item Geometry of the level sets. Algorithm to estimate the geodesics along level sets. Measure of curvature of these sets. 
\end{itemize}


------- 


Optimization is a critical component in deep learning, governing its success in different areas of computer vision, speech processing and natural language processing. The prevalent optimization strategy is Stochastic Gradient Descent, invented by Robbins and Munro in the 50s. On the one hand, the performance of SGD is better than one could expect 
in generic, arbitrary non-convex loss surfaces. On the other hand, one may wonder if this generic optimization strategy can be adapted to operate in the class of loss surfaces defined by deep neural networks. Some reasons to believe that such adaptation is possible come from the 
variety of modifications of SGD algorithms yielding significant speedups \cite{duchi2011adaptive, hinton2012lecture, ioffe2015batch, kingma2014adam}.
This raises a number of theoretical questions as to why neural network optimization does not suffer in practice from poor local minima. Likewise, it introduces opportunities to leverage training data into adapted optimization. 

%Exploration
%Topology: theoretical model. 
The loss surface of deep neural networks has recently attracted interest 
in the optimization and machine learning communities as a paradigmatic example of 
a hard, high-dimensional, non-convex problem. 
Recent work has explored models from statistical physics \cite{dauphin2014identifying}, such as spin glasses \cite{choromanska2015loss}, 
in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model.
In this proposal, we do not make any such assumption and study conditions 
on the data distribution and model architecture that prevent the existence 
of bad local minima. Together with 
recent results that rigorously establish that gradient descent does not 
get stuck on saddle points \cite{lee2016gradient}, our first objective will yield guarantees that gradient descent converges
to a global optimum in deep rectified networks. 

The loss surface $F(\theta)$ of a given model can be expressed in terms of its level sets $\Omega_\lambda$, which contain all parameters $\theta$ yielding a loss smaller or equal than the energy level $\lambda$: $\Omega_\lambda = \{ \theta ~;~F(\theta) \leq \lambda\}$. 
A first question we address concerns the topology of these level sets, i.e. under which conditions they are connected. Connected level sets imply that one can always find a descent direction at each energy level, and therefore that no poor local minima can exist. One can verify that linear neural networks (cascaded linear operators without point-wise nonlinearities), despite defining a non-convex loss $F(\theta)$, have connected level sets and therefore can be optimized globally using gradient descent strategies \cite{danieljoan}. It is also easy to verify that the non-linear case is not true in general \cite{shamir}, and our first objective is thus to characterize the interplay between data distribution, overparametrization and model architecture that is needed to obtain connected level sets \cite{danieljoan}.

%Geometry: exploring loss surface
Beyond the question of whether the loss contains poor local minima, the immediate follow-up question that determines the convergence of algorithms in practice is the local conditioning of the loss surface. It is thus related not to the topology but to the shape or geometry of the level sets. As the energy level decays, one expects the level sets to reveal more complex irregular structures, which correspond to regions where $F(\theta)$ has small curvature.