{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'multilayer_perceptron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a0ca35fd885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#synapse1 = 2*np.random.random((hidden_dim,hidden_dim2)) - 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#synapse2 = 2*np.random.random((hidden_dim2,1)) - 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mcopy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilayer_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'multilayer_perceptron' is not defined"
     ]
    }
   ],
   "source": [
    "#Imports and model parameters\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "#Simple network: Given three integers a,b,c, [-100,100] chooses three random x-values, and evaluates\n",
    "#the quadratic function a*x^2 + b*x + c at those values.\n",
    "\n",
    "import copy\n",
    "\n",
    "alpha,hidden_dim,hidden_dim2 = (.001,4,4)\n",
    "\n",
    "thresh = .04\n",
    "\n",
    "cost_thresh = 1.0\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # Guess quadratic function\n",
    "n_classes = 10 # \n",
    "#synapses = []\n",
    "models = []\n",
    "\n",
    "#Testing starting in the same place\n",
    "#synapse0 = 2*np.random.random((1,hidden_dim)) - 1\n",
    "#synapse1 = 2*np.random.random((hidden_dim,hidden_dim2)) - 1\n",
    "#synapse2 = 2*np.random.random((hidden_dim2,1)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function definitions\n",
    "\n",
    "def func(x,a,b,c):\n",
    "    return x*x*a + x*b + c\n",
    "\n",
    "def generatecandidate4(a,b,c,tot):\n",
    "    \n",
    "    candidate = [[np.random.random() for x in xrange(1)] for y in xrange(tot)]\n",
    "    candidatesolutions = [[func(x[0],a,b,c)] for x in candidate]\n",
    "    \n",
    "    return (candidate, candidatesolutions)\n",
    "\n",
    "def synapse_interpolate(synapse1, synapse2, t):\n",
    "    return (synapse2-synapse1)*t + synapse1\n",
    "\n",
    "def model_interpolate(w1,b1,w2,b2,t):\n",
    "    \n",
    "    m1w = w1\n",
    "    m1b = b1\n",
    "    m2w = w2 \n",
    "    m2b = b2\n",
    "    \n",
    "    mwi = [synapse_interpolate(m1we,m2we,t) for m1we, m2we in zip(m1w,m2w)]\n",
    "    mbi = [synapse_interpolate(m1be,m2be,t) for m1be, m2be in zip(m1b,m2b)]\n",
    "    \n",
    "    return mwi, mbi\n",
    "\n",
    "def InterpBeadError(w1,b1, w2,b2, write = False, name = \"00\"):\n",
    "    errors = []\n",
    "    \n",
    "    #xdat,ydat = generatecandidate4(.5, .25, .1, 1000)\n",
    "    \n",
    "    #xdat,ydat = mnist.train.next_batch(1000)\n",
    "    \n",
    "    xdat = mnist.test.images\n",
    "    ydat = mnist.test.labels\n",
    "    #xdat = np.array(xdat)\n",
    "    #ydat = np.array(ydat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for tt in xrange(20):\n",
    "        #print tt\n",
    "        #accuracy = 0.\n",
    "        t = tt/20.\n",
    "        thiserror = 0\n",
    "\n",
    "        #x0 = tf.placeholder(\"float\", [None, n_input])\n",
    "        #y0 = tf.placeholder(\"float\", [None, n_classes])\n",
    "        weights, biases = model_interpolate(w1,b1,w2,b2, t)\n",
    "        interp_model = multilayer_perceptron(w=weights, b=biases)\n",
    "        \n",
    "        with interp_model.g.as_default():\n",
    "            \n",
    "            #interp_model.UpdateWeights(weights, biases)\n",
    "\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, n_input])\n",
    "            y = tf.placeholder(\"float\", [None, n_classes])\n",
    "            pred = interp_model.predict(x)\n",
    "            init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print \"Accuracy:\", 1 - accuracy.eval({x: xdat, y: ydat}),\"\\t\",tt,weights[0][1][0],weights[0][1][1]\n",
    "                thiserror = 1 - accuracy.eval({x: xdat, y: ydat})\n",
    "\n",
    "\n",
    "        errors.append(thiserror)\n",
    "\n",
    "    if write == True:\n",
    "        with open(\"f\" + str(name) + \".out\",'w+') as f:\n",
    "            for e in errors:\n",
    "                f.write(str(e) + \"\\n\")\n",
    "    \n",
    "    return max(errors), np.argmax(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class definitions\n",
    "\n",
    "class multilayer_perceptron():\n",
    "    \n",
    "    #weights = {}\n",
    "    #biases = {}\n",
    "    \n",
    "    def __init__(self, w=0, b=0, ind='00'):\n",
    "        \n",
    "        self.index = ind #used for reading values from file\n",
    "        #See the filesystem convention below (is this really necessary?)\n",
    "        #I'm going to eschew writing to file for now because I'll be generating too many files\n",
    "        #Currently, the last value of the parameters is stored in self.params to be read\n",
    "        \n",
    "        learning_rate = 0.001\n",
    "        training_epochs = 15\n",
    "        batch_size = 100\n",
    "        display_step = 1\n",
    "\n",
    "        # Network Parameters\n",
    "        n_hidden_1 = 256 # 1st layer number of features\n",
    "        n_hidden_2 = 256 # 2nd layer number of features\n",
    "        n_input = 784 # Guess quadratic function\n",
    "        n_classes = 10 # \n",
    "        self.g = tf.Graph()\n",
    "        \n",
    "        \n",
    "        self.params = []\n",
    "        \n",
    "        with self.g.as_default():\n",
    "        \n",
    "            #Note that by default, weights and biases will be initialized to random normal dists\n",
    "            if w==0:\n",
    "                \n",
    "                self.weights = {\n",
    "                    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "                    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "                    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "                }\n",
    "                self.weightslist = [self.weights['h1'],self.weights['h2'],self.weights['out']]\n",
    "                self.biases = {\n",
    "                    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "                    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "                    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "                }\n",
    "                self.biaseslist = [self.biases['b1'],self.biases['b2'],self.biases['out']]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                self.weights = {\n",
    "                    'h1': tf.Variable(w[0]),\n",
    "                    'h2': tf.Variable(w[1]),\n",
    "                    'out': tf.Variable(w[2])\n",
    "                }\n",
    "                self.weightslist = [self.weights['h1'],self.weights['h2'],self.weights['out']]\n",
    "                self.biases = {\n",
    "                    'b1': tf.Variable(b[0]),\n",
    "                    'b2': tf.Variable(b[1]),\n",
    "                    'out': tf.Variable(b[2])\n",
    "                }\n",
    "                self.biaseslist = [self.biases['b1'],self.biases['b2'],self.biases['out']]\n",
    "            self.saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    def UpdateWeights(self, w, b):\n",
    "        with self.g.as_default():\n",
    "            self.weights = {\n",
    "                    'h1': tf.Variable(w[0]),\n",
    "                    'h2': tf.Variable(w[1]),\n",
    "                    'out': tf.Variable(w[2])\n",
    "                }\n",
    "            self.weightslist = [self.weights['h1'],self.weights['h2'],self.weights['out']]\n",
    "            self.biases = {\n",
    "                'b1': tf.Variable(b[0]),\n",
    "                'b2': tf.Variable(b[1]),\n",
    "                'out': tf.Variable(b[2])\n",
    "            }\n",
    "            self.biaseslist = [self.biases['b1'],self.biases['b2'],self.biases['out']]\n",
    "            \n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        with self.g.as_default():\n",
    "            layer_1 = tf.add(tf.matmul(x, self.weights['h1']), self.biases['b1'])\n",
    "            layer_1 = tf.nn.relu(layer_1)\n",
    "            # Hidden layer with RELU activation\n",
    "            layer_2 = tf.add(tf.matmul(layer_1, self.weights['h2']), self.biases['b2'])\n",
    "            layer_2 = tf.nn.relu(layer_2)\n",
    "            # Output layer with linear activation\n",
    "            out_layer = tf.matmul(layer_2, self.weights['out']) + self.biases['out']\n",
    "            return out_layer\n",
    "        \n",
    "    def ReturnParamsAsList(self):\n",
    "        \n",
    "        with self.g.as_default():\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                # Restore variables from disk\n",
    "                self.saver.restore(sess, \"/home/dfreeman/PythonFun/tmp/model\"+str(self.index)+\".ckpt\")                \n",
    "                return sess.run(self.weightslist), sess.run(self.biaseslist)\n",
    "\n",
    "        \n",
    "        \n",
    "class WeightString:\n",
    "    \n",
    "    def __init__(self, w1, b1, w2, b2, numbeads, threshold):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        #self.w2, self.b2 = m2.params\n",
    "        self.AllBeads = []\n",
    "\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.AllBeads.append([w1,b1])\n",
    "        \n",
    "        \n",
    "        for n in xrange(numbeads):\n",
    "            ws,bs = model_interpolate(w1,b1,w2,b2, (n + 1.)/(numbeads+1.))\n",
    "            self.AllBeads.append([ws,bs])\n",
    "            \n",
    "        self.AllBeads.append([w2,b2])\n",
    "        \n",
    "        \n",
    "        self.ConvergedList = [False for f in xrange(len(self.AllBeads))]\n",
    "        self.ConvergedList[0] = True\n",
    "        self.ConvergedList[-1] = True\n",
    "    \n",
    "    \n",
    "    def SpringNorm(self, order):\n",
    "        \n",
    "        total = 0.\n",
    "        \n",
    "        #Energy between mobile beads\n",
    "        for i,b in enumerate(self.AllBeads):\n",
    "            if i < len(self.AllBeads)-1:\n",
    "                #print \"Tallying energy between bead \" + str(i) + \" and bead \" + str(i+1)\n",
    "                subtotal = 0.\n",
    "                for j in xrange(len(b)):\n",
    "                    subtotal += np.linalg.norm(np.subtract(self.AllBeads[i][0][j],self.AllBeads[i+1][0][j]),ord=order)#/len(self.beads[0][j])\n",
    "                for j in xrange(len(b)):\n",
    "                    subtotal += np.linalg.norm(np.subtract(self.AllBeads[i][1][j],self.AllBeads[i+1][1][j]),ord=order)#/len(self.beads[0][j])\n",
    "                total+=subtotal\n",
    "        \n",
    "        return total#/len(self.beads)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def SGDBead(self, bead, thresh, maxindex):\n",
    "        \n",
    "        finalerror = 0.\n",
    "        \n",
    "        #thresh = .05\n",
    "\n",
    "        # Parameters\n",
    "        learning_rate = 0.01\n",
    "        training_epochs = 15\n",
    "        batch_size = 1000\n",
    "        display_step = 1\n",
    "        \n",
    "        curWeights, curBiases = self.AllBeads[bead]\n",
    "        test_model = multilayer_perceptron(w=curWeights, b=curBiases)\n",
    "\n",
    "        with test_model.g.as_default():\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, n_input])\n",
    "            y = tf.placeholder(\"float\", [None, n_classes])\n",
    "            pred = test_model.predict(x)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            init = tf.initialize_all_variables()\n",
    "            stopcond = True\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                xtest = mnist.test.images\n",
    "                ytest = mnist.test.labels\n",
    "                \n",
    "                thiserror = 0.\n",
    "                j = 0\n",
    "                while stopcond:\n",
    "                    for epoch in range(training_epochs):\n",
    "                        avg_cost = 0.\n",
    "                        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                        if (avg_cost > thresh or avg_cost == 0.) and stopcond:\n",
    "                        # Loop over all batches\n",
    "                            for i in range(total_batch):\n",
    "                                batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                                              y: batch_y})\n",
    "                                # Compute average loss\n",
    "                                avg_cost += c / total_batch\n",
    "                            # Display logs per epoch step\n",
    "                            #if epoch % display_step == 0:\n",
    "                            #    print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                            #        \"{:.9f}\".format(avg_cost)\n",
    "                            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                            # Calculate accuracy\n",
    "                            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                            #print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "                            thiserror = 1 - accuracy.eval({x: xtest, y: ytest})\n",
    "                            if thiserror < thresh:\n",
    "                                stopcond = False\n",
    "                    #print \"Optimization Finished!\"\n",
    "\n",
    "                    # Test model\n",
    "                    #correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    #correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    # Calculate accuracy\n",
    "                    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    #print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "\n",
    "                    #if (j%5000) == 0:\n",
    "                    #    print \"Error after \"+str(j)+\" iterations:\" + str(accuracy.eval({x: xtest, y: ytest}))\n",
    "\n",
    "                    finalerror = 1 - accuracy.eval({x: xtest, y: ytest})\n",
    "                    \n",
    "                    if finalerror < thresh or stopcond==False:# or j > maxindex:\n",
    "                        #print \"Changing stopcond!\"\n",
    "                        stopcond = False\n",
    "                        #print \"Final params:\"\n",
    "                        test_model.params = sess.run(test_model.weightslist), sess.run(test_model.biaseslist)\n",
    "                        self.AllBeads[bead]=test_model.params\n",
    "                        print \"Final bead error: \" + str(finalerror)\n",
    "                        \n",
    "                    j+=1\n",
    "\n",
    "            return finalerror\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Accuracy: 0.9118\n",
      "Error after 0 iterations:0.9118\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9316\n",
      "Error after 0 iterations:0.9316\n",
      "Optimization Finished!\n",
      "Accuracy: 0.94\n",
      "Error after 0 iterations:0.94\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9418\n",
      "Error after 0 iterations:0.9418\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9409\n",
      "Error after 0 iterations:0.9409\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9456\n",
      "Error after 0 iterations:0.9456\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9474\n",
      "Error after 0 iterations:0.9474\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9495\n",
      "Error after 0 iterations:0.9495\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9524\n",
      "Error after 0 iterations:0.9524\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9513\n",
      "Error after 0 iterations:0.9513\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9507\n",
      "Error after 0 iterations:0.9507\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9531\n",
      "Error after 0 iterations:0.9531\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9551\n",
      "Error after 0 iterations:0.9551\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9537\n",
      "Error after 0 iterations:0.9537\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9536\n",
      "Error after 0 iterations:0.9536\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9555\n",
      "Error after 0 iterations:0.9555\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9572\n",
      "Error after 0 iterations:0.9572\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9525\n",
      "Error after 0 iterations:0.9525\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9583\n",
      "Error after 0 iterations:0.9583\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9572\n",
      "Error after 0 iterations:0.9572\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9566\n",
      "Error after 0 iterations:0.9566\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9577\n",
      "Error after 0 iterations:0.9577\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9576\n",
      "Error after 0 iterations:0.9576\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9589\n",
      "Error after 0 iterations:0.9589\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9572\n",
      "Error after 0 iterations:0.9572\n",
      "Optimization Finished!\n",
      "Accuracy: 0.957\n",
      "Error after 0 iterations:0.957\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9559\n",
      "Error after 0 iterations:0.9559\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9598\n",
      "Error after 0 iterations:0.9598\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9602\n",
      "Error after 0 iterations:0.9602\n",
      "Final params:\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9152\n",
      "Error after 0 iterations:0.9152\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9331\n",
      "Error after 0 iterations:0.9331\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9381\n",
      "Error after 0 iterations:0.9381\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9423\n",
      "Error after 0 iterations:0.9423\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9439\n",
      "Error after 0 iterations:0.9439\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9478\n",
      "Error after 0 iterations:0.9478\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9481\n",
      "Error after 0 iterations:0.9481\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9488\n",
      "Error after 0 iterations:0.9488\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9518\n",
      "Error after 0 iterations:0.9518\n",
      "Optimization Finished!\n",
      "Accuracy: 0.951\n",
      "Error after 0 iterations:0.951\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9521\n",
      "Error after 0 iterations:0.9521\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9545\n",
      "Error after 0 iterations:0.9545\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9532\n",
      "Error after 0 iterations:0.9532\n",
      "Optimization Finished!\n",
      "Accuracy: 0.954\n",
      "Error after 0 iterations:0.954\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9579\n",
      "Error after 0 iterations:0.9579\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9555\n",
      "Error after 0 iterations:0.9555\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9558\n",
      "Error after 0 iterations:0.9558\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9534\n",
      "Error after 0 iterations:0.9534\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9553\n",
      "Error after 0 iterations:0.9553\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9545\n",
      "Error after 0 iterations:0.9545\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9562\n",
      "Error after 0 iterations:0.9562\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9582\n",
      "Error after 0 iterations:0.9582\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9568\n",
      "Error after 0 iterations:0.9568\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9562\n",
      "Error after 0 iterations:0.9562\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9589\n",
      "Error after 0 iterations:0.9589\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9578\n",
      "Error after 0 iterations:0.9578\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9584\n",
      "Error after 0 iterations:0.9584\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9593\n",
      "Error after 0 iterations:0.9593\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9579\n",
      "Error after 0 iterations:0.9579\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9608\n",
      "Error after 0 iterations:0.9608\n",
      "Final params:\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9175\n",
      "Error after 0 iterations:0.9175\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9297\n",
      "Error after 0 iterations:0.9297\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9362\n",
      "Error after 0 iterations:0.9362\n",
      "Optimization Finished!\n",
      "Accuracy: 0.942\n",
      "Error after 0 iterations:0.942\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9413\n",
      "Error after 0 iterations:0.9413\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9443\n",
      "Error after 0 iterations:0.9443\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9458\n",
      "Error after 0 iterations:0.9458\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9472\n",
      "Error after 0 iterations:0.9472\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9489\n",
      "Error after 0 iterations:0.9489\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9478\n",
      "Error after 0 iterations:0.9478\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9513\n",
      "Error after 0 iterations:0.9513\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9489\n",
      "Error after 0 iterations:0.9489\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9502\n",
      "Error after 0 iterations:0.9502\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9528\n",
      "Error after 0 iterations:0.9528\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9537\n",
      "Error after 0 iterations:0.9537\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9526\n",
      "Error after 0 iterations:0.9526\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9532\n",
      "Error after 0 iterations:0.9532\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9532\n",
      "Error after 0 iterations:0.9532\n",
      "Optimization Finished!\n",
      "Accuracy: 0.953\n",
      "Error after 0 iterations:0.953\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9528\n",
      "Error after 0 iterations:0.9528\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9532\n",
      "Error after 0 iterations:0.9532\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9562\n",
      "Error after 0 iterations:0.9562\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9538\n",
      "Error after 0 iterations:0.9538\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9544\n",
      "Error after 0 iterations:0.9544\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9526\n",
      "Error after 0 iterations:0.9526\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9563\n",
      "Error after 0 iterations:0.9563\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9535\n",
      "Error after 0 iterations:0.9535\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9552\n",
      "Error after 0 iterations:0.9552\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9557\n",
      "Error after 0 iterations:0.9557\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9546\n",
      "Error after 0 iterations:0.9546\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9578\n",
      "Error after 0 iterations:0.9578\n",
      "Optimization Finished!\n",
      "Accuracy: 0.954\n",
      "Error after 0 iterations:0.954\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9567\n",
      "Error after 0 iterations:0.9567\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9564\n",
      "Error after 0 iterations:0.9564\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9579\n",
      "Error after 0 iterations:0.9579\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9569\n",
      "Error after 0 iterations:0.9569\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9568\n",
      "Error after 0 iterations:0.9568\n",
      "Optimization Finished!\n",
      "Accuracy: 0.958\n",
      "Error after 0 iterations:0.958\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9575\n",
      "Error after 0 iterations:0.9575\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9607\n",
      "Error after 0 iterations:0.9607\n",
      "Final params:\n"
     ]
    }
   ],
   "source": [
    "#Model generation\n",
    "copy_model = multilayer_perceptron(ind=0)\n",
    "\n",
    "for ii in xrange(3):\n",
    "\n",
    "    '''weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }'''\n",
    "\n",
    "    # Construct model with different initial weights\n",
    "    test_model = multilayer_perceptron(ind=ii)\n",
    "    \n",
    "    #Construct model with same initial weights\n",
    "    #test_model = copy.copy(copy_model)\n",
    "    #test_model.index = ii\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print test_model.weights\n",
    "    \n",
    "\n",
    "    \n",
    "    models.append(test_model)\n",
    "    with test_model.g.as_default():\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, n_input])\n",
    "        y = tf.placeholder(\"float\", [None, n_classes])\n",
    "        pred = test_model.predict(x)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "        #remove the comment to get random initialization\n",
    "        stopcond = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            xtest = mnist.test.images\n",
    "            ytest = mnist.test.labels\n",
    "            while stopcond:\n",
    "                #print 'epoch:' + str(e)\n",
    "                #X = []\n",
    "                #y = []\n",
    "                j = 0\n",
    "                # Training cycle\n",
    "                for epoch in range(training_epochs):\n",
    "                    avg_cost = 0.\n",
    "                    total_batch = int(10000/batch_size)\n",
    "\n",
    "                    if (avg_cost > thresh or avg_cost == 0.) and stopcond:\n",
    "                    # Loop over all batches\n",
    "                        for i in range(total_batch):\n",
    "                            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                                          y: batch_y})\n",
    "                            # Compute average loss\n",
    "                            avg_cost += c / total_batch\n",
    "                        # Display logs per epoch step\n",
    "                        #if epoch % display_step == 0:\n",
    "                        #    #print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                        #    #    \"{:.9f}\".format(avg_cost)\n",
    "                        \n",
    "                        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                        # Calculate accuracy\n",
    "                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                        #print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "                        thiserror = 1 - accuracy.eval({x: xtest, y: ytest})\n",
    "                        if thiserror < thresh:\n",
    "                            stopcond = False\n",
    "                            \n",
    "                print \"Optimization Finished!\"\n",
    "\n",
    "                # Test model\n",
    "                #correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                # Calculate accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "\n",
    "                if (j%5000) == 0:\n",
    "                    print \"Error after \"+str(j)+\" iterations:\" + str(accuracy.eval({x: xtest, y: ytest}))\n",
    "\n",
    "                if 1 - accuracy.eval({x: xtest, y: ytest}) < thresh or stopcond == False:\n",
    "                    #print \"Changing stopcond!\"\n",
    "                    stopcond = False\n",
    "                    print \"Final params:\"\n",
    "                    test_model.params = sess.run(test_model.weightslist), sess.run(test_model.biaseslist)\n",
    "                    save_path = test_model.saver.save(sess,\"/home/dfreeman/PythonFun/tmp/model\" + str(ii) + \".ckpt\")\n",
    "                j+=1\n",
    "    #remove the comment to get random initialization\n",
    "\n",
    "    \n",
    "    #synapses.append([synapse_0,synapse_1,synapse_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0, 1]\n",
      "Final bead error: 0.0315999984741\n",
      "[True, True, True]\n",
      "Accuracy: 0.0397999882698 \t0 -0.235095 -1.66214\n",
      "Accuracy: 0.0414000153542 \t1 -0.196781 -1.6079\n",
      "Accuracy: 0.0428000092506 \t2 -0.158466 -1.55365\n",
      "Accuracy: 0.0442000031471 \t3 -0.120151 -1.4994\n",
      "Accuracy: 0.0467000007629 \t4 -0.0818361 -1.44516\n",
      "Accuracy: 0.0505999922752 \t5 -0.0435213 -1.39091\n",
      "Accuracy: 0.0552999973297 \t6 -0.00520651 -1.33666\n",
      "Accuracy: 0.0587000250816 \t7 0.0331083 -1.28242\n",
      "Accuracy: 0.0593000054359 \t8 0.0714231 -1.22817\n",
      "Accuracy: 0.0631999969482 \t9 0.109738 -1.17392\n",
      "Accuracy: 0.065800011158 \t10 0.148053 -1.11968\n",
      "Accuracy: 0.0667999982834 \t11 0.186368 -1.06543\n",
      "Accuracy: 0.0649999976158 \t12 0.224682 -1.01118\n",
      "Accuracy: 0.0608999729156 \t13 0.262997 -0.956938\n",
      "Accuracy: 0.0551999807358 \t14 0.301312 -0.902691\n",
      "Accuracy: 0.0485000014305 \t15 0.339627 -0.848445\n",
      "Accuracy: 0.0425999760628 \t16 0.377942 -0.794198\n",
      "Accuracy: 0.0389999747276 \t17 0.416256 -0.739951\n",
      "Accuracy: 0.0349000096321 \t18 0.454571 -0.685705\n",
      "Accuracy: 0.0333999991417 \t19 0.492886 -0.631458\n",
      "Accuracy: 0.0315999984741 \t0 0.531201 -0.577211\n",
      "Accuracy: 0.0325999855995 \t1 0.569516 -0.522965\n",
      "Accuracy: 0.0347999930382 \t2 0.60783 -0.468718\n",
      "Accuracy: 0.0388000011444 \t3 0.646145 -0.414472\n",
      "Accuracy: 0.0443999767303 \t4 0.68446 -0.360225\n",
      "Accuracy: 0.050400018692 \t5 0.722775 -0.305978\n",
      "Accuracy: 0.056500017643 \t6 0.76109 -0.251732\n",
      "Accuracy: 0.0637000203133 \t7 0.799404 -0.197485\n",
      "Accuracy: 0.069100022316 \t8 0.837719 -0.143238\n",
      "Accuracy: 0.0738999843597 \t9 0.876034 -0.0889918\n",
      "Accuracy: 0.0756999850273 \t10 0.914349 -0.0347452\n",
      "Accuracy: 0.0760999917984 \t11 0.952664 0.0195015\n",
      "Accuracy: 0.0758000016212 \t12 0.990978 0.0737481\n",
      "Accuracy: 0.0705000162125 \t13 1.02929 0.127995\n",
      "Accuracy: 0.0638999938965 \t14 1.06761 0.182241\n",
      "Accuracy: 0.0583999752998 \t15 1.10592 0.236488\n",
      "Accuracy: 0.0530999898911 \t16 1.14424 0.290735\n",
      "Accuracy: 0.0479000210762 \t17 1.18255 0.344981\n",
      "Accuracy: 0.04390001297 \t18 1.22087 0.399228\n",
      "Accuracy: 0.0408999919891 \t19 1.25918 0.453475\n",
      "[(0.06679999828338623, 11), (0.076099991798400879, 11)]\n",
      "[1, 0, 3, 2]\n",
      "Final bead error: 0.0315999984741\n",
      "Final bead error: 0.0318999886513\n",
      "[True, True, True, True, True]\n",
      "Accuracy: 0.0397999882698 \t0 -0.235095 -1.66214\n",
      "Accuracy: 0.0399000048637 \t1 -0.214022 -1.63231\n",
      "Accuracy: 0.0393000245094 \t2 -0.192949 -1.60247\n",
      "Accuracy: 0.0390999913216 \t3 -0.171876 -1.57264\n",
      "Accuracy: 0.0386999845505 \t4 -0.150803 -1.5428\n",
      "Accuracy: 0.0385000109673 \t5 -0.12973 -1.51297\n",
      "Accuracy: 0.0382999777794 \t6 -0.108656 -1.48313\n",
      "Accuracy: 0.0382999777794 \t7 -0.0875834 -1.45329\n",
      "Accuracy: 0.0379999876022 \t8 -0.0665102 -1.42346\n",
      "Accuracy: 0.037299990654 \t9 -0.0454371 -1.39362\n",
      "Accuracy: 0.0378999710083 \t10 -0.0243639 -1.36379\n",
      "Accuracy: 0.0367000102997 \t11 -0.00329077 -1.33395\n",
      "Accuracy: 0.0370000004768 \t12 0.0177824 -1.30412\n",
      "Accuracy: 0.0364000201225 \t13 0.0388555 -1.27428\n",
      "Accuracy: 0.0365999937057 \t14 0.0599286 -1.24445\n",
      "Accuracy: 0.0350999832153 \t15 0.0810018 -1.21461\n",
      "Accuracy: 0.0346999764442 \t16 0.102075 -1.18477\n",
      "Accuracy: 0.0336999893188 \t17 0.123148 -1.15494\n",
      "Accuracy: 0.0340999960899 \t18 0.144221 -1.1251\n",
      "Accuracy: 0.0324000120163 \t19 0.165294 -1.09527\n",
      "Accuracy: 0.0315999984741 \t0 0.186368 -1.06543\n",
      "Accuracy: 0.0310999751091 \t1 0.203609 -1.04102\n",
      "Accuracy: 0.0307999849319 \t2 0.220851 -1.01661\n",
      "Accuracy: 0.0310000181198 \t3 0.238093 -0.992198\n",
      "Accuracy: 0.0321999788284 \t4 0.255334 -0.967787\n",
      "Accuracy: 0.031300008297 \t5 0.272576 -0.943376\n",
      "Accuracy: 0.0310999751091 \t6 0.289818 -0.918965\n",
      "Accuracy: 0.0304999947548 \t7 0.307059 -0.894554\n",
      "Accuracy: 0.0303000211716 \t8 0.324301 -0.870143\n",
      "Accuracy: 0.0302000045776 \t9 0.341542 -0.845732\n",
      "Accuracy: 0.0302000045776 \t10 0.358784 -0.821321\n",
      "Accuracy: 0.0296000242233 \t11 0.376026 -0.79691\n",
      "Accuracy: 0.0300999879837 \t12 0.393267 -0.772499\n",
      "Accuracy: 0.031300008297 \t13 0.410509 -0.748088\n",
      "Accuracy: 0.0317000150681 \t14 0.427751 -0.723677\n",
      "Accuracy: 0.0315999984741 \t15 0.444992 -0.699266\n",
      "Accuracy: 0.0314000248909 \t16 0.462234 -0.674855\n",
      "Accuracy: 0.0317999720573 \t17 0.479476 -0.650444\n",
      "Accuracy: 0.0317000150681 \t18 0.496717 -0.626033\n",
      "Accuracy: 0.0315999984741 \t19 0.513959 -0.601622\n",
      "Accuracy: 0.0315999984741 \t0 0.531201 -0.577211\n",
      "Accuracy: 0.0314999818802 \t1 0.552274 -0.547376\n",
      "Accuracy: 0.0320000052452 \t2 0.573347 -0.51754\n",
      "Accuracy: 0.0325999855995 \t3 0.59442 -0.487705\n",
      "Accuracy: 0.0333999991417 \t4 0.615493 -0.457869\n",
      "Accuracy: 0.0340999960899 \t5 0.636567 -0.428033\n",
      "Accuracy: 0.0346999764442 \t6 0.65764 -0.398198\n",
      "Accuracy: 0.035000026226 \t7 0.678713 -0.368362\n",
      "Accuracy: 0.0357999801636 \t8 0.699786 -0.338526\n",
      "Accuracy: 0.034500002861 \t9 0.720859 -0.308691\n",
      "Accuracy: 0.0347999930382 \t10 0.741932 -0.278855\n",
      "Accuracy: 0.0350999832153 \t11 0.763005 -0.249019\n",
      "Accuracy: 0.0347999930382 \t12 0.784078 -0.219184\n",
      "Accuracy: 0.0338000059128 \t13 0.805152 -0.189348\n",
      "Accuracy: 0.0328999757767 \t14 0.826225 -0.159512\n",
      "Accuracy: 0.0328000187874 \t15 0.847298 -0.129677\n",
      "Accuracy: 0.0324000120163 \t16 0.868371 -0.0998411\n",
      "Accuracy: 0.0317000150681 \t17 0.889444 -0.0700054\n",
      "Accuracy: 0.0317999720573 \t18 0.910517 -0.0401698\n",
      "Accuracy: 0.0314000248909 \t19 0.93159 -0.0103341\n",
      "Accuracy: 0.0318999886513 \t0 0.952664 0.0195015\n",
      "Accuracy: 0.0310999751091 \t1 0.969905 0.0439125\n",
      "Accuracy: 0.0317000150681 \t2 0.987147 0.0683235\n",
      "Accuracy: 0.0317999720573 \t3 1.00439 0.0927345\n",
      "Accuracy: 0.0322999954224 \t4 1.02163 0.117145\n",
      "Accuracy: 0.0325999855995 \t5 1.03887 0.141556\n",
      "Accuracy: 0.0328999757767 \t6 1.05611 0.165967\n",
      "Accuracy: 0.0329999923706 \t7 1.07336 0.190378\n",
      "Accuracy: 0.0335999727249 \t8 1.0906 0.214789\n",
      "Accuracy: 0.0343000292778 \t9 1.10784 0.2392\n",
      "Accuracy: 0.0343999862671 \t10 1.12508 0.263611\n",
      "Accuracy: 0.034600019455 \t11 1.14232 0.288022\n",
      "Accuracy: 0.0347999930382 \t12 1.15956 0.312433\n",
      "Accuracy: 0.0353000164032 \t13 1.17681 0.336844\n",
      "Accuracy: 0.0357999801636 \t14 1.19405 0.361255\n",
      "Accuracy: 0.0368000268936 \t15 1.21129 0.385666\n",
      "Accuracy: 0.0371000170708 \t16 1.22853 0.410077\n",
      "Accuracy: 0.0378999710083 \t17 1.24577 0.434488\n",
      "Accuracy: 0.0386000275612 \t18 1.26301 0.458899\n",
      "Accuracy: 0.0383999943733 \t19 1.28026 0.48331\n",
      "[(0.039900004863739014, 1), (0.032199978828430176, 4), (0.035799980163574219, 8), (0.038600027561187744, 18)]\n",
      "[0, 1]\n",
      "Final bead error: 0.0318999886513\n",
      "[True, True, True]\n",
      "Accuracy: 0.0397999882698 \t0 -0.235095 -1.66214\n",
      "Accuracy: 0.041100025177 \t1 -0.2126 -1.61378\n",
      "Accuracy: 0.0429999828339 \t2 -0.190104 -1.56542\n",
      "Accuracy: 0.0475000143051 \t3 -0.167609 -1.51706\n",
      "Accuracy: 0.0508999824524 \t4 -0.145114 -1.4687\n",
      "Accuracy: 0.0547000169754 \t5 -0.122618 -1.42034\n",
      "Accuracy: 0.0608999729156 \t6 -0.100123 -1.37197\n",
      "Accuracy: 0.0665000081062 \t7 -0.0776271 -1.32361\n",
      "Accuracy: 0.0722000002861 \t8 -0.0551317 -1.27525\n",
      "Accuracy: 0.0785999894142 \t9 -0.0326362 -1.22689\n",
      "Accuracy: 0.0809999704361 \t10 -0.0101407 -1.17853\n",
      "Accuracy: 0.0795999765396 \t11 0.0123547 -1.13017\n",
      "Accuracy: 0.0763000249863 \t12 0.0348502 -1.0818\n",
      "Accuracy: 0.0702000260353 \t13 0.0573456 -1.03344\n",
      "Accuracy: 0.0612000226974 \t14 0.0798411 -0.985082\n",
      "Accuracy: 0.0547999739647 \t15 0.102337 -0.93672\n",
      "Accuracy: 0.0464000105858 \t16 0.124832 -0.888359\n",
      "Accuracy: 0.0401999950409 \t17 0.147327 -0.839997\n",
      "Accuracy: 0.0360999703407 \t18 0.169823 -0.791635\n",
      "Accuracy: 0.0336999893188 \t19 0.192318 -0.743274\n",
      "Accuracy: 0.0318999886513 \t0 0.214814 -0.694912\n",
      "Accuracy: 0.0321000218391 \t1 0.237309 -0.646551\n",
      "Accuracy: 0.0367000102997 \t2 0.259805 -0.598189\n",
      "Accuracy: 0.0404000282288 \t3 0.2823 -0.549827\n",
      "Accuracy: 0.0447000265121 \t4 0.304796 -0.501466\n",
      "Accuracy: 0.0505999922752 \t5 0.327291 -0.453104\n",
      "Accuracy: 0.0583000183105 \t6 0.349787 -0.404742\n",
      "Accuracy: 0.0643000006676 \t7 0.372282 -0.356381\n",
      "Accuracy: 0.0701000094414 \t8 0.394778 -0.308019\n",
      "Accuracy: 0.0730000138283 \t9 0.417273 -0.259658\n",
      "Accuracy: 0.0738999843597 \t10 0.439768 -0.211296\n",
      "Accuracy: 0.0738999843597 \t11 0.462264 -0.162934\n",
      "Accuracy: 0.0701000094414 \t12 0.484759 -0.114573\n",
      "Accuracy: 0.0651000142097 \t13 0.507255 -0.0662112\n",
      "Accuracy: 0.0600000023842 \t14 0.52975 -0.0178496\n",
      "Accuracy: 0.0530999898911 \t15 0.552246 0.030512\n",
      "Accuracy: 0.0480999946594 \t16 0.574741 0.0788736\n",
      "Accuracy: 0.0450000166893 \t17 0.597237 0.127235\n",
      "Accuracy: 0.0436000227928 \t18 0.619732 0.175597\n",
      "Accuracy: 0.0411999821663 \t19 0.642228 0.223958\n",
      "[(0.080999970436096191, 10), (0.073899984359741211, 10)]\n",
      "[1, 0, 3, 2]\n",
      "Final bead error: 0.031300008297\n",
      "Final bead error: 0.0304999947548\n",
      "[True, True, True, True, True]\n",
      "Accuracy: 0.0397999882698 \t0 -0.235095 -1.66214\n",
      "Accuracy: 0.0396999716759 \t1 -0.223848 -1.63796\n",
      "Accuracy: 0.0394999980927 \t2 -0.2126 -1.61378\n",
      "Accuracy: 0.0383999943733 \t3 -0.201352 -1.5896\n",
      "Accuracy: 0.0383999943733 \t4 -0.190104 -1.56542\n",
      "Accuracy: 0.0376999974251 \t5 -0.178857 -1.54124\n",
      "Accuracy: 0.0364999771118 \t6 -0.167609 -1.51706\n",
      "Accuracy: 0.0360000133514 \t7 -0.156361 -1.49288\n",
      "Accuracy: 0.0360999703407 \t8 -0.145114 -1.4687\n",
      "Accuracy: 0.0356000065804 \t9 -0.133866 -1.44452\n",
      "Accuracy: 0.0349000096321 \t10 -0.122618 -1.42034\n",
      "Accuracy: 0.0349000096321 \t11 -0.11137 -1.39616\n",
      "Accuracy: 0.0339000225067 \t12 -0.100123 -1.37197\n",
      "Accuracy: 0.0327000021935 \t13 -0.0888749 -1.34779\n",
      "Accuracy: 0.0318999886513 \t14 -0.0776271 -1.32361\n",
      "Accuracy: 0.031300008297 \t15 -0.0663794 -1.29943\n",
      "Accuracy: 0.031300008297 \t16 -0.0551317 -1.27525\n",
      "Accuracy: 0.0321999788284 \t17 -0.0438839 -1.25107\n",
      "Accuracy: 0.0322999954224 \t18 -0.0326362 -1.22689\n",
      "Accuracy: 0.0309000015259 \t19 -0.0213885 -1.20271\n",
      "Accuracy: 0.031300008297 \t0 -0.0101407 -1.17853\n",
      "Accuracy: 0.031199991703 \t1 0.00110698 -1.15435\n",
      "Accuracy: 0.0321999788284 \t2 0.0123547 -1.13017\n",
      "Accuracy: 0.0320000052452 \t3 0.0236024 -1.10599\n",
      "Accuracy: 0.0325000286102 \t4 0.0348502 -1.0818\n",
      "Accuracy: 0.0327000021935 \t5 0.0460979 -1.05762\n",
      "Accuracy: 0.0339000225067 \t6 0.0573456 -1.03344\n",
      "Accuracy: 0.0338000059128 \t7 0.0685934 -1.00926\n",
      "Accuracy: 0.0333999991417 \t8 0.0798411 -0.985082\n",
      "Accuracy: 0.033999979496 \t9 0.0910888 -0.960901\n",
      "Accuracy: 0.0333999991417 \t10 0.102337 -0.93672\n",
      "Accuracy: 0.0336999893188 \t11 0.113584 -0.912539\n",
      "Accuracy: 0.0335000157356 \t12 0.124832 -0.888359\n",
      "Accuracy: 0.0332000255585 \t13 0.13608 -0.864178\n",
      "Accuracy: 0.0325999855995 \t14 0.147327 -0.839997\n",
      "Accuracy: 0.0328999757767 \t15 0.158575 -0.815816\n",
      "Accuracy: 0.0328000187874 \t16 0.169823 -0.791635\n",
      "Accuracy: 0.0333999991417 \t17 0.181071 -0.767455\n",
      "Accuracy: 0.0325999855995 \t18 0.192318 -0.743274\n",
      "Accuracy: 0.0321000218391 \t19 0.203566 -0.719093\n",
      "Accuracy: 0.0318999886513 \t0 0.214814 -0.694912\n",
      "Accuracy: 0.0310999751091 \t1 0.226062 -0.670731\n",
      "Accuracy: 0.0314000248909 \t2 0.237309 -0.646551\n",
      "Accuracy: 0.0314999818802 \t3 0.248557 -0.62237\n",
      "Accuracy: 0.0307999849319 \t4 0.259805 -0.598189\n",
      "Accuracy: 0.0296999812126 \t5 0.271053 -0.574008\n",
      "Accuracy: 0.0291000008583 \t6 0.2823 -0.549827\n",
      "Accuracy: 0.0296999812126 \t7 0.293548 -0.525647\n",
      "Accuracy: 0.0303999781609 \t8 0.304796 -0.501466\n",
      "Accuracy: 0.0304999947548 \t9 0.316043 -0.477285\n",
      "Accuracy: 0.0317999720573 \t10 0.327291 -0.453104\n",
      "Accuracy: 0.0318999886513 \t11 0.338539 -0.428923\n",
      "Accuracy: 0.0321999788284 \t12 0.349787 -0.404742\n",
      "Accuracy: 0.031199991703 \t13 0.361034 -0.380562\n",
      "Accuracy: 0.0320000052452 \t14 0.372282 -0.356381\n",
      "Accuracy: 0.0322999954224 \t15 0.38353 -0.3322\n",
      "Accuracy: 0.0317000150681 \t16 0.394778 -0.308019\n",
      "Accuracy: 0.0314999818802 \t17 0.406025 -0.283838\n",
      "Accuracy: 0.0310999751091 \t18 0.417273 -0.259658\n",
      "Accuracy: 0.0297999978065 \t19 0.428521 -0.235477\n",
      "Accuracy: 0.0304999947548 \t0 0.439768 -0.211296\n",
      "Accuracy: 0.0303000211716 \t1 0.451016 -0.187115\n",
      "Accuracy: 0.0307000279427 \t2 0.462264 -0.162934\n",
      "Accuracy: 0.0317999720573 \t3 0.473512 -0.138754\n",
      "Accuracy: 0.0314999818802 \t4 0.484759 -0.114573\n",
      "Accuracy: 0.0331000089645 \t5 0.496007 -0.090392\n",
      "Accuracy: 0.0332000255585 \t6 0.507255 -0.0662112\n",
      "Accuracy: 0.0343000292778 \t7 0.518503 -0.0420304\n",
      "Accuracy: 0.0339000225067 \t8 0.52975 -0.0178496\n",
      "Accuracy: 0.0346999764442 \t9 0.540998 0.00633118\n",
      "Accuracy: 0.035000026226 \t10 0.552246 0.030512\n",
      "Accuracy: 0.0353000164032 \t11 0.563494 0.0546928\n",
      "Accuracy: 0.0356000065804 \t12 0.574741 0.0788736\n",
      "Accuracy: 0.0358999967575 \t13 0.585989 0.103054\n",
      "Accuracy: 0.0365999937057 \t14 0.597237 0.127235\n",
      "Accuracy: 0.0370000004768 \t15 0.608484 0.151416\n",
      "Accuracy: 0.0370000004768 \t16 0.619732 0.175597\n",
      "Accuracy: 0.037299990654 \t17 0.63098 0.199778\n",
      "Accuracy: 0.0382999777794 \t18 0.642228 0.223958\n",
      "Accuracy: 0.0386999845505 \t19 0.653475 0.248139\n",
      "[(0.039799988269805908, 0), (0.033999979496002197, 9), (0.032299995422363281, 15), (0.038699984550476074, 19)]\n",
      "1\n",
      "2\n",
      "Thresh: 0.04\n",
      "Comps: 1\n",
      "***\n",
      "174.168085814\n",
      "3.02714133263\n"
     ]
    }
   ],
   "source": [
    "#Connected components search\n",
    "\n",
    "\n",
    "#Used for softening the training criteria.  There's some fuzz required due to the difference in \n",
    "#training error between test and training\n",
    "thresh_multiplier = 1.1\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "connecteddict = {}\n",
    "for i1 in xrange(len(models)):\n",
    "    connecteddict[i1] = 'not connected'\n",
    "\n",
    "\n",
    "for i1 in xrange(len(models)):\n",
    "    print i1\n",
    "    for i2 in xrange(len(models)):\n",
    "        \n",
    "        if i2 > i1 and ((connecteddict[i1] != connecteddict[i2]) or (connecteddict[i1] == 'not connected' or connecteddict[i2] == 'not connected')) :\n",
    "            #print \"slow1?\"\n",
    "            #print i1,i2\n",
    "            #print models[0]\n",
    "            #print models[1]\n",
    "            #print models[0].params\n",
    "            #print models[1].params\n",
    "            test = WeightString(models[i1].params[0],models[i1].params[1],models[i2].params[0],models[i2].params[1],1,1)\n",
    "\n",
    "            training_threshold = thresh\n",
    "\n",
    "            depth = 0\n",
    "            d_max = 10\n",
    "\n",
    "            #Check error between beads\n",
    "            #Alg: for each bead at depth i, SGD until converged.\n",
    "            #For beads with max error along path too large, add another bead between them, repeat\n",
    "\n",
    "            \n",
    "            #Keeps track of which indices to check the interpbeaderror between\n",
    "            newindices = [0,1]\n",
    "            \n",
    "            while (depth < d_max):\n",
    "                print newindices\n",
    "                #print \"slow2?\"\n",
    "                #X, y = GenTest(X,y)\n",
    "                counter = 0\n",
    "\n",
    "                for i,c in enumerate(test.ConvergedList):\n",
    "                    if c == False:\n",
    "                        #print \"slow3?\"\n",
    "                        error = test.SGDBead(i, .8*training_threshold, 20)\n",
    "                        #print \"slow4?\"\n",
    "                            #if counter%5000==0:\n",
    "                            #    print counter\n",
    "                            #    print error\n",
    "                        test.ConvergedList[i] = True\n",
    "\n",
    "                print test.ConvergedList\n",
    "\n",
    "                interperrors = []\n",
    "                interp_bead_indices = []\n",
    "                for b in xrange(len(test.AllBeads)-1):\n",
    "                    if b in newindices:\n",
    "                        e = InterpBeadError(test.AllBeads[b][0],test.AllBeads[b][1], test.AllBeads[b+1][0], test.AllBeads[b+1][1])\n",
    "\n",
    "                        interperrors.append(e)\n",
    "                        interp_bead_indices.append(b)\n",
    "                print interperrors\n",
    "\n",
    "                if max([ee[0] for ee in interperrors]) < thresh_multiplier*training_threshold:\n",
    "                    depth = 2*d_max\n",
    "                    #print test.ConvergedList\n",
    "                    #print test.SpringNorm(2)\n",
    "                    #print \"Done!\"\n",
    "\n",
    "                else:\n",
    "                    del newindices[:]\n",
    "                    #Interperrors stores the maximum error on the path between beads\n",
    "                    #shift index to account for added beads\n",
    "                    shift = 0\n",
    "                    for i, ie in enumerate(interperrors):\n",
    "                        if ie[0] > thresh_multiplier*training_threshold:\n",
    "                            k = interp_bead_indices[i]\n",
    "                            \n",
    "                            ws,bs = model_interpolate(test.AllBeads[k+shift][0],test.AllBeads[k+shift][1],\\\n",
    "                                                      test.AllBeads[k+shift+1][0],test.AllBeads[k+shift+1][1],\\\n",
    "                                                      ie[1]/20.)\n",
    "                            \n",
    "                            test.AllBeads.insert(k+shift+1,[ws,bs])\n",
    "                            test.ConvergedList.insert(k+shift+1, False)\n",
    "                            newindices.append(k+shift+1)\n",
    "                            newindices.append(k+shift)\n",
    "                            shift+=1\n",
    "                            #print test.ConvergedList\n",
    "                            #print test.SpringNorm(2)\n",
    "\n",
    "\n",
    "                    #print d_max\n",
    "                    depth += 1\n",
    "            if depth == 2*d_max:\n",
    "                results.append([i1,i2,test.SpringNorm(2),\"Connected\"])\n",
    "                if connecteddict[i1] == 'not connected' and connecteddict[i2] == 'not connected':\n",
    "                    connecteddict[i1] = i1\n",
    "                    connecteddict[i2] = i1\n",
    "\n",
    "                if connecteddict[i1] == 'not connected':\n",
    "                    connecteddict[i1] = connecteddict[i2]\n",
    "                else:\n",
    "                    if connecteddict[i2] == 'not connected':\n",
    "                        connecteddict[i2] = connecteddict[i1]\n",
    "                    else:\n",
    "                        if connecteddict[i1] != 'not connected' and connecteddict[i2] != 'not connected':\n",
    "                            hold = connecteddict[i2]\n",
    "                            connecteddict[i2] = connecteddict[i1]\n",
    "                            for h in xrange(len(models)):\n",
    "                                if connecteddict[h] == hold:\n",
    "                                    connecteddict[h] = connecteddict[i1]\n",
    "                    \n",
    "            else:\n",
    "                results.append([i1,i2,test.SpringNorm(2),\"Disconnected\"])\n",
    "            #print results[-1]\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\n",
    "uniquecomps = []\n",
    "totalcomps = 0\n",
    "for i in xrange(len(models)):\n",
    "    if not (connecteddict[i] in uniquecomps):\n",
    "        uniquecomps.append(connecteddict[i])\n",
    "    \n",
    "    if connecteddict[i] == 'not connected':\n",
    "        totalcomps += 1\n",
    "        \n",
    "    #print i,connecteddict[i]\n",
    "\n",
    "notconoffset = 0\n",
    "\n",
    "if 'not connected' in uniquecomps:\n",
    "    notconoffset = -1\n",
    "    \n",
    "print \"Thresh: \" + str(thresh)\n",
    "print \"Comps: \" + str(len(uniquecomps) + notconoffset + totalcomps)\n",
    "\n",
    "\n",
    "\n",
    "#for i in xrange(len(synapses)):\n",
    "#    print connecteddict[i]\n",
    "\n",
    "connsum = []\n",
    "for r in results:\n",
    "    if r[3] == \"Connected\":\n",
    "        connsum.append(r[2])\n",
    "        #print r[2]\n",
    "        \n",
    "print \"***\"\n",
    "print np.average(connsum)\n",
    "print np.std(connsum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.multilayer_perceptron instance at 0x7fc3f3ce0518>,\n",
       " <__main__.multilayer_perceptron instance at 0x7fc3f3ce00e0>,\n",
       " <__main__.multilayer_perceptron instance at 0x7fc3f366cb00>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.AllBeads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0494999885559 \t0 0.15941 1.24111\n",
      "Accuracy: 0.0486999750137 \t1 0.16302 1.23706\n",
      "Accuracy: 0.0487999916077 \t2 0.16663 1.23302\n",
      "Accuracy: 0.0494999885559 \t3 0.17024 1.22898\n",
      "Accuracy: 0.0493000149727 \t4 0.173849 1.22494\n",
      "Accuracy: 0.0496000051498 \t5 0.177459 1.22089\n",
      "Accuracy: 0.0490999817848 \t6 0.181069 1.21685\n",
      "Accuracy: 0.0490999817848 \t7 0.184679 1.21281\n",
      "Accuracy: 0.0486999750137 \t8 0.188288 1.20877\n",
      "Accuracy: 0.0483000278473 \t9 0.191898 1.20473\n",
      "Accuracy: 0.0482000112534 \t10 0.195508 1.20068\n",
      "Accuracy: 0.0483999848366 \t11 0.199118 1.19664\n",
      "Accuracy: 0.0476999878883 \t12 0.202728 1.1926\n",
      "Accuracy: 0.0475999712944 \t13 0.206337 1.18856\n",
      "Accuracy: 0.0476999878883 \t14 0.209947 1.18451\n",
      "Accuracy: 0.0475999712944 \t15 0.213557 1.18047\n",
      "Accuracy: 0.0475000143051 \t16 0.217167 1.17643\n",
      "Accuracy: 0.0472999811172 \t17 0.220776 1.17239\n",
      "Accuracy: 0.047100007534 \t18 0.224386 1.16835\n",
      "Accuracy: 0.047200024128 \t19 0.227996 1.1643\n",
      "Accuracy: 0.047200024128 \t20 0.231606 1.16026\n",
      "Accuracy: 0.0468999743462 \t21 0.235215 1.15622\n",
      "Accuracy: 0.047200024128 \t22 0.238825 1.15218\n",
      "Accuracy: 0.047200024128 \t23 0.242435 1.14813\n",
      "Accuracy: 0.0473999977112 \t24 0.246045 1.14409\n",
      "Accuracy: 0.047200024128 \t25 0.249654 1.14005\n",
      "Accuracy: 0.047200024128 \t26 0.253264 1.13601\n",
      "Accuracy: 0.047200024128 \t27 0.256874 1.13196\n",
      "Accuracy: 0.0468999743462 \t28 0.260484 1.12792\n",
      "Accuracy: 0.0468000173569 \t29 0.264093 1.12388\n",
      "Accuracy: 0.0460000038147 \t30 0.267703 1.11984\n",
      "Accuracy: 0.0457999706268 \t31 0.271313 1.1158\n",
      "Accuracy: 0.0458999872208 \t32 0.274923 1.11175\n",
      "Accuracy: 0.0454000234604 \t33 0.278532 1.10771\n",
      "Accuracy: 0.0453000068665 \t34 0.282142 1.10367\n",
      "Accuracy: 0.0450000166893 \t35 0.285752 1.09963\n",
      "Accuracy: 0.0450000166893 \t36 0.289362 1.09558\n",
      "Accuracy: 0.0450999736786 \t37 0.292971 1.09154\n",
      "Accuracy: 0.0450999736786 \t38 0.296581 1.0875\n",
      "Accuracy: 0.0449000000954 \t39 0.300191 1.08346\n",
      "Accuracy: 0.0451999902725 \t40 0.303801 1.07942\n",
      "Accuracy: 0.0450000166893 \t41 0.30741 1.07537\n",
      "Accuracy: 0.0447999835014 \t42 0.31102 1.07133\n",
      "Accuracy: 0.0450000166893 \t43 0.31463 1.06729\n",
      "Accuracy: 0.0447999835014 \t44 0.31824 1.06325\n",
      "Accuracy: 0.0447999835014 \t45 0.321849 1.0592\n",
      "Accuracy: 0.0447000265121 \t46 0.325459 1.05516\n",
      "Accuracy: 0.0447000265121 \t47 0.329069 1.05112\n",
      "Accuracy: 0.0450000166893 \t48 0.332679 1.04708\n",
      "Accuracy: 0.0447000265121 \t49 0.336288 1.04303\n",
      "Accuracy: 0.04390001297 \t50 0.339898 1.03899\n",
      "Accuracy: 0.0436999797821 \t51 0.343508 1.03495\n",
      "Accuracy: 0.0436999797821 \t52 0.347118 1.03091\n",
      "Accuracy: 0.0440000295639 \t53 0.350727 1.02687\n",
      "Accuracy: 0.043799996376 \t54 0.354337 1.02282\n",
      "Accuracy: 0.0430999994278 \t55 0.357947 1.01878\n",
      "Accuracy: 0.0426999926567 \t56 0.361557 1.01474\n",
      "Accuracy: 0.0419999957085 \t57 0.365166 1.0107\n",
      "Accuracy: 0.0421000123024 \t58 0.368776 1.00665\n",
      "Accuracy: 0.0418000221252 \t59 0.372386 1.00261\n",
      "Accuracy: 0.0419999957085 \t60 0.375996 0.99857\n",
      "Accuracy: 0.0422000288963 \t61 0.379606 0.994528\n",
      "Accuracy: 0.0419999957085 \t62 0.383215 0.990485\n",
      "Accuracy: 0.0422999858856 \t63 0.386825 0.986443\n",
      "Accuracy: 0.0418000221252 \t64 0.390435 0.982401\n",
      "Accuracy: 0.041100025177 \t65 0.394045 0.978359\n",
      "Accuracy: 0.0407999753952 \t66 0.397654 0.974316\n",
      "Accuracy: 0.0408999919891 \t67 0.401264 0.970274\n",
      "Accuracy: 0.040499985218 \t68 0.404874 0.966232\n",
      "Accuracy: 0.0401999950409 \t69 0.408484 0.96219\n",
      "Accuracy: 0.0396000146866 \t70 0.412093 0.958147\n",
      "Accuracy: 0.0390999913216 \t71 0.415703 0.954105\n",
      "Accuracy: 0.0393999814987 \t72 0.419313 0.950063\n",
      "Accuracy: 0.0390999913216 \t73 0.422923 0.946021\n",
      "Accuracy: 0.0390999913216 \t74 0.426532 0.941978\n",
      "Accuracy: 0.0389999747276 \t75 0.430142 0.937936\n",
      "Accuracy: 0.0389999747276 \t76 0.433752 0.933894\n",
      "Accuracy: 0.0388000011444 \t77 0.437362 0.929852\n",
      "Accuracy: 0.0389999747276 \t78 0.440971 0.925809\n",
      "Accuracy: 0.0392000079155 \t79 0.444581 0.921767\n",
      "Accuracy: 0.0396000146866 \t80 0.448191 0.917725\n",
      "Accuracy: 0.0399000048637 \t81 0.451801 0.913682\n",
      "Accuracy: 0.0396999716759 \t82 0.45541 0.90964\n",
      "Accuracy: 0.0392000079155 \t83 0.45902 0.905598\n",
      "Accuracy: 0.0392000079155 \t84 0.46263 0.901556\n",
      "Accuracy: 0.0389999747276 \t85 0.46624 0.897513\n",
      "Accuracy: 0.0389999747276 \t86 0.469849 0.893471\n",
      "Accuracy: 0.0389000177383 \t87 0.473459 0.889429\n",
      "Accuracy: 0.0389000177383 \t88 0.477069 0.885387\n",
      "Accuracy: 0.0396000146866 \t89 0.480679 0.881344\n",
      "Accuracy: 0.0396000146866 \t90 0.484288 0.877302\n",
      "Accuracy: 0.0389000177383 \t91 0.487898 0.87326\n",
      "Accuracy: 0.0389999747276 \t92 0.491508 0.869218\n",
      "Accuracy: 0.0390999913216 \t93 0.495118 0.865175\n",
      "Accuracy: 0.0390999913216 \t94 0.498727 0.861133\n",
      "Accuracy: 0.0392000079155 \t95 0.502337 0.857091\n",
      "Accuracy: 0.0389000177383 \t96 0.505947 0.853049\n",
      "Accuracy: 0.0389000177383 \t97 0.509557 0.849006\n",
      "Accuracy: 0.0386000275612 \t98 0.513166 0.844964\n",
      "Accuracy: 0.0385000109673 \t99 0.516776 0.840922\n",
      "Accuracy: 0.0386999845505 \t0 0.520386 0.836879\n",
      "Accuracy: 0.0389000177383 \t1 0.523461 0.833436\n",
      "Accuracy: 0.0390999913216 \t2 0.526536 0.829993\n",
      "Accuracy: 0.0389999747276 \t3 0.529611 0.826549\n",
      "Accuracy: 0.0389000177383 \t4 0.532686 0.823106\n",
      "Accuracy: 0.0388000011444 \t5 0.535761 0.819662\n",
      "Accuracy: 0.0389000177383 \t6 0.538836 0.816219\n",
      "Accuracy: 0.0390999913216 \t7 0.541911 0.812776\n",
      "Accuracy: 0.0389999747276 \t8 0.544986 0.809332\n",
      "Accuracy: 0.0392000079155 \t9 0.548061 0.805889\n",
      "Accuracy: 0.0392000079155 \t10 0.551136 0.802445\n",
      "Accuracy: 0.0392000079155 \t11 0.554211 0.799002\n",
      "Accuracy: 0.0396999716759 \t12 0.557286 0.795559\n",
      "Accuracy: 0.0396000146866 \t13 0.560361 0.792115\n",
      "Accuracy: 0.0396999716759 \t14 0.563436 0.788672\n",
      "Accuracy: 0.0392000079155 \t15 0.566511 0.785228\n",
      "Accuracy: 0.0393000245094 \t16 0.569586 0.781785\n",
      "Accuracy: 0.0393999814987 \t17 0.572661 0.778341\n",
      "Accuracy: 0.0396999716759 \t18 0.575736 0.774898\n",
      "Accuracy: 0.0397999882698 \t19 0.578811 0.771455\n",
      "Accuracy: 0.0399000048637 \t20 0.581886 0.768011\n",
      "Accuracy: 0.040099978447 \t21 0.58496 0.764568\n",
      "Accuracy: 0.040600001812 \t22 0.588035 0.761124\n",
      "Accuracy: 0.0410000085831 \t23 0.59111 0.757681\n",
      "Accuracy: 0.0407999753952 \t24 0.594185 0.754238\n",
      "Accuracy: 0.040499985218 \t25 0.59726 0.750794\n",
      "Accuracy: 0.0408999919891 \t26 0.600335 0.747351\n",
      "Accuracy: 0.040600001812 \t27 0.60341 0.743907\n",
      "Accuracy: 0.0403000116348 \t28 0.606485 0.740464\n",
      "Accuracy: 0.0401999950409 \t29 0.60956 0.737021\n",
      "Accuracy: 0.0401999950409 \t30 0.612635 0.733577\n",
      "Accuracy: 0.0407999753952 \t31 0.61571 0.730134\n",
      "Accuracy: 0.041100025177 \t32 0.618785 0.72669\n",
      "Accuracy: 0.0411999821663 \t33 0.62186 0.723247\n",
      "Accuracy: 0.0412999987602 \t34 0.624935 0.719803\n",
      "Accuracy: 0.041100025177 \t35 0.62801 0.71636\n",
      "Accuracy: 0.0414000153542 \t36 0.631085 0.712917\n",
      "Accuracy: 0.0415999889374 \t37 0.63416 0.709473\n",
      "Accuracy: 0.0417000055313 \t38 0.637235 0.70603\n",
      "Accuracy: 0.0418000221252 \t39 0.64031 0.702586\n",
      "Accuracy: 0.0417000055313 \t40 0.643385 0.699143\n",
      "Accuracy: 0.0419999957085 \t41 0.64646 0.6957\n",
      "Accuracy: 0.0419999957085 \t42 0.649535 0.692256\n",
      "Accuracy: 0.0421000123024 \t43 0.65261 0.688813\n",
      "Accuracy: 0.0422000288963 \t44 0.655685 0.685369\n",
      "Accuracy: 0.0429999828339 \t45 0.65876 0.681926\n",
      "Accuracy: 0.0432000160217 \t46 0.661835 0.678483\n",
      "Accuracy: 0.0435000061989 \t47 0.66491 0.675039\n",
      "Accuracy: 0.043299973011 \t48 0.667985 0.671596\n",
      "Accuracy: 0.0429000258446 \t49 0.67106 0.668152\n",
      "Accuracy: 0.0426999926567 \t50 0.674135 0.664709\n",
      "Accuracy: 0.0425000190735 \t51 0.67721 0.661265\n",
      "Accuracy: 0.0429999828339 \t52 0.680285 0.657822\n",
      "Accuracy: 0.0432000160217 \t53 0.68336 0.654379\n",
      "Accuracy: 0.0435000061989 \t54 0.686435 0.650935\n",
      "Accuracy: 0.0429999828339 \t55 0.68951 0.647492\n",
      "Accuracy: 0.0425999760628 \t56 0.692585 0.644048\n",
      "Accuracy: 0.0425000190735 \t57 0.69566 0.640605\n",
      "Accuracy: 0.0422999858856 \t58 0.698735 0.637162\n",
      "Accuracy: 0.0425000190735 \t59 0.70181 0.633718\n",
      "Accuracy: 0.0422999858856 \t60 0.704885 0.630275\n",
      "Accuracy: 0.0422999858856 \t61 0.70796 0.626831\n",
      "Accuracy: 0.0422000288963 \t62 0.711035 0.623388\n",
      "Accuracy: 0.0419999957085 \t63 0.71411 0.619945\n",
      "Accuracy: 0.0421000123024 \t64 0.717184 0.616501\n",
      "Accuracy: 0.0418000221252 \t65 0.720259 0.613058\n",
      "Accuracy: 0.0419999957085 \t66 0.723334 0.609614\n",
      "Accuracy: 0.0418999791145 \t67 0.726409 0.606171\n",
      "Accuracy: 0.0414000153542 \t68 0.729484 0.602728\n",
      "Accuracy: 0.0414000153542 \t69 0.732559 0.599284\n",
      "Accuracy: 0.0411999821663 \t70 0.735634 0.595841\n",
      "Accuracy: 0.0410000085831 \t71 0.738709 0.592397\n",
      "Accuracy: 0.0410000085831 \t72 0.741784 0.588954\n",
      "Accuracy: 0.0410000085831 \t73 0.744859 0.58551\n",
      "Accuracy: 0.0410000085831 \t74 0.747934 0.582067\n",
      "Accuracy: 0.0404000282288 \t75 0.751009 0.578624\n",
      "Accuracy: 0.0403000116348 \t76 0.754084 0.57518\n",
      "Accuracy: 0.0399000048637 \t77 0.757159 0.571737\n",
      "Accuracy: 0.0397999882698 \t78 0.760234 0.568293\n",
      "Accuracy: 0.0400000214577 \t79 0.763309 0.56485\n",
      "Accuracy: 0.0400000214577 \t80 0.766384 0.561407\n",
      "Accuracy: 0.0397999882698 \t81 0.769459 0.557963\n",
      "Accuracy: 0.0396000146866 \t82 0.772534 0.55452\n",
      "Accuracy: 0.0397999882698 \t83 0.775609 0.551076\n",
      "Accuracy: 0.0399000048637 \t84 0.778684 0.547633\n",
      "Accuracy: 0.040099978447 \t85 0.781759 0.54419\n",
      "Accuracy: 0.040099978447 \t86 0.784834 0.540746\n",
      "Accuracy: 0.0396000146866 \t87 0.787909 0.537303\n",
      "Accuracy: 0.0396000146866 \t88 0.790984 0.533859\n",
      "Accuracy: 0.0396000146866 \t89 0.794059 0.530416\n",
      "Accuracy: 0.0393000245094 \t90 0.797134 0.526973\n",
      "Accuracy: 0.0390999913216 \t91 0.800209 0.523529\n",
      "Accuracy: 0.0390999913216 \t92 0.803284 0.520086\n",
      "Accuracy: 0.0389000177383 \t93 0.806359 0.516642\n",
      "Accuracy: 0.0390999913216 \t94 0.809434 0.513199\n",
      "Accuracy: 0.0392000079155 \t95 0.812509 0.509755\n",
      "Accuracy: 0.0392000079155 \t96 0.815584 0.506312\n",
      "Accuracy: 0.0390999913216 \t97 0.818659 0.502869\n",
      "Accuracy: 0.0388000011444 \t98 0.821734 0.499425\n",
      "Accuracy: 0.0388000011444 \t99 0.824809 0.495982\n",
      "Accuracy: 0.0389999747276 \t0 0.827884 0.492538\n",
      "Accuracy: 0.0389999747276 \t1 0.830691 0.489394\n",
      "Accuracy: 0.0389000177383 \t2 0.833499 0.48625\n",
      "Accuracy: 0.0386000275612 \t3 0.836306 0.483106\n",
      "Accuracy: 0.0388000011444 \t4 0.839114 0.479962\n",
      "Accuracy: 0.0382999777794 \t5 0.841922 0.476818\n",
      "Accuracy: 0.0389000177383 \t6 0.844729 0.473674\n",
      "Accuracy: 0.0390999913216 \t7 0.847537 0.470531\n",
      "Accuracy: 0.0390999913216 \t8 0.850344 0.467387\n",
      "Accuracy: 0.0392000079155 \t9 0.853152 0.464243\n",
      "Accuracy: 0.0393999814987 \t10 0.85596 0.461099\n",
      "Accuracy: 0.0393999814987 \t11 0.858767 0.457955\n",
      "Accuracy: 0.0397999882698 \t12 0.861575 0.454811\n",
      "Accuracy: 0.0397999882698 \t13 0.864382 0.451667\n",
      "Accuracy: 0.0397999882698 \t14 0.86719 0.448523\n",
      "Accuracy: 0.0396000146866 \t15 0.869997 0.445379\n",
      "Accuracy: 0.0396000146866 \t16 0.872805 0.442235\n",
      "Accuracy: 0.0396000146866 \t17 0.875613 0.439091\n",
      "Accuracy: 0.0397999882698 \t18 0.87842 0.435947\n",
      "Accuracy: 0.0401999950409 \t19 0.881228 0.432803\n",
      "Accuracy: 0.040499985218 \t20 0.884035 0.429659\n",
      "Accuracy: 0.040499985218 \t21 0.886843 0.426515\n",
      "Accuracy: 0.0407999753952 \t22 0.889651 0.423371\n",
      "Accuracy: 0.0407000184059 \t23 0.892458 0.420227\n",
      "Accuracy: 0.0404000282288 \t24 0.895266 0.417083\n",
      "Accuracy: 0.0407000184059 \t25 0.898073 0.413939\n",
      "Accuracy: 0.0407999753952 \t26 0.900881 0.410795\n",
      "Accuracy: 0.0408999919891 \t27 0.903688 0.407651\n",
      "Accuracy: 0.0410000085831 \t28 0.906496 0.404507\n",
      "Accuracy: 0.0412999987602 \t29 0.909304 0.401363\n",
      "Accuracy: 0.041100025177 \t30 0.912111 0.398219\n",
      "Accuracy: 0.0407999753952 \t31 0.914919 0.395075\n",
      "Accuracy: 0.0407999753952 \t32 0.917726 0.391931\n",
      "Accuracy: 0.0408999919891 \t33 0.920534 0.388787\n",
      "Accuracy: 0.0404000282288 \t34 0.923342 0.385643\n",
      "Accuracy: 0.0401999950409 \t35 0.926149 0.382499\n",
      "Accuracy: 0.0403000116348 \t36 0.928957 0.379355\n",
      "Accuracy: 0.0400000214577 \t37 0.931764 0.376211\n",
      "Accuracy: 0.040099978447 \t38 0.934572 0.373067\n",
      "Accuracy: 0.040099978447 \t39 0.93738 0.369923\n",
      "Accuracy: 0.0396000146866 \t40 0.940187 0.366779\n",
      "Accuracy: 0.0396999716759 \t41 0.942995 0.363635\n",
      "Accuracy: 0.0396999716759 \t42 0.945802 0.360491\n",
      "Accuracy: 0.0397999882698 \t43 0.94861 0.357347\n",
      "Accuracy: 0.0399000048637 \t44 0.951417 0.354203\n",
      "Accuracy: 0.0401999950409 \t45 0.954225 0.351059\n",
      "Accuracy: 0.0401999950409 \t46 0.957033 0.347915\n",
      "Accuracy: 0.0400000214577 \t47 0.95984 0.344771\n",
      "Accuracy: 0.0394999980927 \t48 0.962648 0.341627\n",
      "Accuracy: 0.0394999980927 \t49 0.965455 0.338483\n",
      "Accuracy: 0.0399000048637 \t50 0.968263 0.335339\n",
      "Accuracy: 0.0400000214577 \t51 0.971071 0.332195\n",
      "Accuracy: 0.0401999950409 \t52 0.973878 0.329051\n",
      "Accuracy: 0.0400000214577 \t53 0.976686 0.325907\n",
      "Accuracy: 0.040099978447 \t54 0.979493 0.322763\n",
      "Accuracy: 0.0401999950409 \t55 0.982301 0.319619\n",
      "Accuracy: 0.0401999950409 \t56 0.985108 0.316475\n",
      "Accuracy: 0.040600001812 \t57 0.987916 0.313331\n",
      "Accuracy: 0.040499985218 \t58 0.990724 0.310187\n",
      "Accuracy: 0.040499985218 \t59 0.993531 0.307043\n",
      "Accuracy: 0.040499985218 \t60 0.996339 0.303899\n",
      "Accuracy: 0.0404000282288 \t61 0.999146 0.300755\n",
      "Accuracy: 0.0404000282288 \t62 1.00195 0.297611\n",
      "Accuracy: 0.040600001812 \t63 1.00476 0.294467\n",
      "Accuracy: 0.0407000184059 \t64 1.00757 0.291323\n",
      "Accuracy: 0.0407000184059 \t65 1.01038 0.288179\n",
      "Accuracy: 0.040600001812 \t66 1.01318 0.285035\n",
      "Accuracy: 0.0403000116348 \t67 1.01599 0.281891\n",
      "Accuracy: 0.0401999950409 \t68 1.0188 0.278747\n",
      "Accuracy: 0.0397999882698 \t69 1.02161 0.275604\n",
      "Accuracy: 0.0396000146866 \t70 1.02441 0.27246\n",
      "Accuracy: 0.0393999814987 \t71 1.02722 0.269316\n",
      "Accuracy: 0.0394999980927 \t72 1.03003 0.266172\n",
      "Accuracy: 0.0393999814987 \t73 1.03284 0.263028\n",
      "Accuracy: 0.0390999913216 \t74 1.03565 0.259884\n",
      "Accuracy: 0.0389999747276 \t75 1.03845 0.25674\n",
      "Accuracy: 0.0386999845505 \t76 1.04126 0.253596\n",
      "Accuracy: 0.0381000041962 \t77 1.04407 0.250452\n",
      "Accuracy: 0.0382999777794 \t78 1.04688 0.247308\n",
      "Accuracy: 0.0385000109673 \t79 1.04968 0.244164\n",
      "Accuracy: 0.0385000109673 \t80 1.05249 0.24102\n",
      "Accuracy: 0.0382999777794 \t81 1.0553 0.237876\n",
      "Accuracy: 0.0388000011444 \t82 1.05811 0.234732\n",
      "Accuracy: 0.0386000275612 \t83 1.06091 0.231588\n",
      "Accuracy: 0.0383999943733 \t84 1.06372 0.228444\n",
      "Accuracy: 0.0381000041962 \t85 1.06653 0.2253\n",
      "Accuracy: 0.037800014019 \t86 1.06934 0.222156\n",
      "Accuracy: 0.0376999974251 \t87 1.07214 0.219012\n",
      "Accuracy: 0.0376999974251 \t88 1.07495 0.215868\n",
      "Accuracy: 0.037800014019 \t89 1.07776 0.212724\n",
      "Accuracy: 0.037800014019 \t90 1.08057 0.20958\n",
      "Accuracy: 0.0379999876022 \t91 1.08337 0.206436\n",
      "Accuracy: 0.0381000041962 \t92 1.08618 0.203292\n",
      "Accuracy: 0.0382999777794 \t93 1.08899 0.200148\n",
      "Accuracy: 0.0385000109673 \t94 1.0918 0.197004\n",
      "Accuracy: 0.0386000275612 \t95 1.0946 0.19386\n",
      "Accuracy: 0.0386999845505 \t96 1.09741 0.190716\n",
      "Accuracy: 0.0388000011444 \t97 1.10022 0.187572\n",
      "Accuracy: 0.0392000079155 \t98 1.10303 0.184428\n",
      "Accuracy: 0.0389000177383 \t99 1.10583 0.181284\n",
      "Accuracy: 0.0393999814987 \t0 1.10864 0.17814\n",
      "Accuracy: 0.0394999980927 \t1 1.11252 0.173798\n",
      "Accuracy: 0.0396999716759 \t2 1.1164 0.169457\n",
      "Accuracy: 0.0396999716759 \t3 1.12027 0.165115\n",
      "Accuracy: 0.0404000282288 \t4 1.12415 0.160773\n",
      "Accuracy: 0.0408999919891 \t5 1.12803 0.156432\n",
      "Accuracy: 0.041100025177 \t6 1.13191 0.15209\n",
      "Accuracy: 0.0412999987602 \t7 1.13578 0.147748\n",
      "Accuracy: 0.0415999889374 \t8 1.13966 0.143406\n",
      "Accuracy: 0.0412999987602 \t9 1.14354 0.139065\n",
      "Accuracy: 0.0419999957085 \t10 1.14741 0.134723\n",
      "Accuracy: 0.0422999858856 \t11 1.15129 0.130381\n",
      "Accuracy: 0.0425999760628 \t12 1.15517 0.12604\n",
      "Accuracy: 0.0425999760628 \t13 1.15905 0.121698\n",
      "Accuracy: 0.0428000092506 \t14 1.16292 0.117356\n",
      "Accuracy: 0.0432000160217 \t15 1.1668 0.113015\n",
      "Accuracy: 0.043799996376 \t16 1.17068 0.108673\n",
      "Accuracy: 0.0442000031471 \t17 1.17455 0.104331\n",
      "Accuracy: 0.0443999767303 \t18 1.17843 0.0999896\n",
      "Accuracy: 0.0446000099182 \t19 1.18231 0.0956479\n",
      "Accuracy: 0.0450999736786 \t20 1.18619 0.0913062\n",
      "Accuracy: 0.0454999804497 \t21 1.19006 0.0869645\n",
      "Accuracy: 0.0455999970436 \t22 1.19394 0.0826228\n",
      "Accuracy: 0.046599984169 \t23 1.19782 0.0782811\n",
      "Accuracy: 0.0469999909401 \t24 1.20169 0.0739394\n",
      "Accuracy: 0.0469999909401 \t25 1.20557 0.0695977\n",
      "Accuracy: 0.0476999878883 \t26 1.20945 0.065256\n",
      "Accuracy: 0.0479999780655 \t27 1.21333 0.0609143\n",
      "Accuracy: 0.0483000278473 \t28 1.2172 0.0565726\n",
      "Accuracy: 0.0483000278473 \t29 1.22108 0.052231\n",
      "Accuracy: 0.0479999780655 \t30 1.22496 0.0478893\n",
      "Accuracy: 0.0480999946594 \t31 1.22883 0.0435476\n",
      "Accuracy: 0.0480999946594 \t32 1.23271 0.0392059\n",
      "Accuracy: 0.0489000082016 \t33 1.23659 0.0348642\n",
      "Accuracy: 0.0486999750137 \t34 1.24047 0.0305225\n",
      "Accuracy: 0.0490999817848 \t35 1.24434 0.0261808\n",
      "Accuracy: 0.049399971962 \t36 1.24822 0.0218391\n",
      "Accuracy: 0.0497000217438 \t37 1.2521 0.0174974\n",
      "Accuracy: 0.0497999787331 \t38 1.25597 0.0131557\n",
      "Accuracy: 0.0497999787331 \t39 1.25985 0.00881404\n",
      "Accuracy: 0.0501000285149 \t40 1.26373 0.00447235\n",
      "Accuracy: 0.050400018692 \t41 1.26761 0.000130653\n",
      "Accuracy: 0.0500000119209 \t42 1.27148 -0.00421104\n",
      "Accuracy: 0.049899995327 \t43 1.27536 -0.00855273\n",
      "Accuracy: 0.0501999855042 \t44 1.27924 -0.0128944\n",
      "Accuracy: 0.0503000020981 \t45 1.28311 -0.0172361\n",
      "Accuracy: 0.0496000051498 \t46 1.28699 -0.0215778\n",
      "Accuracy: 0.0496000051498 \t47 1.29087 -0.0259195\n",
      "Accuracy: 0.049899995327 \t48 1.29475 -0.0302612\n",
      "Accuracy: 0.0497999787331 \t49 1.29862 -0.0346029\n",
      "Accuracy: 0.0490000247955 \t50 1.3025 -0.0389446\n",
      "Accuracy: 0.0490000247955 \t51 1.30638 -0.0432863\n",
      "Accuracy: 0.0494999885559 \t52 1.31025 -0.0476279\n",
      "Accuracy: 0.049399971962 \t53 1.31413 -0.0519696\n",
      "Accuracy: 0.0496000051498 \t54 1.31801 -0.0563114\n",
      "Accuracy: 0.049899995327 \t55 1.32189 -0.060653\n",
      "Accuracy: 0.0497999787331 \t56 1.32576 -0.0649947\n",
      "Accuracy: 0.0490000247955 \t57 1.32964 -0.0693364\n",
      "Accuracy: 0.049399971962 \t58 1.33352 -0.0736781\n",
      "Accuracy: 0.0490999817848 \t59 1.33739 -0.0780198\n",
      "Accuracy: 0.0489000082016 \t60 1.34127 -0.0823615\n",
      "Accuracy: 0.0490000247955 \t61 1.34515 -0.0867032\n",
      "Accuracy: 0.0487999916077 \t62 1.34903 -0.0910449\n",
      "Accuracy: 0.0487999916077 \t63 1.3529 -0.0953866\n",
      "Accuracy: 0.0493000149727 \t64 1.35678 -0.0997283\n",
      "Accuracy: 0.0491999983788 \t65 1.36066 -0.10407\n",
      "Accuracy: 0.0493000149727 \t66 1.36453 -0.108412\n",
      "Accuracy: 0.0490000247955 \t67 1.36841 -0.112753\n",
      "Accuracy: 0.0487999916077 \t68 1.37229 -0.117095\n",
      "Accuracy: 0.0487999916077 \t69 1.37617 -0.121437\n",
      "Accuracy: 0.0491999983788 \t70 1.38004 -0.125778\n",
      "Accuracy: 0.049399971962 \t71 1.38392 -0.13012\n",
      "Accuracy: 0.0496000051498 \t72 1.3878 -0.134462\n",
      "Accuracy: 0.0491999983788 \t73 1.39167 -0.138803\n",
      "Accuracy: 0.0493000149727 \t74 1.39555 -0.143145\n",
      "Accuracy: 0.0494999885559 \t75 1.39943 -0.147487\n",
      "Accuracy: 0.0490999817848 \t76 1.40331 -0.151829\n",
      "Accuracy: 0.0487999916077 \t77 1.40718 -0.15617\n",
      "Accuracy: 0.0483000278473 \t78 1.41106 -0.160512\n",
      "Accuracy: 0.0479999780655 \t79 1.41494 -0.164854\n",
      "Accuracy: 0.0480999946594 \t80 1.41881 -0.169195\n",
      "Accuracy: 0.0482000112534 \t81 1.42269 -0.173537\n",
      "Accuracy: 0.0475999712944 \t82 1.42657 -0.177879\n",
      "Accuracy: 0.0479999780655 \t83 1.43045 -0.18222\n",
      "Accuracy: 0.0478000044823 \t84 1.43432 -0.186562\n",
      "Accuracy: 0.0478000044823 \t85 1.4382 -0.190904\n",
      "Accuracy: 0.0478000044823 \t86 1.44208 -0.195245\n",
      "Accuracy: 0.0476999878883 \t87 1.44595 -0.199587\n",
      "Accuracy: 0.0479999780655 \t88 1.44983 -0.203929\n",
      "Accuracy: 0.0476999878883 \t89 1.45371 -0.208271\n",
      "Accuracy: 0.0473999977112 \t90 1.45759 -0.212612\n",
      "Accuracy: 0.047200024128 \t91 1.46146 -0.216954\n",
      "Accuracy: 0.0475999712944 \t92 1.46534 -0.221296\n",
      "Accuracy: 0.0482000112534 \t93 1.46922 -0.225637\n",
      "Accuracy: 0.0478000044823 \t94 1.47309 -0.229979\n",
      "Accuracy: 0.0480999946594 \t95 1.47697 -0.234321\n",
      "Accuracy: 0.0480999946594 \t96 1.48085 -0.238662\n",
      "Accuracy: 0.0480999946594 \t97 1.48473 -0.243004\n",
      "Accuracy: 0.0482000112534 \t98 1.4886 -0.247346\n",
      "Accuracy: 0.0482000112534 \t99 1.49248 -0.251687\n"
     ]
    }
   ],
   "source": [
    "for b in xrange(len(test.AllBeads)-1):\n",
    "    e = InterpBeadError(test.AllBeads[b][0],test.AllBeads[b][1], test.AllBeads[b+1][0], test.AllBeads[b+1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w,b = test.AllBeads[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.52026641,  0.24772249,  0.30443385, -0.74694669, -0.61910379,\n",
       "        -0.96364206, -0.31234485, -0.04062728, -0.31264544,  0.79933226,\n",
       "        -0.95725209,  0.30137059,  1.15755188, -1.62399101, -1.40891051,\n",
       "         1.03353941,  0.50101578, -1.38635409, -0.16988727,  0.65464318,\n",
       "        -0.49132031,  0.2587409 ,  0.4232485 , -1.12940395, -1.17089713,\n",
       "         0.21410605, -1.40715575, -0.53397185,  0.69357997, -0.11505934,\n",
       "        -1.20675778, -0.01668277, -0.49536428, -1.22702384, -1.13588881,\n",
       "        -1.20673954, -0.59726006,  0.62010694,  0.69467616,  0.03794095,\n",
       "        -0.16322906,  1.05579317, -0.88469303, -1.09426343,  1.05520177,\n",
       "        -0.71906632,  0.59672976, -0.50000161, -0.5576089 ,  1.50208366,\n",
       "        -2.50997949,  0.59063935,  0.43107703, -0.53128642, -0.04713962,\n",
       "         0.19859725,  2.02716494,  1.4697119 , -1.04942441, -0.39021888,\n",
       "         0.74863291, -1.50379992, -1.21944368,  0.42019922,  1.05488157,\n",
       "        -1.2739656 , -1.17630577, -0.58982897,  1.74044049, -0.15614201,\n",
       "        -1.95725965,  0.22762525, -0.43448061,  2.11176133, -0.36525011,\n",
       "        -1.27180409, -0.45832905, -0.68754828,  0.71281052, -1.08011281,\n",
       "        -0.01405062,  1.56414759, -1.12112153,  0.89823389, -1.02622235,\n",
       "         1.5640043 ,  0.56441647, -0.60250258,  0.08498309,  0.07512405,\n",
       "         0.8184731 , -0.11774384, -0.63389194,  0.7879253 ,  1.24543571,\n",
       "        -0.55245817, -2.20619941,  0.15305792, -0.15095206,  0.61309052,\n",
       "         2.08695745, -0.26535568, -1.41212142, -0.54862523, -0.11481999,\n",
       "         0.19107763, -0.20386672,  0.25322413,  0.77498215,  1.02639604,\n",
       "         0.46344551,  0.15323865, -0.57457662, -0.66503602, -1.63733602,\n",
       "        -0.49078637, -1.09675825,  2.23333144,  1.56395853,  1.06442392,\n",
       "        -0.32931131,  0.47142223, -0.84639472, -0.03360561,  0.7630983 ,\n",
       "         1.09900773, -0.85410815, -0.2778208 ,  0.61274922, -0.15112965,\n",
       "        -0.98521763,  1.76582778,  0.8848176 ,  0.24938644,  0.25634173,\n",
       "         0.31735703,  0.72662902, -1.08217037, -0.97182274, -0.27280936,\n",
       "         0.61966795,  0.11838301, -0.15098131, -1.67015183, -0.86999738,\n",
       "        -1.80483186, -1.20380735,  0.33744729,  0.32784501,  0.55487835,\n",
       "        -0.42065921, -2.26104259,  0.20326507, -1.05227637, -1.01744986,\n",
       "         1.05173516, -1.8231616 , -0.80066895,  0.50194466, -1.95388067,\n",
       "        -1.4391402 ,  0.13966687, -1.67697215, -1.24225962,  0.84286296,\n",
       "        -0.54905736,  1.19326067, -1.17864764, -1.22458756, -0.29295793,\n",
       "         0.30057362,  0.00705888,  0.37743023, -1.23920453, -0.15744478,\n",
       "         0.2918469 , -0.29207739,  1.97760868,  0.15874423, -0.86647135,\n",
       "        -0.14525419, -1.18887436,  0.11934876,  1.52983165, -1.42522275,\n",
       "        -0.20683654,  1.95480835,  0.16375861, -0.3260259 ,  0.93608552,\n",
       "         1.19607568, -0.66071457,  2.36982465, -1.79331374, -0.63636023,\n",
       "        -0.1087935 ,  1.24920833, -1.49991786,  1.72524059, -0.25291798,\n",
       "        -0.88024724,  0.22240987, -0.23660582, -0.58132148,  1.47205067,\n",
       "         0.19754164, -0.21640907,  1.7684561 , -0.5570702 , -0.15071924,\n",
       "        -0.26569909,  1.83550334,  1.12755537, -0.84360546, -0.68844855,\n",
       "        -1.22405112, -0.40174514, -0.72491741, -0.3908442 , -0.48991638,\n",
       "        -0.18473668, -1.04876864,  1.09889734, -1.52195394, -0.23336567,\n",
       "         0.338213  , -1.02336526,  0.39991125,  0.20593315, -1.80190277,\n",
       "         0.83319831, -1.80572712,  0.20746477,  0.10967674,  0.67174762,\n",
       "         0.03207009,  0.58736247, -0.05316891, -1.01571727, -0.05488862,\n",
       "         1.33087611, -1.42514133, -0.68828815, -0.15759154, -0.08729997,\n",
       "        -0.01085017, -0.24643923,  1.10964143,  1.10586405, -1.11690211,\n",
       "         0.6076405 , -2.49322486,  1.45090854,  0.4801316 , -0.74751318,\n",
       "         0.88272679], dtype=float32),\n",
       " array([ -8.21595252e-01,  -2.22721230e-02,  -9.76333737e-01,\n",
       "         -1.38896811e+00,  -6.87877655e-01,   1.18521643e+00,\n",
       "          7.66462088e-01,  -9.56141353e-02,  -1.14207186e-01,\n",
       "          1.63640112e-01,  -1.97290972e-01,  -2.10825011e-01,\n",
       "          1.69831336e+00,  -7.60490894e-02,  -1.10037732e+00,\n",
       "         -4.17452335e-01,   8.58393729e-01,   1.42138398e+00,\n",
       "         -2.98899889e-01,   2.11727805e-03,  -1.57120034e-01,\n",
       "          1.35947990e+00,  -6.48545384e-01,  -6.62959814e-01,\n",
       "         -1.16728854e+00,   7.48800576e-01,  -7.39751875e-01,\n",
       "          1.24571770e-01,  -9.35620427e-01,   1.49829119e-01,\n",
       "          3.65435719e-01,   1.73232102e+00,  -1.90552771e+00,\n",
       "          1.50847304e+00,   4.96079385e-01,  -8.76551211e-01,\n",
       "          9.51387107e-01,   1.34580100e+00,   3.10356915e-01,\n",
       "          9.31321457e-02,  -1.96810389e+00,   5.61275005e-01,\n",
       "          9.65428948e-01,   9.76991653e-01,   1.45071554e+00,\n",
       "          1.31641373e-01,   1.65384603e+00,   1.51370239e+00,\n",
       "         -1.78499413e+00,  -3.76800179e-01,   1.89858645e-01,\n",
       "         -1.75206363e+00,  -7.60813355e-01,   1.87430263e+00,\n",
       "         -2.52064113e-02,  -1.51955998e+00,   1.20996507e-02,\n",
       "          5.29639661e-01,  -1.74715376e+00,   2.87368149e-01,\n",
       "         -9.84721109e-02,  -4.74223383e-02,  -3.06302810e+00,\n",
       "          6.99477077e-01,  -3.02687913e-01,  -6.80822909e-01,\n",
       "         -4.51687187e-01,  -1.64143562e+00,   7.61203647e-01,\n",
       "         -5.73516190e-01,  -7.75239646e-01,  -7.29189754e-01,\n",
       "         -4.67335820e-01,  -1.36266279e+00,  -2.04433584e+00,\n",
       "         -2.52698958e-01,  -1.92006528e+00,   1.29426137e-01,\n",
       "          1.10093367e+00,   1.51384485e+00,   1.56335020e+00,\n",
       "         -6.16457403e-01,   2.72284687e-01,  -2.72939026e-01,\n",
       "         -6.51860356e-01,  -5.80804169e-01,   3.70719540e-03,\n",
       "         -2.12593839e-01,   3.45503300e-01,  -3.84427041e-01,\n",
       "         -1.32019496e+00,  -1.04059768e+00,  -9.97523189e-01,\n",
       "          1.25891697e+00,  -1.90580213e+00,   2.50689775e-01,\n",
       "          3.26620549e-01,   5.58647454e-01,   2.45166212e-01,\n",
       "         -6.33782864e-01,   5.44911981e-01,  -9.44834769e-01,\n",
       "         -1.03981388e+00,   2.89378077e-01,   2.49647930e-01,\n",
       "         -4.90579218e-01,  -7.60564208e-01,  -6.06193617e-02,\n",
       "         -3.00590694e-01,  -1.56739521e+00,  -3.84743750e-01,\n",
       "         -9.40849662e-01,   9.23318624e-01,  -9.03862000e-01,\n",
       "          1.16799891e-01,   8.25044453e-01,   6.96205437e-01,\n",
       "          2.42476654e+00,  -6.93747580e-01,  -6.52617931e-01,\n",
       "         -1.08003902e+00,   5.44459283e-01,   1.13595128e+00,\n",
       "         -5.77786088e-01,   6.04558825e-01,  -1.39842048e-01,\n",
       "          8.85597050e-01,  -6.78144470e-02,  -3.65855992e-02,\n",
       "          6.55721366e-01,  -1.25857353e+00,  -1.07726775e-01,\n",
       "          1.00116169e+00,  -1.33260143e+00,  -2.26636982e+00,\n",
       "         -1.23874700e+00,  -4.12589490e-01,   7.08425105e-01,\n",
       "         -4.98440742e-01,  -1.15652454e+00,  -5.49404562e-01,\n",
       "         -4.39788759e-01,   1.38037503e+00,  -4.79600102e-01,\n",
       "         -4.39077348e-01,  -6.65886328e-02,  -2.67328191e+00,\n",
       "         -3.29776973e-01,   3.95685554e-01,   2.89580584e-01,\n",
       "          5.95919430e-01,   1.07386303e+00,  -6.67644620e-01,\n",
       "         -4.90350842e-01,  -9.16802526e-01,   6.42959476e-01,\n",
       "          1.13136685e+00,  -3.37020010e-01,   9.86904204e-01,\n",
       "          7.88809478e-01,  -5.06890059e-01,   5.71731985e-01,\n",
       "         -1.74489057e+00,  -7.56577194e-01,   2.61685640e-01,\n",
       "          4.31760818e-01,  -5.51138222e-01,  -2.21222043e-01,\n",
       "          1.16399157e+00,  -7.68629074e-01,   2.67541289e+00,\n",
       "          8.63890871e-02,   5.57408094e-01,   2.97064692e-01,\n",
       "          6.45241201e-01,  -5.42945325e-01,  -7.44524062e-01,\n",
       "          1.02702224e+00,   3.85128111e-01,  -1.93029404e-01,\n",
       "         -7.83132493e-01,  -6.11906052e-02,   6.48386478e-02,\n",
       "         -6.98789001e-01,  -7.59570301e-01,  -2.89260596e-01,\n",
       "          1.97721079e-01,   4.20850694e-01,  -8.08305144e-01,\n",
       "         -1.93403438e-01,   9.12539065e-01,   3.39912206e-01,\n",
       "         -3.67852934e-02,   1.26253653e+00,  -4.26331699e-01,\n",
       "         -2.54645205e+00,  -8.58297944e-01,   9.94566232e-02,\n",
       "         -2.86502331e-01,  -1.28695488e-01,  -1.37592399e+00,\n",
       "          1.77185416e+00,  -8.68417084e-01,  -1.69211721e+00,\n",
       "          3.19970489e-01,  -2.88471490e-01,   3.96305919e-01,\n",
       "         -5.54621756e-01,   1.14738524e+00,   1.00884295e+00,\n",
       "          7.53781557e-01,   1.30466938e+00,  -1.55493903e+00,\n",
       "         -5.64799845e-01,   7.90246129e-01,   5.84462583e-01,\n",
       "          1.28228307e-01,  -1.29814446e-01,  -6.23074472e-01,\n",
       "          5.43519616e-01,   1.69531620e+00,   7.17151403e-01,\n",
       "          1.47323155e+00,  -1.27093756e+00,   9.60818291e-01,\n",
       "         -4.54179645e-01,   9.46830869e-01,  -1.23857118e-01,\n",
       "          1.55570865e+00,  -1.38381690e-01,   2.00770587e-01,\n",
       "         -7.95185566e-01,  -4.98316586e-01,   1.40864658e+00,\n",
       "         -1.04276812e+00,   1.25466311e+00,  -4.57322039e-02,\n",
       "          1.07208228e+00,   1.43595725e-01,   1.12169027e+00,\n",
       "          5.45263231e-01,  -9.41491008e-01,  -1.79404306e+00,\n",
       "          1.44434440e+00,   6.69220209e-01,  -1.30321312e+00,\n",
       "         -1.86194289e+00,  -1.06763482e+00,  -1.24295461e+00,\n",
       "          9.05483603e-01,   1.23212481e+00,  -1.57927001e+00,\n",
       "          9.38302815e-01,   7.11243033e-01,   3.87621850e-01,\n",
       "          6.12435713e-02], dtype=float32),\n",
       " array([-1.2435081 , -1.3895067 ,  0.53644782,  0.60423809, -0.19422041,\n",
       "         0.79507917,  1.13472712, -1.90221465, -1.02565968, -1.01484549], dtype=float32)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.427446254\n",
      "Accuracy: 0.9642\n",
      "Epoch: 0002 cost= 0.388155263\n",
      "Accuracy: 0.9631\n",
      "Epoch: 0003 cost= 0.215077950\n",
      "Accuracy: 0.9659\n",
      "Epoch: 0004 cost= 0.254136921\n",
      "Accuracy: 0.9643\n",
      "Epoch: 0005 cost= 0.266782776\n",
      "Accuracy: 0.9622\n",
      "Epoch: 0006 cost= 0.286865226\n",
      "Accuracy: 0.9634\n",
      "Epoch: 0007 cost= 0.349483893\n",
      "Accuracy: 0.9652\n",
      "Epoch: 0008 cost= 0.322126526\n",
      "Accuracy: 0.9666\n",
      "Epoch: 0009 cost= 0.102091477\n",
      "Accuracy: 0.967\n",
      "Epoch: 0010 cost= 0.144894568\n",
      "Accuracy: 0.964\n",
      "Epoch: 0011 cost= 0.156021222\n",
      "Accuracy: 0.9675\n",
      "Epoch: 0012 cost= 0.154399537\n",
      "Accuracy: 0.9677\n",
      "Epoch: 0013 cost= 0.157314702\n",
      "Accuracy: 0.9666\n",
      "Epoch: 0014 cost= 0.097399672\n",
      "Accuracy: 0.9662\n",
      "Epoch: 0015 cost= 0.070537900\n",
      "Accuracy: 0.9664\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9664\n",
      "Error after 0 iterations:0.9664\n",
      "Epoch: 0001 cost= 0.069893413\n",
      "Accuracy: 0.9658\n",
      "Epoch: 0002 cost= 0.111843362\n",
      "Accuracy: 0.9656\n",
      "Epoch: 0003 cost= 0.145028126\n",
      "Accuracy: 0.9668\n",
      "Epoch: 0004 cost= 0.110090393\n",
      "Accuracy: 0.9677\n",
      "Epoch: 0005 cost= 0.075510024\n",
      "Accuracy: 0.9658\n",
      "Epoch: 0006 cost= 0.091460152\n",
      "Accuracy: 0.9668\n",
      "Epoch: 0007 cost= 0.041279547\n",
      "Accuracy: 0.9674\n",
      "Epoch: 0008 cost= 0.092657605\n",
      "Accuracy: 0.9662\n",
      "Epoch: 0009 cost= 0.092687997\n",
      "Accuracy: 0.9661\n",
      "Epoch: 0010 cost= 0.055850054\n",
      "Accuracy: 0.9675\n",
      "Epoch: 0011 cost= 0.068601606\n",
      "Accuracy: 0.9679\n",
      "Epoch: 0012 cost= 0.093378161\n",
      "Accuracy: 0.9685\n",
      "Epoch: 0013 cost= 0.065880271\n",
      "Accuracy: 0.967\n",
      "Epoch: 0014 cost= 0.119259865\n",
      "Accuracy: 0.9676\n",
      "Epoch: 0015 cost= 0.123184984\n",
      "Accuracy: 0.9659\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9659\n",
      "Error after 0 iterations:0.9659\n",
      "Epoch: 0001 cost= 0.062646902\n",
      "Accuracy: 0.9668\n",
      "Epoch: 0002 cost= 0.118531805\n",
      "Accuracy: 0.9658\n",
      "Epoch: 0003 cost= 0.144570656\n",
      "Accuracy: 0.9681\n",
      "Epoch: 0004 cost= 0.117370040\n",
      "Accuracy: 0.9684\n",
      "Epoch: 0005 cost= 0.094370778\n",
      "Accuracy: 0.9673\n",
      "Epoch: 0006 cost= 0.106851742\n",
      "Accuracy: 0.9686\n",
      "Epoch: 0007 cost= 0.044365020\n",
      "Accuracy: 0.967\n",
      "Epoch: 0008 cost= 0.075937668\n",
      "Accuracy: 0.9673\n",
      "Epoch: 0009 cost= 0.039660773\n",
      "Accuracy: 0.9667\n",
      "Epoch: 0010 cost= 0.065723944\n",
      "Accuracy: 0.9669\n",
      "Epoch: 0011 cost= 0.107796821\n",
      "Accuracy: 0.9658\n",
      "Epoch: 0012 cost= 0.106408267\n",
      "Accuracy: 0.9668\n",
      "Epoch: 0013 cost= 0.056187395\n",
      "Accuracy: 0.9683\n",
      "Epoch: 0014 cost= 0.082605585\n",
      "Accuracy: 0.9695\n",
      "Epoch: 0015 cost= 0.066437753\n",
      "Accuracy: 0.9682\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9682\n",
      "Error after 0 iterations:0.9682\n",
      "Epoch: 0001 cost= 0.113998719\n",
      "Accuracy: 0.9655\n",
      "Epoch: 0002 cost= 0.097598098\n",
      "Accuracy: 0.9688\n",
      "Epoch: 0003 cost= 0.096100489\n",
      "Accuracy: 0.9662\n",
      "Epoch: 0004 cost= 0.090809174\n",
      "Accuracy: 0.9684\n",
      "Epoch: 0005 cost= 0.077002753\n",
      "Accuracy: 0.9673\n",
      "Epoch: 0006 cost= 0.055263686\n",
      "Accuracy: 0.9682\n",
      "Epoch: 0007 cost= 0.060709617\n",
      "Accuracy: 0.9661\n",
      "Epoch: 0008 cost= 0.089572028\n",
      "Accuracy: 0.9693\n",
      "Epoch: 0009 cost= 0.064838966\n",
      "Accuracy: 0.9702\n",
      "Epoch: 0010 cost= 0.063324587\n",
      "Accuracy: 0.9695\n",
      "Epoch: 0011 cost= 0.051863025\n",
      "Accuracy: 0.9701\n",
      "Epoch: 0012 cost= 0.045127958\n",
      "Accuracy: 0.9703\n",
      "Epoch: 0013 cost= 0.040996780\n",
      "Accuracy: 0.9694\n",
      "Epoch: 0014 cost= 0.041134293\n",
      "Accuracy: 0.9703\n",
      "Epoch: 0015 cost= 0.078443705\n",
      "Accuracy: 0.9695\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9695\n",
      "Error after 0 iterations:0.9695\n",
      "Epoch: 0001 cost= 0.045867218\n",
      "Accuracy: 0.9698\n",
      "Epoch: 0002 cost= 0.068232461\n",
      "Accuracy: 0.9706\n",
      "Epoch: 0003 cost= 0.100191265\n",
      "Accuracy: 0.9681\n",
      "Epoch: 0004 cost= 0.058207033\n",
      "Accuracy: 0.9685\n",
      "Epoch: 0005 cost= 0.073468120\n",
      "Accuracy: 0.9672\n",
      "Epoch: 0006 cost= 0.115724135\n",
      "Accuracy: 0.9677\n",
      "Epoch: 0007 cost= 0.069882303\n",
      "Accuracy: 0.9686\n",
      "Epoch: 0008 cost= 0.103297367\n",
      "Accuracy: 0.9692\n",
      "Epoch: 0009 cost= 0.054458868\n",
      "Accuracy: 0.9696\n",
      "Epoch: 0010 cost= 0.059468066\n",
      "Accuracy: 0.9699\n",
      "Epoch: 0011 cost= 0.081108575\n",
      "Accuracy: 0.965\n",
      "Epoch: 0012 cost= 0.115991086\n",
      "Accuracy: 0.9672\n",
      "Epoch: 0013 cost= 0.127581704\n",
      "Accuracy: 0.9673\n",
      "Epoch: 0014 cost= 0.050746322\n",
      "Accuracy: 0.9677\n",
      "Epoch: 0015 cost= 0.027768867\n",
      "Accuracy: 0.9674\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9674\n",
      "Error after 0 iterations:0.9674\n",
      "Epoch: 0001 cost= 0.035731059\n",
      "Accuracy: 0.9698\n",
      "Epoch: 0002 cost= 0.032648279\n",
      "Accuracy: 0.9683\n",
      "Epoch: 0003 cost= 0.072057581\n",
      "Accuracy: 0.9692\n",
      "Epoch: 0004 cost= 0.101116247\n",
      "Accuracy: 0.9694\n",
      "Epoch: 0005 cost= 0.084560442\n",
      "Accuracy: 0.9672\n",
      "Epoch: 0006 cost= 0.076841464\n",
      "Accuracy: 0.9687\n",
      "Epoch: 0007 cost= 0.075424822\n",
      "Accuracy: 0.969\n",
      "Epoch: 0008 cost= 0.040257943\n",
      "Accuracy: 0.968\n",
      "Epoch: 0009 cost= 0.059859093\n",
      "Accuracy: 0.9685\n",
      "Epoch: 0010 cost= 0.059823168\n",
      "Accuracy: 0.9707\n",
      "Epoch: 0011 cost= 0.044282347\n",
      "Accuracy: 0.9716\n",
      "Epoch: 0012 cost= 0.057426597\n",
      "Accuracy: 0.9689\n",
      "Epoch: 0013 cost= 0.040995671\n",
      "Accuracy: 0.9718\n",
      "Epoch: 0014 cost= 0.029555581\n",
      "Accuracy: 0.9706\n",
      "Epoch: 0015 cost= 0.033128446\n",
      "Accuracy: 0.9701\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9701\n",
      "Error after 0 iterations:0.9701\n",
      "Epoch: 0001 cost= 0.029655925\n",
      "Accuracy: 0.97\n",
      "Epoch: 0002 cost= 0.036713677\n",
      "Accuracy: 0.9695\n",
      "Epoch: 0003 cost= 0.009859686\n",
      "Accuracy: 0.9697\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-0d07f118cc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;31m# Loop over all batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                             \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                             _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.pyc\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self, batch_size, fake_data)\u001b[0m\n\u001b[1;32m    145\u001b[0m           fake_label for _ in xrange(batch_size)]\n\u001b[1;32m    146\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_in_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_in_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0;31m# Finished epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Model generation\n",
    "\n",
    "w,b = test.AllBeads[1]\n",
    "thresh = .005\n",
    "\n",
    "for ii in xrange(1):\n",
    "\n",
    "    '''weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }'''\n",
    "\n",
    "    # Construct model with different initial weights\n",
    "    test_model = multilayer_perceptron(w=w, b=b, ind=100)\n",
    "    \n",
    "    #Construct model with same initial weights\n",
    "    #test_model = copy.copy(copy_model)\n",
    "    #test_model.index = ii\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print test_model.weights\n",
    "    \n",
    "\n",
    "    \n",
    "    #models.append(test_model)\n",
    "    with test_model.g.as_default():\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, n_input])\n",
    "        y = tf.placeholder(\"float\", [None, n_classes])\n",
    "        pred = test_model.predict(x)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "        #remove the comment to get random initialization\n",
    "        stopcond = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            xtest = mnist.test.images\n",
    "            ytest = mnist.test.labels\n",
    "            while stopcond:\n",
    "                #print 'epoch:' + str(e)\n",
    "                #X = []\n",
    "                #y = []\n",
    "                j = 0\n",
    "                # Training cycle\n",
    "                for epoch in range(training_epochs):\n",
    "                    avg_cost = 0.\n",
    "                    total_batch = int(10000/batch_size)\n",
    "\n",
    "                    if (avg_cost > thresh or avg_cost == 0.) and stopcond:\n",
    "                    # Loop over all batches\n",
    "                        for i in range(total_batch):\n",
    "                            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                                          y: batch_y})\n",
    "                            # Compute average loss\n",
    "                            avg_cost += c / total_batch\n",
    "                        # Display logs per epoch step\n",
    "                        if epoch % display_step == 0:\n",
    "                            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                                \"{:.9f}\".format(avg_cost)\n",
    "                        \n",
    "                        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                        # Calculate accuracy\n",
    "                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                        print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "                        thiserror = 1 - accuracy.eval({x: xtest, y: ytest})\n",
    "                        if thiserror < thresh:\n",
    "                            stopcond = False\n",
    "                            \n",
    "                print \"Optimization Finished!\"\n",
    "\n",
    "                # Test model\n",
    "                #correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                # Calculate accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "\n",
    "                if (j%5000) == 0:\n",
    "                    print \"Error after \"+str(j)+\" iterations:\" + str(accuracy.eval({x: xtest, y: ytest}))\n",
    "\n",
    "                if 1 - accuracy.eval({x: xtest, y: ytest}) < thresh or stopcond == False:\n",
    "                    #print \"Changing stopcond!\"\n",
    "                    stopcond = False\n",
    "                    print \"Final params:\"\n",
    "                    test_model.params = sess.run(test_model.weightslist), sess.run(test_model.biaseslist)\n",
    "                    save_path = test_model.saver.save(sess,\"/home/dfreeman/PythonFun/tmp/model\" + str(ii) + \".ckpt\")\n",
    "                j+=1\n",
    "    #remove the comment to get random initialization\n",
    "\n",
    "    \n",
    "    #synapses.append([synapse_0,synapse_1,synapse_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
