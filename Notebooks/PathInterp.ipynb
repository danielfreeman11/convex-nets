{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4322\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.8962\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9003\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9113\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9111\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9067\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9168\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9135\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9142\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.9178\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "\n",
    "for alpha in [.5]:\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "\n",
    "        if i%100 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "            print W.eval(sess)\n",
    "    #weights.append(W.eval(sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.06755192e-06  -2.92071984e-08   3.39211401e-05  -1.87954896e-07\n",
      "  -2.40277004e-05  -2.20967440e-08  -5.70709790e-06  -4.84702980e-07\n",
      "  -1.16136789e-06  -1.23375150e-06]\n"
     ]
    }
   ],
   "source": [
    "print weights[0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FLAGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-de3f0183d09b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"
     ]
    }
   ],
   "source": [
    "data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9202\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Simple network: Given three integers a,b,c, [-100,100] chooses three random x-values, and evaluates\n",
    "#the quadratic function a*x^2 + b*x + c at those values.\n",
    "def func(x,a,b,c):\n",
    "    return x*x*a + x*b + c\n",
    "\n",
    "def generatecandidate3(a,b,c):\n",
    "\n",
    "\n",
    "    candidate = [np.random.random() for x in xrange(1)]\n",
    "    candidatesolutions = [func(x,a,b,c) for x in candidate]\n",
    "    \n",
    "    \n",
    "    return candidate, candidatesolutions\n",
    "\n",
    "def generatecandidate4(a,b,c,tot):\n",
    "    \n",
    "    candidate = [[np.random.random() for x in xrange(1)] for y in xrange(tot)]\n",
    "    candidatesolutions = [[func(x[0],a,b,c)] for x in candidate]\n",
    "    \n",
    "    return (candidate, candidatesolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 4 # 1st layer number of features\n",
    "n_hidden_2 = 4 # 2nd layer number of features\n",
    "n_input = 1 # Guess quadratic function\n",
    "n_classes = 1 # \n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def multilayer_perceptron(x, weights, biases):\\n    # Hidden layer with RELU activation\\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\\n    layer_1 = tf.nn.relu(layer_1)\\n    # Hidden layer with RELU activation\\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\\n    layer_2 = tf.nn.relu(layer_2)\\n    # Output layer with linear activation\\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\\n    return out_layer\""
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "\n",
    "class multilayer_perceptron():\n",
    "    \n",
    "    #weights = {}\n",
    "    #biases = {}\n",
    "    \n",
    "    def __init__(self, w=0, b=0, ind='00'):\n",
    "        \n",
    "        self.index = ind #used for reading values from file\n",
    "        #See the filesystem convention below (is this really necessary?)\n",
    "        #I'm going to eschew writing to file for now because I'll be generating too many files\n",
    "        #Currently, the last value of the parameters is stored in self.params to be read\n",
    "        \n",
    "        learning_rate = 0.01\n",
    "        training_epochs = 15\n",
    "        batch_size = 1000\n",
    "        display_step = 1\n",
    "\n",
    "        # Network Parameters\n",
    "        n_hidden_1 = 4 # 1st layer number of features\n",
    "        n_hidden_2 = 4 # 2nd layer number of features\n",
    "        n_input = 1 # Guess quadratic function\n",
    "        n_classes = 1 # \n",
    "        self.g = tf.Graph()\n",
    "        \n",
    "        \n",
    "        self.params = []\n",
    "        \n",
    "        with self.g.as_default():\n",
    "        \n",
    "            #Note that by default, weights and biases will be initialized to random normal dists\n",
    "            if w==0:\n",
    "                \n",
    "                self.weights = {\n",
    "                    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "                    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "                    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "                }\n",
    "                self.weightslist = [self.weights['h1'],self.weights['h2'],self.weights['out']]\n",
    "                self.biases = {\n",
    "                    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "                    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "                    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "                }\n",
    "                self.biaseslist = [self.biases['b1'],self.biases['b2'],self.biases['out']]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                self.weights = {\n",
    "                    'h1': tf.Variable(w[0]),\n",
    "                    'h2': tf.Variable(w[1]),\n",
    "                    'out': tf.Variable(w[2])\n",
    "                }\n",
    "                self.weightslist = [self.weights['h1'],self.weights['h2'],self.weights['out']]\n",
    "                self.biases = {\n",
    "                    'b1': tf.Variable(b[0]),\n",
    "                    'b2': tf.Variable(b[1]),\n",
    "                    'out': tf.Variable(b[2])\n",
    "                }\n",
    "                self.biaseslist = [self.biases['b1'],self.biases['b2'],self.biases['out']]\n",
    "            self.saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    def UpdateWeights(self, w, b):\n",
    "        with self.g.as_default():\n",
    "            self.weights = {\n",
    "                    'h1': tf.Variable(w[0]),\n",
    "                    'h2': tf.Variable(w[1]),\n",
    "                    'out': tf.Variable(w[2])\n",
    "                }\n",
    "            self.weightslist = [self.weights['h1'],self.weights['h2'],self.weights['out']]\n",
    "            self.biases = {\n",
    "                'b1': tf.Variable(b[0]),\n",
    "                'b2': tf.Variable(b[1]),\n",
    "                'out': tf.Variable(b[2])\n",
    "            }\n",
    "            self.biaseslist = [self.biases['b1'],self.biases['b2'],self.biases['out']]\n",
    "            \n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        with self.g.as_default():\n",
    "            layer_1 = tf.add(tf.matmul(x, self.weights['h1']), self.biases['b1'])\n",
    "            layer_1 = tf.nn.relu(layer_1)\n",
    "            # Hidden layer with RELU activation\n",
    "            layer_2 = tf.add(tf.matmul(layer_1, self.weights['h2']), self.biases['b2'])\n",
    "            layer_2 = tf.nn.relu(layer_2)\n",
    "            # Output layer with linear activation\n",
    "            out_layer = tf.matmul(layer_2, self.weights['out']) + self.biases['out']\n",
    "            return out_layer\n",
    "        \n",
    "    def ReturnParamsAsList(self):\n",
    "        \n",
    "        with self.g.as_default():\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                # Restore variables from disk\n",
    "                self.saver.restore(sess, \"/home/dfreeman/PythonFun/tmp/model\"+str(self.index)+\".ckpt\")                \n",
    "                return sess.run(self.weightslist), sess.run(self.biaseslist)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "'''def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "test_model = multilayer_perceptron(weights, biases)\n",
    "\n",
    "pred = test_model.predict(x)\n",
    "\n",
    "# Define loss and optimizer\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "cost = tf.reduce_mean(tf.square(pred-y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tensorflow.python.framework.ops.Operation object at 0x7fdafd982d10> of <tensorflow.python.framework.ops.Operation object at 0x7fdafd982d10> cannot be interpreted as a Tensor. (Operation name: \"init_2\"\nop: \"NoOp\"\ninput: \"^Variable/Assign\"\ninput: \"^Variable_1/Assign\"\ninput: \"^Variable_2/Assign\"\ninput: \"^Variable_3/Assign\"\ninput: \"^Variable_4/Assign\"\ninput: \"^Variable_5/Assign\"\n is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-78ddd71c1329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Launch the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Training cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;31m# Validate and process fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0mprocessed_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0munique_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_process_fetches\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n\u001b[0;32m--> 496\u001b[0;31m                            'Tensor. (%s)' % (subfetch, fetch, str(e)))\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n",
      "\u001b[0;31mValueError\u001b[0m: Fetch argument <tensorflow.python.framework.ops.Operation object at 0x7fdafd982d10> of <tensorflow.python.framework.ops.Operation object at 0x7fdafd982d10> cannot be interpreted as a Tensor. (Operation name: \"init_2\"\nop: \"NoOp\"\ninput: \"^Variable/Assign\"\ninput: \"^Variable_1/Assign\"\ninput: \"^Variable_2/Assign\"\ninput: \"^Variable_3/Assign\"\ninput: \"^Variable_4/Assign\"\ninput: \"^Variable_5/Assign\"\n is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "xtest, ytest = generatecandidate4(.5,.25,.1,1000)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(10000/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = generatecandidate4(.5,.25,.1,batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "[[0.2913145477141732], [0.7197537451338574]]\n",
      "[[0.21526071978349995], [0.5389611631005712]]\n"
     ]
    }
   ],
   "source": [
    "x,y = mnist.train.next_batch(2)\n",
    "print x\n",
    "print y\n",
    "x,y = generatecandidate4(.5,.25,.1,2)\n",
    "print x\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6449075 ]\n",
      " [ 0.45473615]\n",
      " [ 0.33488262]\n",
      " [ 0.46034365]\n",
      " [ 0.83584571]\n",
      " [ 0.89909018]\n",
      " [ 0.76755036]\n",
      " [ 0.49760915]\n",
      " [ 0.90528123]\n",
      " [ 0.73978044]] [[ 0.46917971]\n",
      " [ 0.31707652]\n",
      " [ 0.23979384]\n",
      " [ 0.32104405]\n",
      " [ 0.65828045]\n",
      " [ 0.72895413]\n",
      " [ 0.58645437]\n",
      " [ 0.34820972]\n",
      " [ 0.73608736]\n",
      " [ 0.55858266]]\n",
      "0.469179712727\n",
      "****\n",
      "[[ 0.11819226  0.38378665  0.27990231 -1.74266922]]\n",
      "*************\n",
      "Pred:\n",
      "[[ 0.55615944]\n",
      " [ 0.3261233 ]\n",
      " [ 0.18114549]\n",
      " [ 0.33290631]\n",
      " [ 0.78712326]\n",
      " [ 0.86362571]\n",
      " [ 0.70451182]\n",
      " [ 0.37798351]\n",
      " [ 0.87111408]\n",
      " [ 0.67092055]]\n",
      "Real:\n",
      "[[ 0.46917971]\n",
      " [ 0.31707652]\n",
      " [ 0.23979384]\n",
      " [ 0.32104405]\n",
      " [ 0.65828045]\n",
      " [ 0.72895413]\n",
      " [ 0.58645437]\n",
      " [ 0.34820972]\n",
      " [ 0.73608736]\n",
      " [ 0.55858266]]\n",
      "Accuracy: 0.00916406\n",
      "([array([[ 0.11819226,  0.38378665,  0.27990231, -1.74266922]], dtype=float32), array([[ 0.22078152,  1.18347752,  0.91004926,  0.42721173],\n",
      "       [ 0.10281421,  0.18740587,  1.87701643, -0.83394569],\n",
      "       [ 0.63315231,  0.03253949, -0.70630091,  1.60297143],\n",
      "       [-1.35935378, -1.31399155, -0.5892598 , -0.83139694]], dtype=float32), array([[-1.59010446],\n",
      "       [-0.17921025],\n",
      "       [ 2.39534712],\n",
      "       [ 0.48026386]], dtype=float32)], [array([ 0.20191318,  0.39881784,  1.57099736, -1.2472862 ], dtype=float32), array([-0.1087203 , -1.55095255,  0.33243829, -0.15809895], dtype=float32), array([-0.06598968], dtype=float32)])\n"
     ]
    }
   ],
   "source": [
    "#xdat,ydat = generatecandidate4(.5, .25, .1, 10)\n",
    "\n",
    "print xdat, ydat\n",
    "\n",
    "xdat = np.array(xdat)\n",
    "ydat = np.array(ydat)\n",
    "\n",
    "print func(xdat[0][0],.5,.25,.1)\n",
    "\n",
    "\n",
    "with models[0].g.as_default():\n",
    "\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    pred = models[0].predict(x)\n",
    "\n",
    "    #cost = tf.reduce_mean(tf.square(pred-y))\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    #init = tf.initialize_all_variables()\n",
    "    #init = tf.initialize_local_variables()\n",
    "    #init = tf.initialize_variables([x,y])\n",
    "\n",
    "\n",
    "    \n",
    "    print \"****\"\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        \n",
    "        #print sess.run(models[0].weights['h1'])\n",
    "        #sess.run(init)\n",
    "        models[0].saver.restore(sess, \"/home/dfreeman/PythonFun/tmp/model0.ckpt\")\n",
    "            \n",
    "        print sess.run(models[0].weights['h1'])\n",
    "        \n",
    "        \n",
    "        print \"*************\"\n",
    "        #print x.eval()\n",
    "        correct_prediction = tf.reduce_mean(tf.square(pred-y))\n",
    "        #correct_prediction = tf.sub(pred,y)\n",
    "        #print \"Diff prediction:\"\n",
    "        #print correct_prediction.eval({x: xdat, y: ydat})\n",
    "        print \"Pred:\"\n",
    "        print pred.eval({x: xdat})\n",
    "        #print sess.run(pred)\n",
    "        print \"Real:\"\n",
    "        print ydat\n",
    "        #print sess.run(pred)\n",
    "        #print sess.run(y)\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print \"Accuracy:\", accuracy.eval({x: xdat, y: ydat})\n",
    "        \n",
    "print models[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.546539235\n",
      "Epoch: 0002 cost= 0.726056075\n",
      "Epoch: 0003 cost= 0.468982208\n",
      "Epoch: 0004 cost= 0.371891242\n",
      "Epoch: 0005 cost= 0.338695151\n",
      "Epoch: 0006 cost= 0.325073898\n",
      "Epoch: 0007 cost= 0.306272101\n",
      "Epoch: 0008 cost= 0.304094410\n",
      "Epoch: 0009 cost= 0.297462350\n",
      "Epoch: 0010 cost= 0.289633816\n",
      "Epoch: 0011 cost= 0.285320055\n",
      "Epoch: 0012 cost= 0.275727344\n",
      "Epoch: 0013 cost= 0.271105832\n",
      "Epoch: 0014 cost= 0.265889364\n",
      "Epoch: 0015 cost= 0.264696819\n",
      "Optimization Finished!\n",
      "Accuracy: 0.253653\n",
      "Error after 0 iterations:0.253653\n",
      "Epoch: 0001 cost= 0.256881350\n",
      "Epoch: 0002 cost= 0.252774486\n",
      "Epoch: 0003 cost= 0.247116360\n",
      "Epoch: 0004 cost= 0.241427022\n",
      "Epoch: 0005 cost= 0.239372128\n",
      "Epoch: 0006 cost= 0.231538355\n",
      "Epoch: 0007 cost= 0.229884392\n",
      "Epoch: 0008 cost= 0.225733417\n",
      "Epoch: 0009 cost= 0.222091913\n",
      "Epoch: 0010 cost= 0.216191697\n",
      "Epoch: 0011 cost= 0.215894768\n",
      "Epoch: 0012 cost= 0.210283139\n",
      "Epoch: 0013 cost= 0.211168277\n",
      "Epoch: 0014 cost= 0.201413536\n",
      "Epoch: 0015 cost= 0.197245428\n",
      "Optimization Finished!\n",
      "Accuracy: 0.195142\n",
      "Error after 0 iterations:0.195142\n",
      "Epoch: 0001 cost= 0.198949969\n",
      "Epoch: 0002 cost= 0.195963615\n",
      "Epoch: 0003 cost= 0.190704900\n",
      "Epoch: 0004 cost= 0.187575325\n",
      "Epoch: 0005 cost= 0.186489898\n",
      "Epoch: 0006 cost= 0.181730253\n",
      "Epoch: 0007 cost= 0.178239095\n",
      "Epoch: 0008 cost= 0.176646730\n",
      "Epoch: 0009 cost= 0.177408561\n",
      "Epoch: 0010 cost= 0.172958112\n",
      "Epoch: 0011 cost= 0.170805529\n",
      "Epoch: 0012 cost= 0.167440605\n",
      "Epoch: 0013 cost= 0.163213798\n",
      "Epoch: 0014 cost= 0.165419233\n",
      "Epoch: 0015 cost= 0.161706936\n",
      "Optimization Finished!\n",
      "Accuracy: 0.157715\n",
      "Error after 0 iterations:0.157715\n",
      "Epoch: 0001 cost= 0.163076648\n",
      "Epoch: 0002 cost= 0.162775302\n",
      "Epoch: 0003 cost= 0.158340147\n",
      "Epoch: 0004 cost= 0.152136916\n",
      "Epoch: 0005 cost= 0.151680198\n",
      "Epoch: 0006 cost= 0.150020403\n",
      "Epoch: 0007 cost= 0.150287020\n",
      "Epoch: 0008 cost= 0.147683969\n",
      "Epoch: 0009 cost= 0.145647821\n",
      "Epoch: 0010 cost= 0.143752423\n",
      "Epoch: 0011 cost= 0.139379817\n",
      "Epoch: 0012 cost= 0.139022496\n",
      "Epoch: 0013 cost= 0.138891986\n",
      "Epoch: 0014 cost= 0.137515041\n",
      "Epoch: 0015 cost= 0.134515047\n",
      "Optimization Finished!\n",
      "Accuracy: 0.131386\n",
      "Error after 0 iterations:0.131386\n",
      "Epoch: 0001 cost= 0.133812436\n",
      "Epoch: 0002 cost= 0.131385994\n",
      "Epoch: 0003 cost= 0.129083854\n",
      "Epoch: 0004 cost= 0.127117170\n",
      "Epoch: 0005 cost= 0.127755603\n",
      "Epoch: 0006 cost= 0.125004120\n",
      "Epoch: 0007 cost= 0.124489291\n",
      "Epoch: 0008 cost= 0.122517926\n",
      "Epoch: 0009 cost= 0.123761418\n",
      "Epoch: 0010 cost= 0.121522665\n",
      "Epoch: 0011 cost= 0.117702048\n",
      "Epoch: 0012 cost= 0.118131408\n",
      "Epoch: 0013 cost= 0.117897923\n",
      "Epoch: 0014 cost= 0.117229010\n",
      "Epoch: 0015 cost= 0.114930528\n",
      "Optimization Finished!\n",
      "Accuracy: 0.112039\n",
      "Error after 0 iterations:0.112039\n",
      "Epoch: 0001 cost= 0.113547328\n",
      "Epoch: 0002 cost= 0.112387024\n",
      "Epoch: 0003 cost= 0.113393638\n",
      "Epoch: 0004 cost= 0.110166049\n",
      "Epoch: 0005 cost= 0.109929189\n",
      "Epoch: 0006 cost= 0.108112994\n",
      "Epoch: 0007 cost= 0.106765455\n",
      "Epoch: 0008 cost= 0.106871258\n",
      "Epoch: 0009 cost= 0.104436879\n",
      "Epoch: 0010 cost= 0.103937900\n",
      "Epoch: 0011 cost= 0.103166109\n",
      "Epoch: 0012 cost= 0.102844770\n",
      "Epoch: 0013 cost= 0.101253751\n",
      "Epoch: 0014 cost= 0.099285546\n",
      "Epoch: 0015 cost= 0.099474207\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0970474\n",
      "Error after 0 iterations:0.0970474\n",
      "Epoch: 0001 cost= 0.098463550\n",
      "Epoch: 0002 cost= 0.097472744\n",
      "Epoch: 0003 cost= 0.097528788\n",
      "Epoch: 0004 cost= 0.094436763\n",
      "Epoch: 0005 cost= 0.094459198\n",
      "Epoch: 0006 cost= 0.092911321\n",
      "Epoch: 0007 cost= 0.093669274\n",
      "Epoch: 0008 cost= 0.091968700\n",
      "Epoch: 0009 cost= 0.093294002\n",
      "Epoch: 0010 cost= 0.090461728\n",
      "Epoch: 0011 cost= 0.088195883\n",
      "Epoch: 0012 cost= 0.090213989\n",
      "Epoch: 0013 cost= 0.089147928\n",
      "Epoch: 0014 cost= 0.087736958\n",
      "Epoch: 0015 cost= 0.088686068\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0850546\n",
      "Error after 0 iterations:0.0850546\n",
      "Epoch: 0001 cost= 0.085532498\n",
      "Epoch: 0002 cost= 0.086135949\n",
      "Epoch: 0003 cost= 0.086004792\n",
      "Epoch: 0004 cost= 0.084344020\n",
      "Epoch: 0005 cost= 0.084772630\n",
      "Epoch: 0006 cost= 0.082218421\n",
      "Epoch: 0007 cost= 0.082147887\n",
      "Epoch: 0008 cost= 0.081462933\n",
      "Epoch: 0009 cost= 0.081632112\n",
      "Epoch: 0010 cost= 0.080422960\n",
      "Epoch: 0011 cost= 0.079048443\n",
      "Epoch: 0012 cost= 0.078605095\n",
      "Epoch: 0013 cost= 0.078135487\n",
      "Epoch: 0014 cost= 0.076852448\n",
      "Epoch: 0015 cost= 0.076110750\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0750833\n",
      "Error after 0 iterations:0.0750833\n",
      "Epoch: 0001 cost= 0.077415177\n",
      "Epoch: 0002 cost= 0.075987865\n",
      "Epoch: 0003 cost= 0.073883742\n",
      "Epoch: 0004 cost= 0.074659795\n",
      "Epoch: 0005 cost= 0.073504931\n",
      "Epoch: 0006 cost= 0.073560694\n",
      "Epoch: 0007 cost= 0.071904735\n",
      "Epoch: 0008 cost= 0.071009888\n",
      "Epoch: 0009 cost= 0.072846240\n",
      "Epoch: 0010 cost= 0.071401595\n",
      "Epoch: 0011 cost= 0.070399921\n",
      "Epoch: 0012 cost= 0.070502067\n",
      "Epoch: 0013 cost= 0.068797827\n",
      "Epoch: 0014 cost= 0.069129442\n",
      "Epoch: 0015 cost= 0.068166229\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0667121\n",
      "Error after 0 iterations:0.0667121\n",
      "Epoch: 0001 cost= 0.066168949\n",
      "Epoch: 0002 cost= 0.067895883\n",
      "Epoch: 0003 cost= 0.067659993\n",
      "Epoch: 0004 cost= 0.066189013\n",
      "Epoch: 0005 cost= 0.064933491\n",
      "Epoch: 0006 cost= 0.064875525\n",
      "Epoch: 0007 cost= 0.065296333\n",
      "Epoch: 0008 cost= 0.063573267\n",
      "Epoch: 0009 cost= 0.064138910\n",
      "Epoch: 0010 cost= 0.062352950\n",
      "Epoch: 0011 cost= 0.061561159\n",
      "Epoch: 0012 cost= 0.062987527\n",
      "Epoch: 0013 cost= 0.061581376\n",
      "Epoch: 0014 cost= 0.061360917\n",
      "Epoch: 0015 cost= 0.061069011\n",
      "Optimization Finished!\n",
      "Accuracy: 0.059466\n",
      "Error after 0 iterations:0.059466\n",
      "Epoch: 0001 cost= 0.060584641\n",
      "Epoch: 0002 cost= 0.060679759\n",
      "Epoch: 0003 cost= 0.059219609\n",
      "Epoch: 0004 cost= 0.059246646\n",
      "Epoch: 0005 cost= 0.058918683\n",
      "Epoch: 0006 cost= 0.057453769\n",
      "Epoch: 0007 cost= 0.059142497\n",
      "Epoch: 0008 cost= 0.057311519\n",
      "Epoch: 0009 cost= 0.057553303\n",
      "Epoch: 0010 cost= 0.056878109\n",
      "Epoch: 0011 cost= 0.055486434\n",
      "Epoch: 0012 cost= 0.056004417\n",
      "Epoch: 0013 cost= 0.055038737\n",
      "Epoch: 0014 cost= 0.054924677\n",
      "Epoch: 0015 cost= 0.053455942\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0532072\n",
      "Error after 0 iterations:0.0532072\n",
      "Epoch: 0001 cost= 0.053924987\n",
      "Epoch: 0002 cost= 0.053547463\n",
      "Epoch: 0003 cost= 0.052853929\n",
      "Epoch: 0004 cost= 0.053364626\n",
      "Epoch: 0005 cost= 0.052634537\n",
      "Epoch: 0006 cost= 0.051543725\n",
      "Epoch: 0007 cost= 0.052122825\n",
      "Epoch: 0008 cost= 0.051007863\n",
      "Epoch: 0009 cost= 0.050092195\n",
      "Epoch: 0010 cost= 0.049482488\n",
      "Epoch: 0011 cost= 0.049911764\n",
      "Epoch: 0012 cost= 0.049988496\n",
      "Epoch: 0013 cost= 0.049300329\n",
      "Epoch: 0014 cost= 0.049083300\n",
      "Epoch: 0015 cost= 0.049274565\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0476675\n",
      "Error after 0 iterations:0.0476675\n",
      "Epoch: 0001 cost= 0.047977978\n",
      "Epoch: 0002 cost= 0.047475428\n",
      "Epoch: 0003 cost= 0.047139394\n",
      "Epoch: 0004 cost= 0.047406337\n",
      "Epoch: 0005 cost= 0.047453449\n",
      "Epoch: 0006 cost= 0.046087275\n",
      "Epoch: 0007 cost= 0.045941752\n",
      "Epoch: 0008 cost= 0.046661605\n",
      "Epoch: 0009 cost= 0.045381975\n",
      "Epoch: 0010 cost= 0.044745644\n",
      "Epoch: 0011 cost= 0.044629844\n",
      "Epoch: 0012 cost= 0.044803131\n",
      "Epoch: 0013 cost= 0.043753164\n",
      "Epoch: 0014 cost= 0.043457160\n",
      "Epoch: 0015 cost= 0.044460298\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0427088\n",
      "Error after 0 iterations:0.0427088\n",
      "Epoch: 0001 cost= 0.042903084\n",
      "Epoch: 0002 cost= 0.043674634\n",
      "Epoch: 0003 cost= 0.042273197\n",
      "Epoch: 0004 cost= 0.042141888\n",
      "Epoch: 0005 cost= 0.042179315\n",
      "Epoch: 0006 cost= 0.041873763\n",
      "Epoch: 0007 cost= 0.041812723\n",
      "Epoch: 0008 cost= 0.041817351\n",
      "Epoch: 0009 cost= 0.040541782\n",
      "Epoch: 0010 cost= 0.040469306\n",
      "Epoch: 0011 cost= 0.040234438\n",
      "Epoch: 0012 cost= 0.039894967\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0390812\n",
      "Error after 0 iterations:0.0390812\n",
      "Final params:\n",
      "Epoch: 0001 cost= 8.566463947\n",
      "Epoch: 0002 cost= 1.064710212\n",
      "Epoch: 0003 cost= 0.796430969\n",
      "Epoch: 0004 cost= 0.690348804\n",
      "Epoch: 0005 cost= 0.642105854\n",
      "Epoch: 0006 cost= 0.594614196\n",
      "Epoch: 0007 cost= 0.559064019\n",
      "Epoch: 0008 cost= 0.524094748\n",
      "Epoch: 0009 cost= 0.486766696\n",
      "Epoch: 0010 cost= 0.468546420\n",
      "Epoch: 0011 cost= 0.436114985\n",
      "Epoch: 0012 cost= 0.411756277\n",
      "Epoch: 0013 cost= 0.389212108\n",
      "Epoch: 0014 cost= 0.360528725\n",
      "Epoch: 0015 cost= 0.346240360\n",
      "Optimization Finished!\n",
      "Accuracy: 0.332947\n",
      "Error after 0 iterations:0.332947\n",
      "Epoch: 0001 cost= 0.333816266\n",
      "Epoch: 0002 cost= 0.309229755\n",
      "Epoch: 0003 cost= 0.302655715\n",
      "Epoch: 0004 cost= 0.282960224\n",
      "Epoch: 0005 cost= 0.272563648\n",
      "Epoch: 0006 cost= 0.265296829\n",
      "Epoch: 0007 cost= 0.248479089\n",
      "Epoch: 0008 cost= 0.242529991\n",
      "Epoch: 0009 cost= 0.229666674\n",
      "Epoch: 0010 cost= 0.227045941\n",
      "Epoch: 0011 cost= 0.213323674\n",
      "Epoch: 0012 cost= 0.206528360\n",
      "Epoch: 0013 cost= 0.202082947\n",
      "Epoch: 0014 cost= 0.196204180\n",
      "Epoch: 0015 cost= 0.185154352\n",
      "Optimization Finished!\n",
      "Accuracy: 0.183362\n",
      "Error after 0 iterations:0.183362\n",
      "Epoch: 0001 cost= 0.185177988\n",
      "Epoch: 0002 cost= 0.176006979\n",
      "Epoch: 0003 cost= 0.168627429\n",
      "Epoch: 0004 cost= 0.168467119\n",
      "Epoch: 0005 cost= 0.162896842\n",
      "Epoch: 0006 cost= 0.160756466\n",
      "Epoch: 0007 cost= 0.154645005\n",
      "Epoch: 0008 cost= 0.150571913\n",
      "Epoch: 0009 cost= 0.150225806\n",
      "Epoch: 0010 cost= 0.143278953\n",
      "Epoch: 0011 cost= 0.143163109\n",
      "Epoch: 0012 cost= 0.137917808\n",
      "Epoch: 0013 cost= 0.133700353\n",
      "Epoch: 0014 cost= 0.132146746\n",
      "Epoch: 0015 cost= 0.132464200\n",
      "Optimization Finished!\n",
      "Accuracy: 0.12687\n",
      "Error after 0 iterations:0.12687\n",
      "Epoch: 0001 cost= 0.127495019\n",
      "Epoch: 0002 cost= 0.126423810\n",
      "Epoch: 0003 cost= 0.122304903\n",
      "Epoch: 0004 cost= 0.122292608\n",
      "Epoch: 0005 cost= 0.116570395\n",
      "Epoch: 0006 cost= 0.114487562\n",
      "Epoch: 0007 cost= 0.113745336\n",
      "Epoch: 0008 cost= 0.113937132\n",
      "Epoch: 0009 cost= 0.109732327\n",
      "Epoch: 0010 cost= 0.110923895\n",
      "Epoch: 0011 cost= 0.107965569\n",
      "Epoch: 0012 cost= 0.106585783\n",
      "Epoch: 0013 cost= 0.104301442\n",
      "Epoch: 0014 cost= 0.105843543\n",
      "Epoch: 0015 cost= 0.100753459\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0998525\n",
      "Error after 0 iterations:0.0998525\n",
      "Epoch: 0001 cost= 0.099734506\n",
      "Epoch: 0002 cost= 0.099917333\n",
      "Epoch: 0003 cost= 0.097511281\n",
      "Epoch: 0004 cost= 0.098219968\n",
      "Epoch: 0005 cost= 0.096415462\n",
      "Epoch: 0006 cost= 0.093392847\n",
      "Epoch: 0007 cost= 0.094391374\n",
      "Epoch: 0008 cost= 0.092680717\n",
      "Epoch: 0009 cost= 0.092915891\n",
      "Epoch: 0010 cost= 0.091703179\n",
      "Epoch: 0011 cost= 0.088996011\n",
      "Epoch: 0012 cost= 0.087280218\n",
      "Epoch: 0013 cost= 0.088482521\n",
      "Epoch: 0014 cost= 0.086573911\n",
      "Epoch: 0015 cost= 0.086244839\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0848296\n",
      "Error after 0 iterations:0.0848296\n",
      "Epoch: 0001 cost= 0.084971145\n",
      "Epoch: 0002 cost= 0.084309912\n",
      "Epoch: 0003 cost= 0.083791973\n",
      "Epoch: 0004 cost= 0.084584427\n",
      "Epoch: 0005 cost= 0.081617291\n",
      "Epoch: 0006 cost= 0.082801946\n",
      "Epoch: 0007 cost= 0.082823662\n",
      "Epoch: 0008 cost= 0.078996341\n",
      "Epoch: 0009 cost= 0.080115581\n",
      "Epoch: 0010 cost= 0.078821410\n",
      "Epoch: 0011 cost= 0.078726292\n",
      "Epoch: 0012 cost= 0.077802026\n",
      "Epoch: 0013 cost= 0.078393909\n",
      "Epoch: 0014 cost= 0.078035265\n",
      "Epoch: 0015 cost= 0.077601656\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0754932\n",
      "Error after 0 iterations:0.0754932\n",
      "Epoch: 0001 cost= 0.075531417\n",
      "Epoch: 0002 cost= 0.075105910\n",
      "Epoch: 0003 cost= 0.076327583\n",
      "Epoch: 0004 cost= 0.074457875\n",
      "Epoch: 0005 cost= 0.074286415\n",
      "Epoch: 0006 cost= 0.075074551\n",
      "Epoch: 0007 cost= 0.073094501\n",
      "Epoch: 0008 cost= 0.074067698\n",
      "Epoch: 0009 cost= 0.071829167\n",
      "Epoch: 0010 cost= 0.072250536\n",
      "Epoch: 0011 cost= 0.070105064\n",
      "Epoch: 0012 cost= 0.070901772\n",
      "Epoch: 0013 cost= 0.071322148\n",
      "Epoch: 0014 cost= 0.068845813\n",
      "Epoch: 0015 cost= 0.069080067\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0693448\n",
      "Error after 0 iterations:0.0693448\n",
      "Epoch: 0001 cost= 0.069503373\n",
      "Epoch: 0002 cost= 0.069107661\n",
      "Epoch: 0003 cost= 0.069306919\n",
      "Epoch: 0004 cost= 0.069003411\n",
      "Epoch: 0005 cost= 0.068499662\n",
      "Epoch: 0006 cost= 0.067780909\n",
      "Epoch: 0007 cost= 0.067542446\n",
      "Epoch: 0008 cost= 0.067936090\n",
      "Epoch: 0009 cost= 0.067400116\n",
      "Epoch: 0010 cost= 0.067066595\n",
      "Epoch: 0011 cost= 0.066455472\n",
      "Epoch: 0012 cost= 0.066700408\n",
      "Epoch: 0013 cost= 0.066432859\n",
      "Epoch: 0014 cost= 0.065332144\n",
      "Epoch: 0015 cost= 0.065632342\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0647464\n",
      "Error after 0 iterations:0.0647464\n",
      "Epoch: 0001 cost= 0.064837322\n",
      "Epoch: 0002 cost= 0.064592192\n",
      "Epoch: 0003 cost= 0.064322582\n",
      "Epoch: 0004 cost= 0.065385102\n",
      "Epoch: 0005 cost= 0.065262750\n",
      "Epoch: 0006 cost= 0.064164549\n",
      "Epoch: 0007 cost= 0.064503368\n",
      "Epoch: 0008 cost= 0.064301394\n",
      "Epoch: 0009 cost= 0.063619985\n",
      "Epoch: 0010 cost= 0.064070615\n",
      "Epoch: 0011 cost= 0.062586683\n",
      "Epoch: 0012 cost= 0.062844453\n",
      "Epoch: 0013 cost= 0.062452724\n",
      "Epoch: 0014 cost= 0.060446823\n",
      "Epoch: 0015 cost= 0.062058555\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0612448\n",
      "Error after 0 iterations:0.0612448\n",
      "Epoch: 0001 cost= 0.060893839\n",
      "Epoch: 0002 cost= 0.062190268\n",
      "Epoch: 0003 cost= 0.061269444\n",
      "Epoch: 0004 cost= 0.061101802\n",
      "Epoch: 0005 cost= 0.060952849\n",
      "Epoch: 0006 cost= 0.060279492\n",
      "Epoch: 0007 cost= 0.059256952\n",
      "Epoch: 0008 cost= 0.059650630\n",
      "Epoch: 0009 cost= 0.059484483\n",
      "Epoch: 0010 cost= 0.059861670\n",
      "Epoch: 0011 cost= 0.059457712\n",
      "Epoch: 0012 cost= 0.059182473\n",
      "Epoch: 0013 cost= 0.060134260\n",
      "Epoch: 0014 cost= 0.059177559\n",
      "Epoch: 0015 cost= 0.058948533\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0584297\n",
      "Error after 0 iterations:0.0584297\n",
      "Epoch: 0001 cost= 0.058869768\n",
      "Epoch: 0002 cost= 0.058879386\n",
      "Epoch: 0003 cost= 0.059115867\n",
      "Epoch: 0004 cost= 0.058690875\n",
      "Epoch: 0005 cost= 0.058058673\n",
      "Epoch: 0006 cost= 0.057099386\n",
      "Epoch: 0007 cost= 0.058092479\n",
      "Epoch: 0008 cost= 0.058133975\n",
      "Epoch: 0009 cost= 0.057097281\n",
      "Epoch: 0010 cost= 0.057827421\n",
      "Epoch: 0011 cost= 0.056372645\n",
      "Epoch: 0012 cost= 0.057329427\n",
      "Epoch: 0013 cost= 0.057429782\n",
      "Epoch: 0014 cost= 0.056763455\n",
      "Epoch: 0015 cost= 0.055340821\n",
      "Optimization Finished!\n",
      "Accuracy: 0.056038\n",
      "Error after 0 iterations:0.056038\n",
      "Epoch: 0001 cost= 0.055752822\n",
      "Epoch: 0002 cost= 0.055908512\n",
      "Epoch: 0003 cost= 0.055744616\n",
      "Epoch: 0004 cost= 0.056788372\n",
      "Epoch: 0005 cost= 0.055741844\n",
      "Epoch: 0006 cost= 0.056041112\n",
      "Epoch: 0007 cost= 0.055551896\n",
      "Epoch: 0008 cost= 0.055808373\n",
      "Epoch: 0009 cost= 0.055538557\n",
      "Epoch: 0010 cost= 0.054873340\n",
      "Epoch: 0011 cost= 0.055890999\n",
      "Epoch: 0012 cost= 0.054538029\n",
      "Epoch: 0013 cost= 0.054010520\n",
      "Epoch: 0014 cost= 0.055385700\n",
      "Epoch: 0015 cost= 0.055012139\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0539788\n",
      "Error after 0 iterations:0.0539788\n",
      "Epoch: 0001 cost= 0.054686516\n",
      "Epoch: 0002 cost= 0.053705826\n",
      "Epoch: 0003 cost= 0.053611966\n",
      "Epoch: 0004 cost= 0.053104047\n",
      "Epoch: 0005 cost= 0.053974798\n",
      "Epoch: 0006 cost= 0.053628957\n",
      "Epoch: 0007 cost= 0.053862391\n",
      "Epoch: 0008 cost= 0.053038710\n",
      "Epoch: 0009 cost= 0.053125345\n",
      "Epoch: 0010 cost= 0.053477424\n",
      "Epoch: 0011 cost= 0.052307987\n",
      "Epoch: 0012 cost= 0.052880688\n",
      "Epoch: 0013 cost= 0.053299719\n",
      "Epoch: 0014 cost= 0.053047597\n",
      "Epoch: 0015 cost= 0.053262226\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0522032\n",
      "Error after 0 iterations:0.0522032\n",
      "Epoch: 0001 cost= 0.053343763\n",
      "Epoch: 0002 cost= 0.053040379\n",
      "Epoch: 0003 cost= 0.052391575\n",
      "Epoch: 0004 cost= 0.052539149\n",
      "Epoch: 0005 cost= 0.052320442\n",
      "Epoch: 0006 cost= 0.052189039\n",
      "Epoch: 0007 cost= 0.051661751\n",
      "Epoch: 0008 cost= 0.052403764\n",
      "Epoch: 0009 cost= 0.052584539\n",
      "Epoch: 0010 cost= 0.051787600\n",
      "Epoch: 0011 cost= 0.052362988\n",
      "Epoch: 0012 cost= 0.051107147\n",
      "Epoch: 0013 cost= 0.052094477\n",
      "Epoch: 0014 cost= 0.051490303\n",
      "Epoch: 0015 cost= 0.051472074\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0506067\n",
      "Error after 0 iterations:0.0506067\n",
      "Epoch: 0001 cost= 0.050801662\n",
      "Epoch: 0002 cost= 0.050570377\n",
      "Epoch: 0003 cost= 0.050793492\n",
      "Epoch: 0004 cost= 0.050811878\n",
      "Epoch: 0005 cost= 0.050715162\n",
      "Epoch: 0006 cost= 0.051126106\n",
      "Epoch: 0007 cost= 0.051154767\n",
      "Epoch: 0008 cost= 0.049691425\n",
      "Epoch: 0009 cost= 0.049928429\n",
      "Epoch: 0010 cost= 0.049692575\n",
      "Epoch: 0011 cost= 0.050048838\n",
      "Epoch: 0012 cost= 0.048723960\n",
      "Epoch: 0013 cost= 0.049974027\n",
      "Epoch: 0014 cost= 0.050198421\n",
      "Epoch: 0015 cost= 0.049943878\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0491496\n",
      "Error after 0 iterations:0.0491496\n",
      "Epoch: 0001 cost= 0.049375637\n",
      "Epoch: 0002 cost= 0.049322151\n",
      "Epoch: 0003 cost= 0.049759895\n",
      "Epoch: 0004 cost= 0.048878849\n",
      "Epoch: 0005 cost= 0.049339660\n",
      "Epoch: 0006 cost= 0.048555721\n",
      "Epoch: 0007 cost= 0.049001835\n",
      "Epoch: 0008 cost= 0.049365056\n",
      "Epoch: 0009 cost= 0.049365703\n",
      "Epoch: 0010 cost= 0.047889847\n",
      "Epoch: 0011 cost= 0.047902779\n",
      "Epoch: 0012 cost= 0.047994951\n",
      "Epoch: 0013 cost= 0.047238435\n",
      "Epoch: 0014 cost= 0.047494858\n",
      "Epoch: 0015 cost= 0.048470453\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0478334\n",
      "Error after 0 iterations:0.0478334\n",
      "Epoch: 0001 cost= 0.048287441\n",
      "Epoch: 0002 cost= 0.047970657\n",
      "Epoch: 0003 cost= 0.047853217\n",
      "Epoch: 0004 cost= 0.048682059\n",
      "Epoch: 0005 cost= 0.047310498\n",
      "Epoch: 0006 cost= 0.048118959\n",
      "Epoch: 0007 cost= 0.047641085\n",
      "Epoch: 0008 cost= 0.047694085\n",
      "Epoch: 0009 cost= 0.046854388\n",
      "Epoch: 0010 cost= 0.047374241\n",
      "Epoch: 0011 cost= 0.046512441\n",
      "Epoch: 0012 cost= 0.046902710\n",
      "Epoch: 0013 cost= 0.047157067\n",
      "Epoch: 0014 cost= 0.047281326\n",
      "Epoch: 0015 cost= 0.047530643\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0465899\n",
      "Error after 0 iterations:0.0465899\n",
      "Epoch: 0001 cost= 0.047589399\n",
      "Epoch: 0002 cost= 0.046444053\n",
      "Epoch: 0003 cost= 0.046073205\n",
      "Epoch: 0004 cost= 0.046695638\n",
      "Epoch: 0005 cost= 0.046598849\n",
      "Epoch: 0006 cost= 0.046403646\n",
      "Epoch: 0007 cost= 0.046923540\n",
      "Epoch: 0008 cost= 0.046435170\n",
      "Epoch: 0009 cost= 0.047734407\n",
      "Epoch: 0010 cost= 0.045901536\n",
      "Epoch: 0011 cost= 0.046502541\n",
      "Epoch: 0012 cost= 0.045914182\n",
      "Epoch: 0013 cost= 0.046231589\n",
      "Epoch: 0014 cost= 0.045819083\n",
      "Epoch: 0015 cost= 0.045305481\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0454031\n",
      "Error after 0 iterations:0.0454031\n",
      "Epoch: 0001 cost= 0.045268896\n",
      "Epoch: 0002 cost= 0.045656554\n",
      "Epoch: 0003 cost= 0.045905023\n",
      "Epoch: 0004 cost= 0.046105563\n",
      "Epoch: 0005 cost= 0.044778515\n",
      "Epoch: 0006 cost= 0.045094290\n",
      "Epoch: 0007 cost= 0.045164354\n",
      "Epoch: 0008 cost= 0.045567322\n",
      "Epoch: 0009 cost= 0.044941000\n",
      "Epoch: 0010 cost= 0.045006262\n",
      "Epoch: 0011 cost= 0.044399160\n",
      "Epoch: 0012 cost= 0.045408165\n",
      "Epoch: 0013 cost= 0.044582972\n",
      "Epoch: 0014 cost= 0.044945883\n",
      "Epoch: 0015 cost= 0.044648310\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0442766\n",
      "Error after 0 iterations:0.0442766\n",
      "Epoch: 0001 cost= 0.045014425\n",
      "Epoch: 0002 cost= 0.044651471\n",
      "Epoch: 0003 cost= 0.044218207\n",
      "Epoch: 0004 cost= 0.043857373\n",
      "Epoch: 0005 cost= 0.043595613\n",
      "Epoch: 0006 cost= 0.044836535\n",
      "Epoch: 0007 cost= 0.043541814\n",
      "Epoch: 0008 cost= 0.044928414\n",
      "Epoch: 0009 cost= 0.043808300\n",
      "Epoch: 0010 cost= 0.043849496\n",
      "Epoch: 0011 cost= 0.044271658\n",
      "Epoch: 0012 cost= 0.043546348\n",
      "Epoch: 0013 cost= 0.043675383\n",
      "Epoch: 0014 cost= 0.043137664\n",
      "Epoch: 0015 cost= 0.043838938\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0431605\n",
      "Error after 0 iterations:0.0431605\n",
      "Epoch: 0001 cost= 0.043409174\n",
      "Epoch: 0002 cost= 0.042649242\n",
      "Epoch: 0003 cost= 0.043513295\n",
      "Epoch: 0004 cost= 0.042931935\n",
      "Epoch: 0005 cost= 0.043061178\n",
      "Epoch: 0006 cost= 0.043389377\n",
      "Epoch: 0007 cost= 0.042110616\n",
      "Epoch: 0008 cost= 0.043165375\n",
      "Epoch: 0009 cost= 0.042470063\n",
      "Epoch: 0010 cost= 0.043074392\n",
      "Epoch: 0011 cost= 0.043196395\n",
      "Epoch: 0012 cost= 0.042930654\n",
      "Epoch: 0013 cost= 0.042853247\n",
      "Epoch: 0014 cost= 0.043040925\n",
      "Epoch: 0015 cost= 0.042569283\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0420717\n",
      "Error after 0 iterations:0.0420717\n",
      "Epoch: 0001 cost= 0.042788143\n",
      "Epoch: 0002 cost= 0.042538600\n",
      "Epoch: 0003 cost= 0.042429248\n",
      "Epoch: 0004 cost= 0.041468786\n",
      "Epoch: 0005 cost= 0.041658641\n",
      "Epoch: 0006 cost= 0.041812805\n",
      "Epoch: 0007 cost= 0.041602422\n",
      "Epoch: 0008 cost= 0.041700535\n",
      "Epoch: 0009 cost= 0.042398057\n",
      "Epoch: 0010 cost= 0.041900747\n",
      "Epoch: 0011 cost= 0.040977459\n",
      "Epoch: 0012 cost= 0.041760762\n",
      "Epoch: 0013 cost= 0.041182321\n",
      "Epoch: 0014 cost= 0.040770950\n",
      "Epoch: 0015 cost= 0.041375127\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0409894\n",
      "Error after 0 iterations:0.0409894\n",
      "Epoch: 0001 cost= 0.040996078\n",
      "Epoch: 0002 cost= 0.041708016\n",
      "Epoch: 0003 cost= 0.041588204\n",
      "Epoch: 0004 cost= 0.041178996\n",
      "Epoch: 0005 cost= 0.041320446\n",
      "Epoch: 0006 cost= 0.041261443\n",
      "Epoch: 0007 cost= 0.040098912\n",
      "Epoch: 0008 cost= 0.040184060\n",
      "Epoch: 0009 cost= 0.040978369\n",
      "Epoch: 0010 cost= 0.040390159\n",
      "Epoch: 0011 cost= 0.040776813\n",
      "Epoch: 0012 cost= 0.039766789\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0401125\n",
      "Error after 0 iterations:0.0401125\n",
      "Final params:\n",
      "Epoch: 0001 cost= 10.790929222\n",
      "Epoch: 0002 cost= 6.615515518\n",
      "Epoch: 0003 cost= 5.041683769\n",
      "Epoch: 0004 cost= 3.926103354\n",
      "Epoch: 0005 cost= 3.138649893\n",
      "Epoch: 0006 cost= 2.549687576\n",
      "Epoch: 0007 cost= 2.098869371\n",
      "Epoch: 0008 cost= 1.760066390\n",
      "Epoch: 0009 cost= 1.477623272\n",
      "Epoch: 0010 cost= 1.253864884\n",
      "Epoch: 0011 cost= 1.063316727\n",
      "Epoch: 0012 cost= 0.912886786\n",
      "Epoch: 0013 cost= 0.787523949\n",
      "Epoch: 0014 cost= 0.689002657\n",
      "Epoch: 0015 cost= 0.594726467\n",
      "Optimization Finished!\n",
      "Accuracy: 0.554579\n",
      "Error after 0 iterations:0.554579\n",
      "Epoch: 0001 cost= 0.524168193\n",
      "Epoch: 0002 cost= 0.463574213\n",
      "Epoch: 0003 cost= 0.403589100\n",
      "Epoch: 0004 cost= 0.362662965\n",
      "Epoch: 0005 cost= 0.324361277\n",
      "Epoch: 0006 cost= 0.290613532\n",
      "Epoch: 0007 cost= 0.257543418\n",
      "Epoch: 0008 cost= 0.234293789\n",
      "Epoch: 0009 cost= 0.210543993\n",
      "Epoch: 0010 cost= 0.188290930\n",
      "Epoch: 0011 cost= 0.173252285\n",
      "Epoch: 0012 cost= 0.157225800\n",
      "Epoch: 0013 cost= 0.144940710\n",
      "Epoch: 0014 cost= 0.133287808\n",
      "Epoch: 0015 cost= 0.122622849\n",
      "Optimization Finished!\n",
      "Accuracy: 0.118823\n",
      "Error after 0 iterations:0.118823\n",
      "Epoch: 0001 cost= 0.116179819\n",
      "Epoch: 0002 cost= 0.107084142\n",
      "Epoch: 0003 cost= 0.099481502\n",
      "Epoch: 0004 cost= 0.094476135\n",
      "Epoch: 0005 cost= 0.089731270\n",
      "Epoch: 0006 cost= 0.083300330\n",
      "Epoch: 0007 cost= 0.078752890\n",
      "Epoch: 0008 cost= 0.076548107\n",
      "Epoch: 0009 cost= 0.073688187\n",
      "Epoch: 0010 cost= 0.069668484\n",
      "Epoch: 0011 cost= 0.067454807\n",
      "Epoch: 0012 cost= 0.066650476\n",
      "Epoch: 0013 cost= 0.064159001\n",
      "Epoch: 0014 cost= 0.061742461\n",
      "Epoch: 0015 cost= 0.060264765\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0606294\n",
      "Error after 0 iterations:0.0606294\n",
      "Epoch: 0001 cost= 0.059388682\n",
      "Epoch: 0002 cost= 0.058468252\n",
      "Epoch: 0003 cost= 0.057498447\n",
      "Epoch: 0004 cost= 0.056769572\n",
      "Epoch: 0005 cost= 0.055129112\n",
      "Epoch: 0006 cost= 0.054352273\n",
      "Epoch: 0007 cost= 0.054286887\n",
      "Epoch: 0008 cost= 0.053857258\n",
      "Epoch: 0009 cost= 0.052729324\n",
      "Epoch: 0010 cost= 0.052721773\n",
      "Epoch: 0011 cost= 0.052059088\n",
      "Epoch: 0012 cost= 0.051396886\n",
      "Epoch: 0013 cost= 0.051348340\n",
      "Epoch: 0014 cost= 0.050721341\n",
      "Epoch: 0015 cost= 0.050438755\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0513711\n",
      "Error after 0 iterations:0.0513711\n",
      "Epoch: 0001 cost= 0.050481229\n",
      "Epoch: 0002 cost= 0.051119781\n",
      "Epoch: 0003 cost= 0.050789712\n",
      "Epoch: 0004 cost= 0.049708962\n",
      "Epoch: 0005 cost= 0.049906341\n",
      "Epoch: 0006 cost= 0.049898318\n",
      "Epoch: 0007 cost= 0.049463717\n",
      "Epoch: 0008 cost= 0.049627953\n",
      "Epoch: 0009 cost= 0.049954203\n",
      "Epoch: 0010 cost= 0.049580370\n",
      "Epoch: 0011 cost= 0.049730325\n",
      "Epoch: 0012 cost= 0.048957632\n",
      "Epoch: 0013 cost= 0.049479124\n",
      "Epoch: 0014 cost= 0.048804142\n",
      "Epoch: 0015 cost= 0.048713103\n",
      "Optimization Finished!\n",
      "Accuracy: 0.049785\n",
      "Error after 0 iterations:0.049785\n",
      "Epoch: 0001 cost= 0.049452427\n",
      "Epoch: 0002 cost= 0.049430264\n",
      "Epoch: 0003 cost= 0.049377968\n",
      "Epoch: 0004 cost= 0.048862975\n",
      "Epoch: 0005 cost= 0.049392025\n",
      "Epoch: 0006 cost= 0.048459014\n",
      "Epoch: 0007 cost= 0.049795487\n",
      "Epoch: 0008 cost= 0.049122655\n",
      "Epoch: 0009 cost= 0.049053328\n",
      "Epoch: 0010 cost= 0.048358537\n",
      "Epoch: 0011 cost= 0.048060153\n",
      "Epoch: 0012 cost= 0.049410796\n",
      "Epoch: 0013 cost= 0.047786710\n",
      "Epoch: 0014 cost= 0.049458544\n",
      "Epoch: 0015 cost= 0.049285045\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0494971\n",
      "Error after 0 iterations:0.0494971\n",
      "Epoch: 0001 cost= 0.049567846\n",
      "Epoch: 0002 cost= 0.048771080\n",
      "Epoch: 0003 cost= 0.049096259\n",
      "Epoch: 0004 cost= 0.048732157\n",
      "Epoch: 0005 cost= 0.049036012\n",
      "Epoch: 0006 cost= 0.048585312\n",
      "Epoch: 0007 cost= 0.048589584\n",
      "Epoch: 0008 cost= 0.048803698\n",
      "Epoch: 0009 cost= 0.048277301\n",
      "Epoch: 0010 cost= 0.049224573\n",
      "Epoch: 0011 cost= 0.049047057\n",
      "Epoch: 0012 cost= 0.048502915\n",
      "Epoch: 0013 cost= 0.048083623\n",
      "Epoch: 0014 cost= 0.048910847\n",
      "Epoch: 0015 cost= 0.049498590\n",
      "Optimization Finished!\n",
      "Accuracy: 0.04942\n",
      "Error after 0 iterations:0.04942\n",
      "Epoch: 0001 cost= 0.049022395\n",
      "Epoch: 0002 cost= 0.049247310\n",
      "Epoch: 0003 cost= 0.048957936\n",
      "Epoch: 0004 cost= 0.048159167\n",
      "Epoch: 0005 cost= 0.048846506\n",
      "Epoch: 0006 cost= 0.049439479\n",
      "Epoch: 0007 cost= 0.048716715\n",
      "Epoch: 0008 cost= 0.048837696\n",
      "Epoch: 0009 cost= 0.048853827\n",
      "Epoch: 0010 cost= 0.048055349\n",
      "Epoch: 0011 cost= 0.049590901\n",
      "Epoch: 0012 cost= 0.047877679\n",
      "Epoch: 0013 cost= 0.048249652\n",
      "Epoch: 0014 cost= 0.048840744\n",
      "Epoch: 0015 cost= 0.048564484\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0493821\n",
      "Error after 0 iterations:0.0493821\n",
      "Epoch: 0001 cost= 0.048902089\n",
      "Epoch: 0002 cost= 0.047923821\n",
      "Epoch: 0003 cost= 0.049143681\n",
      "Epoch: 0004 cost= 0.048490671\n",
      "Epoch: 0005 cost= 0.048816292\n",
      "Epoch: 0006 cost= 0.048719263\n",
      "Epoch: 0007 cost= 0.049137305\n",
      "Epoch: 0008 cost= 0.048094285\n",
      "Epoch: 0009 cost= 0.048589945\n",
      "Epoch: 0010 cost= 0.048131168\n",
      "Epoch: 0011 cost= 0.048135771\n",
      "Epoch: 0012 cost= 0.048365120\n",
      "Epoch: 0013 cost= 0.048822995\n",
      "Epoch: 0014 cost= 0.048628632\n",
      "Epoch: 0015 cost= 0.048749568\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0493513\n",
      "Error after 0 iterations:0.0493513\n",
      "Epoch: 0001 cost= 0.048630971\n",
      "Epoch: 0002 cost= 0.049022853\n",
      "Epoch: 0003 cost= 0.048071335\n",
      "Epoch: 0004 cost= 0.048904847\n",
      "Epoch: 0005 cost= 0.049201741\n",
      "Epoch: 0006 cost= 0.047798178\n",
      "Epoch: 0007 cost= 0.048456882\n",
      "Epoch: 0008 cost= 0.049206592\n",
      "Epoch: 0009 cost= 0.048419531\n",
      "Epoch: 0010 cost= 0.048841723\n",
      "Epoch: 0011 cost= 0.048888360\n",
      "Epoch: 0012 cost= 0.049043179\n",
      "Epoch: 0013 cost= 0.048113661\n",
      "Epoch: 0014 cost= 0.048335703\n",
      "Epoch: 0015 cost= 0.048651598\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0493218\n",
      "Error after 0 iterations:0.0493218\n",
      "Epoch: 0001 cost= 0.048328601\n",
      "Epoch: 0002 cost= 0.048600413\n",
      "Epoch: 0003 cost= 0.048475151\n",
      "Epoch: 0004 cost= 0.048125194\n",
      "Epoch: 0005 cost= 0.050073416\n",
      "Epoch: 0006 cost= 0.048743097\n",
      "Epoch: 0007 cost= 0.048309336\n",
      "Epoch: 0008 cost= 0.048089091\n",
      "Epoch: 0009 cost= 0.048389865\n",
      "Epoch: 0010 cost= 0.049293631\n",
      "Epoch: 0011 cost= 0.049674571\n",
      "Epoch: 0012 cost= 0.049406606\n",
      "Epoch: 0013 cost= 0.048214621\n",
      "Epoch: 0014 cost= 0.048335665\n",
      "Epoch: 0015 cost= 0.047678391\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0492943\n",
      "Error after 0 iterations:0.0492943\n",
      "Epoch: 0001 cost= 0.048692862\n",
      "Epoch: 0002 cost= 0.047671479\n",
      "Epoch: 0003 cost= 0.047695066\n",
      "Epoch: 0004 cost= 0.048486266\n",
      "Epoch: 0005 cost= 0.048635809\n",
      "Epoch: 0006 cost= 0.047914825\n",
      "Epoch: 0007 cost= 0.048382169\n",
      "Epoch: 0008 cost= 0.048761358\n",
      "Epoch: 0009 cost= 0.049037540\n",
      "Epoch: 0010 cost= 0.049278696\n",
      "Epoch: 0011 cost= 0.047787777\n",
      "Epoch: 0012 cost= 0.048869339\n",
      "Epoch: 0013 cost= 0.048121874\n",
      "Epoch: 0014 cost= 0.048206332\n",
      "Epoch: 0015 cost= 0.048607846\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0492687\n",
      "Error after 0 iterations:0.0492687\n",
      "Epoch: 0001 cost= 0.048949092\n",
      "Epoch: 0002 cost= 0.048043349\n",
      "Epoch: 0003 cost= 0.049589956\n",
      "Epoch: 0004 cost= 0.048563167\n",
      "Epoch: 0005 cost= 0.048226089\n",
      "Epoch: 0006 cost= 0.048734631\n",
      "Epoch: 0007 cost= 0.048500268\n",
      "Epoch: 0008 cost= 0.048657852\n",
      "Epoch: 0009 cost= 0.049450391\n",
      "Epoch: 0010 cost= 0.048246451\n",
      "Epoch: 0011 cost= 0.048498495\n",
      "Epoch: 0012 cost= 0.048183686\n",
      "Epoch: 0013 cost= 0.048995344\n",
      "Epoch: 0014 cost= 0.049133755\n",
      "Epoch: 0015 cost= 0.047759859\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0492439\n",
      "Error after 0 iterations:0.0492439\n",
      "Epoch: 0001 cost= 0.047994556\n",
      "Epoch: 0002 cost= 0.048187640\n",
      "Epoch: 0003 cost= 0.048794816\n",
      "Epoch: 0004 cost= 0.048973627\n",
      "Epoch: 0005 cost= 0.048865767\n",
      "Epoch: 0006 cost= 0.049292595\n",
      "Epoch: 0007 cost= 0.048835436\n",
      "Epoch: 0008 cost= 0.048340970\n",
      "Epoch: 0009 cost= 0.048744459\n",
      "Epoch: 0010 cost= 0.047846773\n",
      "Epoch: 0011 cost= 0.048581691\n",
      "Epoch: 0012 cost= 0.048069009\n",
      "Epoch: 0013 cost= 0.048310385\n",
      "Epoch: 0014 cost= 0.048900881\n",
      "Epoch: 0015 cost= 0.048336533\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0492199\n",
      "Error after 0 iterations:0.0492199\n",
      "Epoch: 0001 cost= 0.048866355\n",
      "Epoch: 0002 cost= 0.049169998\n",
      "Epoch: 0003 cost= 0.048837877\n",
      "Epoch: 0004 cost= 0.048725035\n",
      "Epoch: 0005 cost= 0.049641018\n",
      "Epoch: 0006 cost= 0.047804546\n",
      "Epoch: 0007 cost= 0.048410941\n",
      "Epoch: 0008 cost= 0.047706359\n",
      "Epoch: 0009 cost= 0.048740654\n",
      "Epoch: 0010 cost= 0.048118470\n",
      "Epoch: 0011 cost= 0.048698383\n",
      "Epoch: 0012 cost= 0.047513823\n",
      "Epoch: 0013 cost= 0.047584718\n",
      "Epoch: 0014 cost= 0.047808725\n",
      "Epoch: 0015 cost= 0.047476926\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0480732\n",
      "Error after 0 iterations:0.0480732\n",
      "Epoch: 0001 cost= 0.047608682\n",
      "Epoch: 0002 cost= 0.047219665\n",
      "Epoch: 0003 cost= 0.046454381\n",
      "Epoch: 0004 cost= 0.046215186\n",
      "Epoch: 0005 cost= 0.046426025\n",
      "Epoch: 0006 cost= 0.044367995\n",
      "Epoch: 0007 cost= 0.042867199\n",
      "Epoch: 0008 cost= 0.042732234\n",
      "Epoch: 0009 cost= 0.041039610\n",
      "Epoch: 0010 cost= 0.040383118\n",
      "Epoch: 0011 cost= 0.039281069\n",
      "Optimization Finished!\n",
      "Accuracy: 0.0388307\n",
      "Error after 0 iterations:0.0388307\n",
      "Final params:\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "alpha,hidden_dim,hidden_dim2 = (.001,4,4)\n",
    "\n",
    "thresh = .04\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "training_epochs = 15\n",
    "batch_size = 2000\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 4 # 1st layer number of features\n",
    "n_hidden_2 = 4 # 2nd layer number of features\n",
    "n_input = 1 # Guess quadratic function\n",
    "n_classes = 1 # \n",
    "#synapses = []\n",
    "models = []\n",
    "\n",
    "#Testing starting in the same place\n",
    "#synapse0 = 2*np.random.random((1,hidden_dim)) - 1\n",
    "#synapse1 = 2*np.random.random((hidden_dim,hidden_dim2)) - 1\n",
    "#synapse2 = 2*np.random.random((hidden_dim2,1)) - 1\n",
    "copy_model = multilayer_perceptron(ind=0)\n",
    "\n",
    "for ii in xrange(3):\n",
    "\n",
    "    '''weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }'''\n",
    "\n",
    "    # Construct model with different initial weights\n",
    "    test_model = multilayer_perceptron(ind=ii)\n",
    "    \n",
    "    #Construct model with same initial weights\n",
    "    #test_model = copy.copy(copy_model)\n",
    "    #test_model.index = ii\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print test_model.weights\n",
    "    \n",
    "\n",
    "    \n",
    "    models.append(test_model)\n",
    "    with test_model.g.as_default():\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, n_input])\n",
    "        y = tf.placeholder(\"float\", [None, n_classes])\n",
    "        pred = test_model.predict(x)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "        cost = tf.reduce_mean(tf.square(pred-y))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "        #remove the comment to get random initialization\n",
    "        stopcond = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            xtest, ytest = generatecandidate4(.5,.25,.1,1000)\n",
    "\n",
    "            while stopcond:\n",
    "                #print 'epoch:' + str(e)\n",
    "                #X = []\n",
    "                #y = []\n",
    "                j = 0\n",
    "                # Training cycle\n",
    "                for epoch in range(training_epochs):\n",
    "                    avg_cost = 0.\n",
    "                    total_batch = int(10000/batch_size)\n",
    "\n",
    "                    if (avg_cost > thresh or avg_cost == 0.) and stopcond:\n",
    "                    # Loop over all batches\n",
    "                        for i in range(total_batch):\n",
    "                            batch_x, batch_y = generatecandidate4(.5,.25,.1,batch_size)\n",
    "                            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                                          y: batch_y})\n",
    "                            # Compute average loss\n",
    "                            avg_cost += c / total_batch\n",
    "                        # Display logs per epoch step\n",
    "                        if epoch % display_step == 0:\n",
    "                            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                                \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "                        if avg_cost < thresh:\n",
    "                            stopcond = False\n",
    "                            #test_model.params = sess.run(test_model.weightslist), sess.run(test_model.biaseslist)\n",
    "                            #save_path = test_model.saver.save(sess,\"/home/dfreeman/PythonFun/tmp/model\" + str(ii) + \".ckpt\")\n",
    "                            \n",
    "                print \"Optimization Finished!\"\n",
    "\n",
    "                # Test model\n",
    "                #correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                correct_prediction = tf.reduce_mean(tf.square(pred-y))\n",
    "                # Calculate accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "\n",
    "                if (j%5000) == 0:\n",
    "                    print \"Error after \"+str(j)+\" iterations:\" + str(accuracy.eval({x: xtest, y: ytest}))\n",
    "\n",
    "                if accuracy.eval({x: xtest, y: ytest}) < thresh or stopcond == False:\n",
    "                    #print \"Changing stopcond!\"\n",
    "                    stopcond = False\n",
    "                    print \"Final params:\"\n",
    "                    test_model.params = sess.run(test_model.weightslist), sess.run(test_model.biaseslist)\n",
    "                    save_path = test_model.saver.save(sess,\"/home/dfreeman/PythonFun/tmp/model\" + str(ii) + \".ckpt\")\n",
    "                j+=1\n",
    "    #remove the comment to get random initialization\n",
    "\n",
    "    \n",
    "    #synapses.append([synapse_0,synapse_1,synapse_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09790999  0.62101752  1.03343666 -0.38115153]]\n",
      "[[ 0.09790999  0.62101752  1.03343666 -0.38115153]]\n",
      "Model restored.\n",
      "[array([[ 0.09790999,  0.62101752,  1.03343666, -0.38115153]], dtype=float32), array([[ 0.44235808,  0.21506836,  0.761383  ,  0.69092709],\n",
      "       [-0.03715954,  1.03665876, -0.65453172,  0.45300439],\n",
      "       [-1.8361553 ,  0.02977604, -0.00432135,  0.1767289 ],\n",
      "       [-0.53395027,  1.61765206, -0.49521273, -1.70978189]], dtype=float32), array([[ 1.2507863 ],\n",
      "       [-0.10358144],\n",
      "       [ 0.16815047],\n",
      "       [-0.0012698 ]], dtype=float32)]\n",
      "**\n",
      "[array([-0.60335797,  0.39746606, -0.38903069,  0.09408516], dtype=float32), array([-0.19858831, -1.76788437,  1.60696042, -2.91697049], dtype=float32), array([-1.86957383], dtype=float32)]\n",
      "***\n",
      "([array([[-0.25917089, -2.03158522, -0.12641285, -0.82842195]], dtype=float32), array([[ 0.32461718, -1.12694073,  0.07095902, -0.09691573],\n",
      "       [-0.1926053 ,  0.11757999, -1.41996026,  0.05515131],\n",
      "       [-0.11004622,  0.41341448, -0.53388637, -1.1399796 ],\n",
      "       [ 0.58652341, -1.66347241, -1.35447073,  2.79148102]], dtype=float32), array([[-1.09897745],\n",
      "       [ 0.26642719],\n",
      "       [ 1.46171522],\n",
      "       [-0.94601887]], dtype=float32)], [array([-0.60335797,  0.39746606, -0.38903069,  0.09408516], dtype=float32), array([-0.19858831, -1.76788437,  1.60696042, -2.91697049], dtype=float32), array([-1.86957383], dtype=float32)])\n",
      "([array([[-0.25917089, -2.03158522, -0.12641285, -0.82842195]], dtype=float32), array([[ 0.32461718, -1.12694073,  0.07095902, -0.09691573],\n",
      "       [-0.1926053 ,  0.11757999, -1.41996026,  0.05515131],\n",
      "       [-0.11004622,  0.41341448, -0.53388637, -1.1399796 ],\n",
      "       [ 0.58652341, -1.66347241, -1.35447073,  2.79148102]], dtype=float32), array([[-1.09897745],\n",
      "       [ 0.26642719],\n",
      "       [ 1.46171522],\n",
      "       [-0.94601887]], dtype=float32)], [array([-0.60335797,  0.39746606, -0.38903069,  0.09408516], dtype=float32), array([-0.19858831, -1.76788437,  1.60696042, -2.91697049], dtype=float32), array([-1.86957383], dtype=float32)])\n"
     ]
    }
   ],
   "source": [
    "def synapse_interpolate(synapse1, synapse2, t):\n",
    "    return (synapse2-synapse1)*t + synapse1\n",
    "\n",
    "\n",
    "\n",
    "'''ii=0\n",
    "weights = {\n",
    "'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}'''\n",
    "\n",
    "#def model_interpolate(m1, m2, t):\n",
    "    \n",
    "    \n",
    "\n",
    "with models[0].g.as_default():\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Restore variables from disk.\n",
    "        \n",
    "        models[0].saver.restore(sess, \"/home/dfreeman/PythonFun/tmp/model0.ckpt\")\n",
    "        modelparams = sess.run(models[0].weightslist[0])\n",
    "        print sess.run(models[0].weights['h1'])\n",
    "        print modelparams\n",
    "\n",
    "        print(\"Model restored.\")\n",
    "        # Do some work with the model\n",
    "        \n",
    "        print models[0].ReturnParamsAsList()[0]\n",
    "        \n",
    "        print \"**\"\n",
    "        \n",
    "        print models[1].ReturnParamsAsList()[1]\n",
    "        \n",
    "        print \"***\"\n",
    "        \n",
    "        #print models[2].weightslist\n",
    "        print models[1].params\n",
    "        print models[1].ReturnParamsAsList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def synapse_interpolate(synapse1, synapse2, t):\n",
    "    return (synapse2-synapse1)*t + synapse1\n",
    "\n",
    "def model_interpolate(w1,b1,w2,b2,t):\n",
    "    \n",
    "    m1w = w1\n",
    "    m1b = b1\n",
    "    m2w = w2 \n",
    "    m2b = b2\n",
    "    \n",
    "    mwi = [synapse_interpolate(m1we,m2we,t) for m1we, m2we in zip(m1w,m2w)]\n",
    "    mbi = [synapse_interpolate(m1be,m2be,t) for m1be, m2be in zip(m1b,m2b)]\n",
    "    \n",
    "    return mwi, mbi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def synapse_interpolate(synapse1, synapse2, t):\n",
    "    return (synapse2-synapse1)*t + synapse1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WeightString:\n",
    "    \n",
    "    def __init__(self, w1, b1, w2, b2, numbeads, threshold):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        #self.w2, self.b2 = m2.params\n",
    "        self.AllBeads = []\n",
    "\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.AllBeads.append([w1,b1])\n",
    "        \n",
    "        \n",
    "        for n in xrange(numbeads):\n",
    "            ws,bs = model_interpolate(w1,b1,w2,b2, (n + 1.)/(numbeads+1.))\n",
    "            self.AllBeads.append([ws,bs])\n",
    "            \n",
    "        self.AllBeads.append([w2,b2])\n",
    "        \n",
    "        \n",
    "        self.ConvergedList = [False for f in xrange(len(self.AllBeads))]\n",
    "        self.ConvergedList[0] = True\n",
    "        self.ConvergedList[-1] = True\n",
    "    \n",
    "    \n",
    "    def SpringNorm(self, order):\n",
    "        \n",
    "        total = 0.\n",
    "        \n",
    "        #Energy between mobile beads\n",
    "        for i,b in enumerate(self.AllBeads):\n",
    "            if i < len(self.AllBeads)-1:\n",
    "                #print \"Tallying energy between bead \" + str(i) + \" and bead \" + str(i+1)\n",
    "                subtotal = 0.\n",
    "                for j in xrange(len(b)):\n",
    "                    subtotal += np.linalg.norm(np.subtract(self.AllBeads[i][0][j],self.AllBeads[i+1][0][j]),ord=order)#/len(self.beads[0][j])\n",
    "                for j in xrange(len(b)):\n",
    "                    subtotal += np.linalg.norm(np.subtract(self.AllBeads[i][1][j],self.AllBeads[i+1][1][j]),ord=order)#/len(self.beads[0][j])\n",
    "                total+=subtotal\n",
    "        \n",
    "        return total#/len(self.beads)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def SGDBead(self, bead, thresh, maxindex):\n",
    "        \n",
    "        finalerror = 0.\n",
    "        \n",
    "        #thresh = .05\n",
    "\n",
    "        # Parameters\n",
    "        learning_rate = 0.01\n",
    "        training_epochs = 15\n",
    "        batch_size = 1000\n",
    "        display_step = 1\n",
    "        \n",
    "        curWeights, curBiases = self.AllBeads[bead]\n",
    "        test_model = multilayer_perceptron(w=curWeights, b=curBiases)\n",
    "\n",
    "        with test_model.g.as_default():\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, n_input])\n",
    "            y = tf.placeholder(\"float\", [None, n_classes])\n",
    "            pred = test_model.predict(x)\n",
    "            cost = tf.reduce_mean(tf.square(pred-y))\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            init = tf.initialize_all_variables()\n",
    "            stopcond = True\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                xtest, ytest = generatecandidate4(.5,.25,.1,1000)\n",
    "                j = 0\n",
    "                while stopcond:\n",
    "                    for epoch in range(training_epochs):\n",
    "                        avg_cost = 0.\n",
    "                        total_batch = int(10000/batch_size)\n",
    "                        if (avg_cost > thresh or avg_cost == 0.) and stopcond:\n",
    "                        # Loop over all batches\n",
    "                            for i in range(total_batch):\n",
    "                                batch_x, batch_y = generatecandidate4(.5,.25,.1,batch_size)\n",
    "                                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                                              y: batch_y})\n",
    "                                # Compute average loss\n",
    "                                avg_cost += c / total_batch\n",
    "                            # Display logs per epoch step\n",
    "                            #if epoch % display_step == 0:\n",
    "                            #    print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                            #        \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "                            if avg_cost < thresh:\n",
    "                                stopcond = False\n",
    "                    #print \"Optimization Finished!\"\n",
    "\n",
    "                    # Test model\n",
    "                    #correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    correct_prediction = tf.reduce_mean(tf.square(pred-y))\n",
    "                    # Calculate accuracy\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    print \"Accuracy:\", accuracy.eval({x: xtest, y: ytest})\n",
    "\n",
    "                    #if (j%5000) == 0:\n",
    "                    #    print \"Error after \"+str(j)+\" iterations:\" + str(accuracy.eval({x: xtest, y: ytest}))\n",
    "\n",
    "                    finalerror = accuracy.eval({x: xtest, y: ytest})\n",
    "                    \n",
    "                    if finalerror < thresh or stopcond==False:# or j > maxindex:\n",
    "                        #print \"Changing stopcond!\"\n",
    "                        stopcond = False\n",
    "                        #print \"Final params:\"\n",
    "                        test_model.params = sess.run(test_model.weightslist), sess.run(test_model.biaseslist)\n",
    "                        self.AllBeads[bead]=test_model.params\n",
    "                        print \"Final bead error: \" + str(finalerror)\n",
    "                        \n",
    "                    j+=1\n",
    "\n",
    "            return finalerror\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.00979065\n"
     ]
    }
   ],
   "source": [
    "i1=0\n",
    "i2=1\n",
    "test = WeightString(models[i1].params[0],models[i1].params[1],models[i2].params[0],models[i2].params[1],1,1)\n",
    "print len(test.AllBeads)\n",
    "print test.SGDBead(1,.01,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def InterpBeadError(w1,b1, w2,b2, write = False, name = \"00\"):\n",
    "    errors = []\n",
    "    \n",
    "    xdat,ydat = generatecandidate4(.5, .25, .1, 1000)\n",
    "    xdat = np.array(xdat)\n",
    "    ydat = np.array(ydat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for tt in xrange(100):\n",
    "        #print tt\n",
    "        #accuracy = 0.\n",
    "        t = tt/100.\n",
    "        thiserror = 0\n",
    "\n",
    "        #x0 = tf.placeholder(\"float\", [None, n_input])\n",
    "        #y0 = tf.placeholder(\"float\", [None, n_classes])\n",
    "        weights, biases = model_interpolate(w1,b1,w2,b2, t)\n",
    "        interp_model = multilayer_perceptron(w=weights, b=biases)\n",
    "        \n",
    "        with interp_model.g.as_default():\n",
    "            \n",
    "            #interp_model.UpdateWeights(weights, biases)\n",
    "\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, n_input])\n",
    "            y = tf.placeholder(\"float\", [None, n_classes])\n",
    "            pred = interp_model.predict(x)\n",
    "            init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                correct_prediction = tf.reduce_mean(tf.square(pred-y))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print \"Accuracy:\", accuracy.eval({x: xdat, y: ydat}),\"\\t\",tt\n",
    "                thiserror = accuracy.eval({x: xdat, y: ydat})\n",
    "\n",
    "\n",
    "        errors.append(thiserror)\n",
    "\n",
    "    if write == True:\n",
    "        with open(\"f\" + str(name) + \".out\",'w+') as f:\n",
    "            for e in errors:\n",
    "                f.write(str(e) + \"\\n\")\n",
    "    \n",
    "    return max(errors), np.argmax(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25772259"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InterpBeadError(models[0].params[0],models[0].params[1],models[1].params[0],models[1].params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0, 1]\n",
      "Accuracy: 0.0277865\n",
      "Accuracy: 0.0182306\n",
      "Final bead error: 0.0182306\n",
      "[True, True, True]\n",
      "Accuracy: 0.0423283 \t0\n",
      "Accuracy: 0.0424689 \t1\n",
      "Accuracy: 0.0426616 \t2\n",
      "Accuracy: 0.0429018 \t3\n",
      "Accuracy: 0.0431851 \t4\n",
      "Accuracy: 0.043507 \t5\n",
      "Accuracy: 0.0438634 \t6\n",
      "Accuracy: 0.0442503 \t7\n",
      "Accuracy: 0.0446639 \t8\n",
      "Accuracy: 0.0451004 \t9\n",
      "Accuracy: 0.0455565 \t10\n",
      "Accuracy: 0.0460287 \t11\n",
      "Accuracy: 0.0465137 \t12\n",
      "Accuracy: 0.0470087 \t13\n",
      "Accuracy: 0.0475105 \t14\n",
      "Accuracy: 0.0480166 \t15\n",
      "Accuracy: 0.0485243 \t16\n",
      "Accuracy: 0.049031 \t17\n",
      "Accuracy: 0.0495345 \t18\n",
      "Accuracy: 0.0500325 \t19\n",
      "Accuracy: 0.050523 \t20\n",
      "Accuracy: 0.051004 \t21\n",
      "Accuracy: 0.0514738 \t22\n",
      "Accuracy: 0.0519305 \t23\n",
      "Accuracy: 0.0523728 \t24\n",
      "Accuracy: 0.052799 \t25\n",
      "Accuracy: 0.053208 \t26\n",
      "Accuracy: 0.0535984 \t27\n",
      "Accuracy: 0.0539519 \t28\n",
      "Accuracy: 0.05426 \t29\n",
      "Accuracy: 0.0545226 \t30\n",
      "Accuracy: 0.0547568 \t31\n",
      "Accuracy: 0.0549814 \t32\n",
      "Accuracy: 0.0551839 \t33\n",
      "Accuracy: 0.0553951 \t34\n",
      "Accuracy: 0.0555975 \t35\n",
      "Accuracy: 0.0557932 \t36\n",
      "Accuracy: 0.0559965 \t37\n",
      "Accuracy: 0.0562203 \t38\n",
      "Accuracy: 0.0564561 \t39\n",
      "Accuracy: 0.0567029 \t40\n",
      "Accuracy: 0.0569559 \t41\n",
      "Accuracy: 0.0572096 \t42\n",
      "Accuracy: 0.057468 \t43\n",
      "Accuracy: 0.0577236 \t44\n",
      "Accuracy: 0.0579749 \t45\n",
      "Accuracy: 0.0582176 \t46\n",
      "Accuracy: 0.0584549 \t47\n",
      "Accuracy: 0.0586852 \t48\n",
      "Accuracy: 0.058904 \t49\n",
      "Accuracy: 0.0591084 \t50\n",
      "Accuracy: 0.0592881 \t51\n",
      "Accuracy: 0.0594458 \t52\n",
      "Accuracy: 0.0595859 \t53\n",
      "Accuracy: 0.0597003 \t54\n",
      "Accuracy: 0.0597868 \t55\n",
      "Accuracy: 0.0598598 \t56\n",
      "Accuracy: 0.0598995 \t57\n",
      "Accuracy: 0.0598932 \t58\n",
      "Accuracy: 0.0598415 \t59\n",
      "Accuracy: 0.0597469 \t60\n",
      "Accuracy: 0.0596093 \t61\n",
      "Accuracy: 0.0594301 \t62\n",
      "Accuracy: 0.0591959 \t63\n",
      "Accuracy: 0.05891 \t64\n",
      "Accuracy: 0.0585715 \t65\n",
      "Accuracy: 0.0581958 \t66\n",
      "Accuracy: 0.0577541 \t67\n",
      "Accuracy: 0.0572719 \t68\n",
      "Accuracy: 0.0567203 \t69\n",
      "Accuracy: 0.0560836 \t70\n",
      "Accuracy: 0.0553676 \t71\n",
      "Accuracy: 0.0545665 \t72\n",
      "Accuracy: 0.0536779 \t73\n",
      "Accuracy: 0.0527312 \t74\n",
      "Accuracy: 0.0517025 \t75\n",
      "Accuracy: 0.0505939 \t76\n",
      "Accuracy: 0.0494054 \t77\n",
      "Accuracy: 0.0481438 \t78\n",
      "Accuracy: 0.0468075 \t79\n",
      "Accuracy: 0.045399 \t80\n",
      "Accuracy: 0.0439451 \t81\n",
      "Accuracy: 0.0424385 \t82\n",
      "Accuracy: 0.0408455 \t83\n",
      "Accuracy: 0.0391784 \t84\n",
      "Accuracy: 0.0374533 \t85\n",
      "Accuracy: 0.0356874 \t86\n",
      "Accuracy: 0.0338992 \t87\n",
      "Accuracy: 0.0321087 \t88\n",
      "Accuracy: 0.030337 \t89\n",
      "Accuracy: 0.0286071 \t90\n",
      "Accuracy: 0.0269429 \t91\n",
      "Accuracy: 0.0253701 \t92\n",
      "Accuracy: 0.0239155 \t93\n",
      "Accuracy: 0.0226076 \t94\n",
      "Accuracy: 0.0214764 \t95\n",
      "Accuracy: 0.0205532 \t96\n",
      "Accuracy: 0.0198709 \t97\n",
      "Accuracy: 0.0194639 \t98\n",
      "Accuracy: 0.019368 \t99\n",
      "Accuracy: 0.0175801 \t0\n",
      "Accuracy: 0.0190741 \t1\n",
      "Accuracy: 0.0206992 \t2\n",
      "Accuracy: 0.0224984 \t3\n",
      "Accuracy: 0.024563 \t4\n",
      "Accuracy: 0.0269281 \t5\n",
      "Accuracy: 0.0296059 \t6\n",
      "Accuracy: 0.0326133 \t7\n",
      "Accuracy: 0.0360061 \t8\n",
      "Accuracy: 0.0398361 \t9\n",
      "Accuracy: 0.0441531 \t10\n",
      "Accuracy: 0.0489919 \t11\n",
      "Accuracy: 0.0543967 \t12\n",
      "Accuracy: 0.059927 \t13\n",
      "Accuracy: 0.0649335 \t14\n",
      "Accuracy: 0.0696305 \t15\n",
      "Accuracy: 0.0739014 \t16\n",
      "Accuracy: 0.0777752 \t17\n",
      "Accuracy: 0.0809601 \t18\n",
      "Accuracy: 0.0837327 \t19\n",
      "Accuracy: 0.0862666 \t20\n",
      "Accuracy: 0.0885 \t21\n",
      "Accuracy: 0.0905953 \t22\n",
      "Accuracy: 0.0925334 \t23\n",
      "Accuracy: 0.0943451 \t24\n",
      "Accuracy: 0.0959154 \t25\n",
      "Accuracy: 0.0973864 \t26\n",
      "Accuracy: 0.0987426 \t27\n",
      "Accuracy: 0.0999937 \t28\n",
      "Accuracy: 0.101191 \t29\n",
      "Accuracy: 0.102312 \t30\n",
      "Accuracy: 0.103364 \t31\n",
      "Accuracy: 0.104325 \t32\n",
      "Accuracy: 0.105161 \t33\n",
      "Accuracy: 0.105837 \t34\n",
      "Accuracy: 0.106422 \t35\n",
      "Accuracy: 0.107025 \t36\n",
      "Accuracy: 0.10763 \t37\n",
      "Accuracy: 0.108168 \t38\n",
      "Accuracy: 0.108445 \t39\n",
      "Accuracy: 0.108487 \t40\n",
      "Accuracy: 0.108283 \t41\n",
      "Accuracy: 0.107803 \t42\n",
      "Accuracy: 0.107095 \t43\n",
      "Accuracy: 0.106194 \t44\n",
      "Accuracy: 0.105103 \t45\n",
      "Accuracy: 0.103833 \t46\n",
      "Accuracy: 0.102401 \t47\n",
      "Accuracy: 0.100846 \t48\n",
      "Accuracy: 0.099199 \t49\n",
      "Accuracy: 0.0974771 \t50\n",
      "Accuracy: 0.0957158 \t51\n",
      "Accuracy: 0.0939228 \t52\n",
      "Accuracy: 0.0921184 \t53\n",
      "Accuracy: 0.0903216 \t54\n",
      "Accuracy: 0.0885605 \t55\n",
      "Accuracy: 0.0868737 \t56\n",
      "Accuracy: 0.0852606 \t57\n",
      "Accuracy: 0.0836829 \t58\n",
      "Accuracy: 0.0821337 \t59\n",
      "Accuracy: 0.0806186 \t60\n",
      "Accuracy: 0.0791399 \t61\n",
      "Accuracy: 0.0776888 \t62\n",
      "Accuracy: 0.0762658 \t63\n",
      "Accuracy: 0.0748489 \t64\n",
      "Accuracy: 0.0734288 \t65\n",
      "Accuracy: 0.0720062 \t66\n",
      "Accuracy: 0.070582 \t67\n",
      "Accuracy: 0.0691575 \t68\n",
      "Accuracy: 0.0677342 \t69\n",
      "Accuracy: 0.0663135 \t70\n",
      "Accuracy: 0.0648971 \t71\n",
      "Accuracy: 0.0634868 \t72\n",
      "Accuracy: 0.0620847 \t73\n",
      "Accuracy: 0.0606929 \t74\n",
      "Accuracy: 0.0593138 \t75\n",
      "Accuracy: 0.0579498 \t76\n",
      "Accuracy: 0.0566032 \t77\n",
      "Accuracy: 0.0552766 \t78\n",
      "Accuracy: 0.0539727 \t79\n",
      "Accuracy: 0.0526947 \t80\n",
      "Accuracy: 0.0514462 \t81\n",
      "Accuracy: 0.0502294 \t82\n",
      "Accuracy: 0.0490478 \t83\n",
      "Accuracy: 0.047905 \t84\n",
      "Accuracy: 0.0468038 \t85\n",
      "Accuracy: 0.0457466 \t86\n",
      "Accuracy: 0.0447372 \t87\n",
      "Accuracy: 0.0437814 \t88\n",
      "Accuracy: 0.0428833 \t89\n",
      "Accuracy: 0.0420452 \t90\n",
      "Accuracy: 0.0412721 \t91\n",
      "Accuracy: 0.0405686 \t92\n",
      "Accuracy: 0.0399417 \t93\n",
      "Accuracy: 0.0393934 \t94\n",
      "Accuracy: 0.0389253 \t95\n",
      "Accuracy: 0.0385487 \t96\n",
      "Accuracy: 0.0382636 \t97\n",
      "Accuracy: 0.0380781 \t98\n",
      "Accuracy: 0.0379937 \t99\n",
      "[(0.05989955, 57), (0.10848656, 40)]\n",
      "[1, 0, 3, 2]\n",
      "Accuracy: 0.0193971\n",
      "Final bead error: 0.0193971\n",
      "Accuracy: 0.0181225\n",
      "Final bead error: 0.0181225\n",
      "[True, True, True, True, True]\n",
      "Accuracy: 0.0397869 \t0\n",
      "Accuracy: 0.0398243 \t1\n",
      "Accuracy: 0.0398637 \t2\n",
      "Accuracy: 0.0399048 \t3\n",
      "Accuracy: 0.0399472 \t4\n",
      "Accuracy: 0.0399907 \t5\n",
      "Accuracy: 0.040035 \t6\n",
      "Accuracy: 0.0400799 \t7\n",
      "Accuracy: 0.040125 \t8\n",
      "Accuracy: 0.0401703 \t9\n",
      "Accuracy: 0.0402154 \t10\n",
      "Accuracy: 0.0402602 \t11\n",
      "Accuracy: 0.0403045 \t12\n",
      "Accuracy: 0.0403481 \t13\n",
      "Accuracy: 0.0403909 \t14\n",
      "Accuracy: 0.0404328 \t15\n",
      "Accuracy: 0.0404735 \t16\n",
      "Accuracy: 0.0405131 \t17\n",
      "Accuracy: 0.0405513 \t18\n",
      "Accuracy: 0.0405881 \t19\n",
      "Accuracy: 0.0406235 \t20\n",
      "Accuracy: 0.0406574 \t21\n",
      "Accuracy: 0.0406897 \t22\n",
      "Accuracy: 0.0407204 \t23\n",
      "Accuracy: 0.0407495 \t24\n",
      "Accuracy: 0.040777 \t25\n",
      "Accuracy: 0.0408029 \t26\n",
      "Accuracy: 0.0408272 \t27\n",
      "Accuracy: 0.0408499 \t28\n",
      "Accuracy: 0.0408711 \t29\n",
      "Accuracy: 0.0408908 \t30\n",
      "Accuracy: 0.0409091 \t31\n",
      "Accuracy: 0.0409261 \t32\n",
      "Accuracy: 0.0409419 \t33\n",
      "Accuracy: 0.0409565 \t34\n",
      "Accuracy: 0.0409701 \t35\n",
      "Accuracy: 0.0409828 \t36\n",
      "Accuracy: 0.0409947 \t37\n",
      "Accuracy: 0.041006 \t38\n",
      "Accuracy: 0.0410168 \t39\n",
      "Accuracy: 0.0410273 \t40\n",
      "Accuracy: 0.0410376 \t41\n",
      "Accuracy: 0.0410481 \t42\n",
      "Accuracy: 0.0410587 \t43\n",
      "Accuracy: 0.0410699 \t44\n",
      "Accuracy: 0.0410817 \t45\n",
      "Accuracy: 0.0410944 \t46\n",
      "Accuracy: 0.0411082 \t47\n",
      "Accuracy: 0.041121 \t48\n",
      "Accuracy: 0.0410911 \t49\n",
      "Accuracy: 0.041018 \t50\n",
      "Accuracy: 0.0409059 \t51\n",
      "Accuracy: 0.0407762 \t52\n",
      "Accuracy: 0.0406346 \t53\n",
      "Accuracy: 0.040463 \t54\n",
      "Accuracy: 0.0402605 \t55\n",
      "Accuracy: 0.0400226 \t56\n",
      "Accuracy: 0.0397648 \t57\n",
      "Accuracy: 0.0394857 \t58\n",
      "Accuracy: 0.0391688 \t59\n",
      "Accuracy: 0.0388319 \t60\n",
      "Accuracy: 0.0384752 \t61\n",
      "Accuracy: 0.0381017 \t62\n",
      "Accuracy: 0.0377188 \t63\n",
      "Accuracy: 0.0373372 \t64\n",
      "Accuracy: 0.0369403 \t65\n",
      "Accuracy: 0.036526 \t66\n",
      "Accuracy: 0.0360907 \t67\n",
      "Accuracy: 0.0356558 \t68\n",
      "Accuracy: 0.035212 \t69\n",
      "Accuracy: 0.0347545 \t70\n",
      "Accuracy: 0.0342838 \t71\n",
      "Accuracy: 0.0338051 \t72\n",
      "Accuracy: 0.0333076 \t73\n",
      "Accuracy: 0.0328043 \t74\n",
      "Accuracy: 0.0322879 \t75\n",
      "Accuracy: 0.0317654 \t76\n",
      "Accuracy: 0.0312408 \t77\n",
      "Accuracy: 0.0307132 \t78\n",
      "Accuracy: 0.0301839 \t79\n",
      "Accuracy: 0.0296547 \t80\n",
      "Accuracy: 0.0291226 \t81\n",
      "Accuracy: 0.0285876 \t82\n",
      "Accuracy: 0.0280525 \t83\n",
      "Accuracy: 0.0275125 \t84\n",
      "Accuracy: 0.0269653 \t85\n",
      "Accuracy: 0.0264154 \t86\n",
      "Accuracy: 0.0258672 \t87\n",
      "Accuracy: 0.0253203 \t88\n",
      "Accuracy: 0.0247722 \t89\n",
      "Accuracy: 0.0242246 \t90\n",
      "Accuracy: 0.0236798 \t91\n",
      "Accuracy: 0.023138 \t92\n",
      "Accuracy: 0.0225996 \t93\n",
      "Accuracy: 0.0220642 \t94\n",
      "Accuracy: 0.0215343 \t95\n",
      "Accuracy: 0.0210088 \t96\n",
      "Accuracy: 0.0204905 \t97\n",
      "Accuracy: 0.0199798 \t98\n",
      "Accuracy: 0.0194773 \t99\n",
      "Accuracy: 0.0204237 \t0\n",
      "Accuracy: 0.0201368 \t1\n",
      "Accuracy: 0.0198718 \t2\n",
      "Accuracy: 0.0196285 \t3\n",
      "Accuracy: 0.0194064 \t4\n",
      "Accuracy: 0.0192047 \t5\n",
      "Accuracy: 0.0190226 \t6\n",
      "Accuracy: 0.0188593 \t7\n",
      "Accuracy: 0.0187143 \t8\n",
      "Accuracy: 0.0185873 \t9\n",
      "Accuracy: 0.0184792 \t10\n",
      "Accuracy: 0.0183885 \t11\n",
      "Accuracy: 0.0183149 \t12\n",
      "Accuracy: 0.0182578 \t13\n",
      "Accuracy: 0.0182168 \t14\n",
      "Accuracy: 0.0181892 \t15\n",
      "Accuracy: 0.0181752 \t16\n",
      "Accuracy: 0.0181731 \t17\n",
      "Accuracy: 0.0181843 \t18\n",
      "Accuracy: 0.0182094 \t19\n",
      "Accuracy: 0.0182474 \t20\n",
      "Accuracy: 0.0182976 \t21\n",
      "Accuracy: 0.0183583 \t22\n",
      "Accuracy: 0.0184256 \t23\n",
      "Accuracy: 0.0185008 \t24\n",
      "Accuracy: 0.0185861 \t25\n",
      "Accuracy: 0.0186773 \t26\n",
      "Accuracy: 0.0187787 \t27\n",
      "Accuracy: 0.0188908 \t28\n",
      "Accuracy: 0.0190102 \t29\n",
      "Accuracy: 0.0191346 \t30\n",
      "Accuracy: 0.0192627 \t31\n",
      "Accuracy: 0.0193957 \t32\n",
      "Accuracy: 0.0195314 \t33\n",
      "Accuracy: 0.0196705 \t34\n",
      "Accuracy: 0.0198105 \t35\n",
      "Accuracy: 0.0199488 \t36\n",
      "Accuracy: 0.0200835 \t37\n",
      "Accuracy: 0.020213 \t38\n",
      "Accuracy: 0.0203417 \t39\n",
      "Accuracy: 0.0204692 \t40\n",
      "Accuracy: 0.0205927 \t41\n",
      "Accuracy: 0.0207192 \t42\n",
      "Accuracy: 0.0208414 \t43\n",
      "Accuracy: 0.0209615 \t44\n",
      "Accuracy: 0.021075 \t45\n",
      "Accuracy: 0.0211866 \t46\n",
      "Accuracy: 0.0212942 \t47\n",
      "Accuracy: 0.0213964 \t48\n",
      "Accuracy: 0.0214942 \t49\n",
      "Accuracy: 0.0215867 \t50\n",
      "Accuracy: 0.0216772 \t51\n",
      "Accuracy: 0.0217622 \t52\n",
      "Accuracy: 0.0218392 \t53\n",
      "Accuracy: 0.0219116 \t54\n",
      "Accuracy: 0.0219773 \t55\n",
      "Accuracy: 0.0220317 \t56\n",
      "Accuracy: 0.0220718 \t57\n",
      "Accuracy: 0.0220976 \t58\n",
      "Accuracy: 0.0221094 \t59\n",
      "Accuracy: 0.0221075 \t60\n",
      "Accuracy: 0.0220921 \t61\n",
      "Accuracy: 0.0220637 \t62\n",
      "Accuracy: 0.0220224 \t63\n",
      "Accuracy: 0.0219689 \t64\n",
      "Accuracy: 0.0219036 \t65\n",
      "Accuracy: 0.021827 \t66\n",
      "Accuracy: 0.0217396 \t67\n",
      "Accuracy: 0.0216423 \t68\n",
      "Accuracy: 0.0215355 \t69\n",
      "Accuracy: 0.02142 \t70\n",
      "Accuracy: 0.0212967 \t71\n",
      "Accuracy: 0.0211663 \t72\n",
      "Accuracy: 0.0210298 \t73\n",
      "Accuracy: 0.0208879 \t74\n",
      "Accuracy: 0.0207417 \t75\n",
      "Accuracy: 0.0205923 \t76\n",
      "Accuracy: 0.0204407 \t77\n",
      "Accuracy: 0.020288 \t78\n",
      "Accuracy: 0.0201354 \t79\n",
      "Accuracy: 0.0199841 \t80\n",
      "Accuracy: 0.0198354 \t81\n",
      "Accuracy: 0.0196906 \t82\n",
      "Accuracy: 0.0195512 \t83\n",
      "Accuracy: 0.0194185 \t84\n",
      "Accuracy: 0.019294 \t85\n",
      "Accuracy: 0.0191792 \t86\n",
      "Accuracy: 0.0190758 \t87\n",
      "Accuracy: 0.0189854 \t88\n",
      "Accuracy: 0.0189097 \t89\n",
      "Accuracy: 0.0188504 \t90\n",
      "Accuracy: 0.0188093 \t91\n",
      "Accuracy: 0.0187883 \t92\n",
      "Accuracy: 0.0187893 \t93\n",
      "Accuracy: 0.0188142 \t94\n",
      "Accuracy: 0.0188651 \t95\n",
      "Accuracy: 0.0189441 \t96\n",
      "Accuracy: 0.0190532 \t97\n",
      "Accuracy: 0.0191946 \t98\n",
      "Accuracy: 0.0193706 \t99\n",
      "Accuracy: 0.0191336 \t0\n",
      "Accuracy: 0.0197237 \t1\n",
      "Accuracy: 0.0203322 \t2\n",
      "Accuracy: 0.0209601 \t3\n",
      "Accuracy: 0.0216082 \t4\n",
      "Accuracy: 0.0222776 \t5\n",
      "Accuracy: 0.0229698 \t6\n",
      "Accuracy: 0.0237003 \t7\n",
      "Accuracy: 0.0244704 \t8\n",
      "Accuracy: 0.025282 \t9\n",
      "Accuracy: 0.026139 \t10\n",
      "Accuracy: 0.0270452 \t11\n",
      "Accuracy: 0.0279981 \t12\n",
      "Accuracy: 0.0289987 \t13\n",
      "Accuracy: 0.0300578 \t14\n",
      "Accuracy: 0.0311802 \t15\n",
      "Accuracy: 0.0323679 \t16\n",
      "Accuracy: 0.0336229 \t17\n",
      "Accuracy: 0.0349506 \t18\n",
      "Accuracy: 0.0363546 \t19\n",
      "Accuracy: 0.0378337 \t20\n",
      "Accuracy: 0.0394031 \t21\n",
      "Accuracy: 0.0410656 \t22\n",
      "Accuracy: 0.0428112 \t23\n",
      "Accuracy: 0.0446427 \t24\n",
      "Accuracy: 0.0465651 \t25\n",
      "Accuracy: 0.0485839 \t26\n",
      "Accuracy: 0.0505033 \t27\n",
      "Accuracy: 0.052279 \t28\n",
      "Accuracy: 0.0538935 \t29\n",
      "Accuracy: 0.0553287 \t30\n",
      "Accuracy: 0.056567 \t31\n",
      "Accuracy: 0.0576997 \t32\n",
      "Accuracy: 0.058657 \t33\n",
      "Accuracy: 0.0594237 \t34\n",
      "Accuracy: 0.0601156 \t35\n",
      "Accuracy: 0.0606991 \t36\n",
      "Accuracy: 0.061197 \t37\n",
      "Accuracy: 0.0616715 \t38\n",
      "Accuracy: 0.0620502 \t39\n",
      "Accuracy: 0.0623444 \t40\n",
      "Accuracy: 0.0625551 \t41\n",
      "Accuracy: 0.0626738 \t42\n",
      "Accuracy: 0.0626925 \t43\n",
      "Accuracy: 0.0626534 \t44\n",
      "Accuracy: 0.0625356 \t45\n",
      "Accuracy: 0.0623945 \t46\n",
      "Accuracy: 0.0621789 \t47\n",
      "Accuracy: 0.061894 \t48\n",
      "Accuracy: 0.0615376 \t49\n",
      "Accuracy: 0.0611082 \t50\n",
      "Accuracy: 0.0606272 \t51\n",
      "Accuracy: 0.0601003 \t52\n",
      "Accuracy: 0.059521 \t53\n",
      "Accuracy: 0.0588988 \t54\n",
      "Accuracy: 0.0582157 \t55\n",
      "Accuracy: 0.0574859 \t56\n",
      "Accuracy: 0.0567328 \t57\n",
      "Accuracy: 0.055954 \t58\n",
      "Accuracy: 0.0551162 \t59\n",
      "Accuracy: 0.0542273 \t60\n",
      "Accuracy: 0.0533105 \t61\n",
      "Accuracy: 0.0523648 \t62\n",
      "Accuracy: 0.051393 \t63\n",
      "Accuracy: 0.050394 \t64\n",
      "Accuracy: 0.0493669 \t65\n",
      "Accuracy: 0.0483131 \t66\n",
      "Accuracy: 0.0472408 \t67\n",
      "Accuracy: 0.0461497 \t68\n",
      "Accuracy: 0.0450462 \t69\n",
      "Accuracy: 0.043932 \t70\n",
      "Accuracy: 0.0428147 \t71\n",
      "Accuracy: 0.0416895 \t72\n",
      "Accuracy: 0.0405536 \t73\n",
      "Accuracy: 0.0394148 \t74\n",
      "Accuracy: 0.0382739 \t75\n",
      "Accuracy: 0.0371371 \t76\n",
      "Accuracy: 0.0360079 \t77\n",
      "Accuracy: 0.0348908 \t78\n",
      "Accuracy: 0.0337897 \t79\n",
      "Accuracy: 0.0327088 \t80\n",
      "Accuracy: 0.0316525 \t81\n",
      "Accuracy: 0.0306262 \t82\n",
      "Accuracy: 0.0296382 \t83\n",
      "Accuracy: 0.0286926 \t84\n",
      "Accuracy: 0.0278 \t85\n",
      "Accuracy: 0.0269455 \t86\n",
      "Accuracy: 0.0261301 \t87\n",
      "Accuracy: 0.0253513 \t88\n",
      "Accuracy: 0.0246105 \t89\n",
      "Accuracy: 0.0238914 \t90\n",
      "Accuracy: 0.023201 \t91\n",
      "Accuracy: 0.0225356 \t92\n",
      "Accuracy: 0.0218989 \t93\n",
      "Accuracy: 0.0212982 \t94\n",
      "Accuracy: 0.0207367 \t95\n",
      "Accuracy: 0.0202154 \t96\n",
      "Accuracy: 0.0197463 \t97\n",
      "Accuracy: 0.0193406 \t98\n",
      "Accuracy: 0.0190084 \t99\n",
      "Accuracy: 0.0182896 \t0\n",
      "Accuracy: 0.0185782 \t1\n",
      "Accuracy: 0.0188669 \t2\n",
      "Accuracy: 0.0191473 \t3\n",
      "Accuracy: 0.0194229 \t4\n",
      "Accuracy: 0.0196926 \t5\n",
      "Accuracy: 0.0199578 \t6\n",
      "Accuracy: 0.0202216 \t7\n",
      "Accuracy: 0.0204855 \t8\n",
      "Accuracy: 0.0207515 \t9\n",
      "Accuracy: 0.0210225 \t10\n",
      "Accuracy: 0.0212992 \t11\n",
      "Accuracy: 0.0215838 \t12\n",
      "Accuracy: 0.0218782 \t13\n",
      "Accuracy: 0.0221895 \t14\n",
      "Accuracy: 0.02252 \t15\n",
      "Accuracy: 0.0228727 \t16\n",
      "Accuracy: 0.0232444 \t17\n",
      "Accuracy: 0.0236363 \t18\n",
      "Accuracy: 0.0240525 \t19\n",
      "Accuracy: 0.0245004 \t20\n",
      "Accuracy: 0.0249817 \t21\n",
      "Accuracy: 0.0255036 \t22\n",
      "Accuracy: 0.0260723 \t23\n",
      "Accuracy: 0.0266911 \t24\n",
      "Accuracy: 0.0273166 \t25\n",
      "Accuracy: 0.0279379 \t26\n",
      "Accuracy: 0.0285541 \t27\n",
      "Accuracy: 0.0291639 \t28\n",
      "Accuracy: 0.029763 \t29\n",
      "Accuracy: 0.030352 \t30\n",
      "Accuracy: 0.0309306 \t31\n",
      "Accuracy: 0.031498 \t32\n",
      "Accuracy: 0.0320543 \t33\n",
      "Accuracy: 0.0325972 \t34\n",
      "Accuracy: 0.0331275 \t35\n",
      "Accuracy: 0.033645 \t36\n",
      "Accuracy: 0.0341491 \t37\n",
      "Accuracy: 0.0346387 \t38\n",
      "Accuracy: 0.0351131 \t39\n",
      "Accuracy: 0.0355721 \t40\n",
      "Accuracy: 0.0360152 \t41\n",
      "Accuracy: 0.0364426 \t42\n",
      "Accuracy: 0.0368547 \t43\n",
      "Accuracy: 0.0372513 \t44\n",
      "Accuracy: 0.0376324 \t45\n",
      "Accuracy: 0.0379973 \t46\n",
      "Accuracy: 0.0383451 \t47\n",
      "Accuracy: 0.0386765 \t48\n",
      "Accuracy: 0.0389913 \t49\n",
      "Accuracy: 0.0392897 \t50\n",
      "Accuracy: 0.0395716 \t51\n",
      "Accuracy: 0.0398371 \t52\n",
      "Accuracy: 0.0400862 \t53\n",
      "Accuracy: 0.0403185 \t54\n",
      "Accuracy: 0.0405345 \t55\n",
      "Accuracy: 0.0407343 \t56\n",
      "Accuracy: 0.0409182 \t57\n",
      "Accuracy: 0.0410863 \t58\n",
      "Accuracy: 0.0412389 \t59\n",
      "Accuracy: 0.0413762 \t60\n",
      "Accuracy: 0.0414985 \t61\n",
      "Accuracy: 0.0416064 \t62\n",
      "Accuracy: 0.0417003 \t63\n",
      "Accuracy: 0.0417805 \t64\n",
      "Accuracy: 0.0418476 \t65\n",
      "Accuracy: 0.0419021 \t66\n",
      "Accuracy: 0.0419443 \t67\n",
      "Accuracy: 0.0419744 \t68\n",
      "Accuracy: 0.0419944 \t69\n",
      "Accuracy: 0.0420036 \t70\n",
      "Accuracy: 0.0420018 \t71\n",
      "Accuracy: 0.0419901 \t72\n",
      "Accuracy: 0.0419688 \t73\n",
      "Accuracy: 0.0419388 \t74\n",
      "Accuracy: 0.0419024 \t75\n",
      "Accuracy: 0.0418599 \t76\n",
      "Accuracy: 0.0418111 \t77\n",
      "Accuracy: 0.0417561 \t78\n",
      "Accuracy: 0.0416954 \t79\n",
      "Accuracy: 0.0416296 \t80\n",
      "Accuracy: 0.0415609 \t81\n",
      "Accuracy: 0.04149 \t82\n",
      "Accuracy: 0.0414187 \t83\n",
      "Accuracy: 0.0413475 \t84\n",
      "Accuracy: 0.0412764 \t85\n",
      "Accuracy: 0.0412069 \t86\n",
      "Accuracy: 0.0411376 \t87\n",
      "Accuracy: 0.04107 \t88\n",
      "Accuracy: 0.0410065 \t89\n",
      "Accuracy: 0.0409461 \t90\n",
      "Accuracy: 0.040891 \t91\n",
      "Accuracy: 0.0408452 \t92\n",
      "Accuracy: 0.0408047 \t93\n",
      "Accuracy: 0.0407726 \t94\n",
      "Accuracy: 0.0407512 \t95\n",
      "Accuracy: 0.040739 \t96\n",
      "Accuracy: 0.0407397 \t97\n",
      "Accuracy: 0.0407537 \t98\n",
      "Accuracy: 0.0407812 \t99\n",
      "[(0.041120995, 48), (0.022109443, 59), (0.062692501, 43), (0.042003606, 70)]\n",
      "[3, 2]\n",
      "Accuracy: 0.0150774\n",
      "Final bead error: 0.0150774\n",
      "[True, True, True, True, True, True]\n",
      "Accuracy: 0.019174 \t0\n",
      "Accuracy: 0.0193772 \t1\n",
      "Accuracy: 0.0195845 \t2\n",
      "Accuracy: 0.019796 \t3\n",
      "Accuracy: 0.0200121 \t4\n",
      "Accuracy: 0.0202328 \t5\n",
      "Accuracy: 0.0204584 \t6\n",
      "Accuracy: 0.0206891 \t7\n",
      "Accuracy: 0.0209252 \t8\n",
      "Accuracy: 0.0211669 \t9\n",
      "Accuracy: 0.0214145 \t10\n",
      "Accuracy: 0.0216725 \t11\n",
      "Accuracy: 0.0219435 \t12\n",
      "Accuracy: 0.0222239 \t13\n",
      "Accuracy: 0.0225159 \t14\n",
      "Accuracy: 0.0228176 \t15\n",
      "Accuracy: 0.0231322 \t16\n",
      "Accuracy: 0.0234591 \t17\n",
      "Accuracy: 0.0238031 \t18\n",
      "Accuracy: 0.0241648 \t19\n",
      "Accuracy: 0.0245428 \t20\n",
      "Accuracy: 0.0249354 \t21\n",
      "Accuracy: 0.0253438 \t22\n",
      "Accuracy: 0.0257686 \t23\n",
      "Accuracy: 0.0262092 \t24\n",
      "Accuracy: 0.0266687 \t25\n",
      "Accuracy: 0.0271451 \t26\n",
      "Accuracy: 0.0276383 \t27\n",
      "Accuracy: 0.0281507 \t28\n",
      "Accuracy: 0.0286838 \t29\n",
      "Accuracy: 0.0292393 \t30\n",
      "Accuracy: 0.0298169 \t31\n",
      "Accuracy: 0.0304153 \t32\n",
      "Accuracy: 0.0310392 \t33\n",
      "Accuracy: 0.0316888 \t34\n",
      "Accuracy: 0.0323674 \t35\n",
      "Accuracy: 0.033073 \t36\n",
      "Accuracy: 0.033801 \t37\n",
      "Accuracy: 0.0345553 \t38\n",
      "Accuracy: 0.0353363 \t39\n",
      "Accuracy: 0.0361437 \t40\n",
      "Accuracy: 0.036979 \t41\n",
      "Accuracy: 0.0377939 \t42\n",
      "Accuracy: 0.0385542 \t43\n",
      "Accuracy: 0.0392164 \t44\n",
      "Accuracy: 0.0398191 \t45\n",
      "Accuracy: 0.0403648 \t46\n",
      "Accuracy: 0.0408249 \t47\n",
      "Accuracy: 0.0411928 \t48\n",
      "Accuracy: 0.0415104 \t49\n",
      "Accuracy: 0.0417602 \t50\n",
      "Accuracy: 0.0419605 \t51\n",
      "Accuracy: 0.0421197 \t52\n",
      "Accuracy: 0.0422058 \t53\n",
      "Accuracy: 0.0422291 \t54\n",
      "Accuracy: 0.0421901 \t55\n",
      "Accuracy: 0.0421128 \t56\n",
      "Accuracy: 0.0419794 \t57\n",
      "Accuracy: 0.0418121 \t58\n",
      "Accuracy: 0.0416069 \t59\n",
      "Accuracy: 0.0413415 \t60\n",
      "Accuracy: 0.0410459 \t61\n",
      "Accuracy: 0.0407004 \t62\n",
      "Accuracy: 0.0403087 \t63\n",
      "Accuracy: 0.0398955 \t64\n",
      "Accuracy: 0.0394494 \t65\n",
      "Accuracy: 0.038977 \t66\n",
      "Accuracy: 0.0384757 \t67\n",
      "Accuracy: 0.0379395 \t68\n",
      "Accuracy: 0.0373707 \t69\n",
      "Accuracy: 0.0367743 \t70\n",
      "Accuracy: 0.0361564 \t71\n",
      "Accuracy: 0.0355125 \t72\n",
      "Accuracy: 0.034842 \t73\n",
      "Accuracy: 0.0341524 \t74\n",
      "Accuracy: 0.033434 \t75\n",
      "Accuracy: 0.032707 \t76\n",
      "Accuracy: 0.0319718 \t77\n",
      "Accuracy: 0.0312221 \t78\n",
      "Accuracy: 0.0304626 \t79\n",
      "Accuracy: 0.0296976 \t80\n",
      "Accuracy: 0.0289331 \t81\n",
      "Accuracy: 0.0281721 \t82\n",
      "Accuracy: 0.0274107 \t83\n",
      "Accuracy: 0.0266408 \t84\n",
      "Accuracy: 0.0258685 \t85\n",
      "Accuracy: 0.0251017 \t86\n",
      "Accuracy: 0.024344 \t87\n",
      "Accuracy: 0.0235973 \t88\n",
      "Accuracy: 0.0228655 \t89\n",
      "Accuracy: 0.0221445 \t90\n",
      "Accuracy: 0.021433 \t91\n",
      "Accuracy: 0.020735 \t92\n",
      "Accuracy: 0.0200514 \t93\n",
      "Accuracy: 0.0193847 \t94\n",
      "Accuracy: 0.0187369 \t95\n",
      "Accuracy: 0.0181109 \t96\n",
      "Accuracy: 0.0175093 \t97\n",
      "Accuracy: 0.016932 \t98\n",
      "Accuracy: 0.016385 \t99\n",
      "Accuracy: 0.0147135 \t0\n",
      "Accuracy: 0.0147757 \t1\n",
      "Accuracy: 0.0148405 \t2\n",
      "Accuracy: 0.014907 \t3\n",
      "Accuracy: 0.0149758 \t4\n",
      "Accuracy: 0.0150467 \t5\n",
      "Accuracy: 0.0151195 \t6\n",
      "Accuracy: 0.0151944 \t7\n",
      "Accuracy: 0.0152711 \t8\n",
      "Accuracy: 0.0153495 \t9\n",
      "Accuracy: 0.0154303 \t10\n",
      "Accuracy: 0.0155124 \t11\n",
      "Accuracy: 0.0155963 \t12\n",
      "Accuracy: 0.0156818 \t13\n",
      "Accuracy: 0.0157685 \t14\n",
      "Accuracy: 0.0158559 \t15\n",
      "Accuracy: 0.0159434 \t16\n",
      "Accuracy: 0.0160308 \t17\n",
      "Accuracy: 0.0161184 \t18\n",
      "Accuracy: 0.0162057 \t19\n",
      "Accuracy: 0.0162925 \t20\n",
      "Accuracy: 0.0163782 \t21\n",
      "Accuracy: 0.0164631 \t22\n",
      "Accuracy: 0.0165468 \t23\n",
      "Accuracy: 0.0166294 \t24\n",
      "Accuracy: 0.0167112 \t25\n",
      "Accuracy: 0.0167916 \t26\n",
      "Accuracy: 0.0168714 \t27\n",
      "Accuracy: 0.0169497 \t28\n",
      "Accuracy: 0.0170265 \t29\n",
      "Accuracy: 0.0171015 \t30\n",
      "Accuracy: 0.0171749 \t31\n",
      "Accuracy: 0.0172465 \t32\n",
      "Accuracy: 0.0173161 \t33\n",
      "Accuracy: 0.0173837 \t34\n",
      "Accuracy: 0.0174488 \t35\n",
      "Accuracy: 0.0175115 \t36\n",
      "Accuracy: 0.0175719 \t37\n",
      "Accuracy: 0.0176301 \t38\n",
      "Accuracy: 0.0176863 \t39\n",
      "Accuracy: 0.0177395 \t40\n",
      "Accuracy: 0.0177905 \t41\n",
      "Accuracy: 0.0178385 \t42\n",
      "Accuracy: 0.0178838 \t43\n",
      "Accuracy: 0.0179262 \t44\n",
      "Accuracy: 0.0179657 \t45\n",
      "Accuracy: 0.0180022 \t46\n",
      "Accuracy: 0.0180355 \t47\n",
      "Accuracy: 0.0180659 \t48\n",
      "Accuracy: 0.0180935 \t49\n",
      "Accuracy: 0.0181184 \t50\n",
      "Accuracy: 0.0181411 \t51\n",
      "Accuracy: 0.0181611 \t52\n",
      "Accuracy: 0.0181784 \t53\n",
      "Accuracy: 0.0181928 \t54\n",
      "Accuracy: 0.0182047 \t55\n",
      "Accuracy: 0.0182143 \t56\n",
      "Accuracy: 0.0182219 \t57\n",
      "Accuracy: 0.018228 \t58\n",
      "Accuracy: 0.0182323 \t59\n",
      "Accuracy: 0.0182349 \t60\n",
      "Accuracy: 0.0182363 \t61\n",
      "Accuracy: 0.0182363 \t62\n",
      "Accuracy: 0.0182355 \t63\n",
      "Accuracy: 0.0182343 \t64\n",
      "Accuracy: 0.0182322 \t65\n",
      "Accuracy: 0.0182296 \t66\n",
      "Accuracy: 0.0182276 \t67\n",
      "Accuracy: 0.0182259 \t68\n",
      "Accuracy: 0.0182254 \t69\n",
      "Accuracy: 0.0182267 \t70\n",
      "Accuracy: 0.0182297 \t71\n",
      "Accuracy: 0.0182358 \t72\n",
      "Accuracy: 0.0182443 \t73\n",
      "Accuracy: 0.0182527 \t74\n",
      "Accuracy: 0.018261 \t75\n",
      "Accuracy: 0.0182693 \t76\n",
      "Accuracy: 0.0182771 \t77\n",
      "Accuracy: 0.0182846 \t78\n",
      "Accuracy: 0.0182918 \t79\n",
      "Accuracy: 0.0182979 \t80\n",
      "Accuracy: 0.0182996 \t81\n",
      "Accuracy: 0.0182941 \t82\n",
      "Accuracy: 0.0182805 \t83\n",
      "Accuracy: 0.0182615 \t84\n",
      "Accuracy: 0.0182378 \t85\n",
      "Accuracy: 0.0182099 \t86\n",
      "Accuracy: 0.0181762 \t87\n",
      "Accuracy: 0.0181331 \t88\n",
      "Accuracy: 0.0180843 \t89\n",
      "Accuracy: 0.0180319 \t90\n",
      "Accuracy: 0.0179758 \t91\n",
      "Accuracy: 0.0179147 \t92\n",
      "Accuracy: 0.0178471 \t93\n",
      "Accuracy: 0.0177733 \t94\n",
      "Accuracy: 0.0176968 \t95\n",
      "Accuracy: 0.0176178 \t96\n",
      "Accuracy: 0.0175387 \t97\n",
      "Accuracy: 0.01746 \t98\n",
      "Accuracy: 0.0173811 \t99\n",
      "[(0.042229086, 54), (0.018299641, 81)]\n",
      "[0, 1]\n",
      "Accuracy: 0.029521\n",
      "Accuracy: 0.0219839\n",
      "Accuracy: 0.0189029\n",
      "Final bead error: 0.0189029\n",
      "[True, True, True]\n",
      "Accuracy: 0.0390626 \t0\n",
      "Accuracy: 0.0397728 \t1\n",
      "Accuracy: 0.0405221 \t2\n",
      "Accuracy: 0.0413076 \t3\n",
      "Accuracy: 0.0421267 \t4\n",
      "Accuracy: 0.0429766 \t5\n",
      "Accuracy: 0.0438546 \t6\n",
      "Accuracy: 0.0447579 \t7\n",
      "Accuracy: 0.0456838 \t8\n",
      "Accuracy: 0.0466296 \t9\n",
      "Accuracy: 0.0475925 \t10\n",
      "Accuracy: 0.0485699 \t11\n",
      "Accuracy: 0.0495589 \t12\n",
      "Accuracy: 0.0505571 \t13\n",
      "Accuracy: 0.0515617 \t14\n",
      "Accuracy: 0.0525701 \t15\n",
      "Accuracy: 0.0535797 \t16\n",
      "Accuracy: 0.0545879 \t17\n",
      "Accuracy: 0.0555922 \t18\n",
      "Accuracy: 0.0565901 \t19\n",
      "Accuracy: 0.0575791 \t20\n",
      "Accuracy: 0.0585569 \t21\n",
      "Accuracy: 0.059521 \t22\n",
      "Accuracy: 0.060469 \t23\n",
      "Accuracy: 0.0613988 \t24\n",
      "Accuracy: 0.0623081 \t25\n",
      "Accuracy: 0.0631947 \t26\n",
      "Accuracy: 0.0640566 \t27\n",
      "Accuracy: 0.0648917 \t28\n",
      "Accuracy: 0.065698 \t29\n",
      "Accuracy: 0.0664737 \t30\n",
      "Accuracy: 0.067217 \t31\n",
      "Accuracy: 0.0679259 \t32\n",
      "Accuracy: 0.068599 \t33\n",
      "Accuracy: 0.0692346 \t34\n",
      "Accuracy: 0.0698314 \t35\n",
      "Accuracy: 0.0702587 \t36\n",
      "Accuracy: 0.0704463 \t37\n",
      "Accuracy: 0.0704765 \t38\n",
      "Accuracy: 0.0704051 \t39\n",
      "Accuracy: 0.0702869 \t40\n",
      "Accuracy: 0.0701438 \t41\n",
      "Accuracy: 0.0699899 \t42\n",
      "Accuracy: 0.0698345 \t43\n",
      "Accuracy: 0.0696818 \t44\n",
      "Accuracy: 0.0695314 \t45\n",
      "Accuracy: 0.069376 \t46\n",
      "Accuracy: 0.069223 \t47\n",
      "Accuracy: 0.0690696 \t48\n",
      "Accuracy: 0.0689111 \t49\n",
      "Accuracy: 0.0687386 \t50\n",
      "Accuracy: 0.0685463 \t51\n",
      "Accuracy: 0.0683392 \t52\n",
      "Accuracy: 0.0681061 \t53\n",
      "Accuracy: 0.067839 \t54\n",
      "Accuracy: 0.0675386 \t55\n",
      "Accuracy: 0.0672051 \t56\n",
      "Accuracy: 0.0668325 \t57\n",
      "Accuracy: 0.0664169 \t58\n",
      "Accuracy: 0.0659511 \t59\n",
      "Accuracy: 0.0654359 \t60\n",
      "Accuracy: 0.0648744 \t61\n",
      "Accuracy: 0.0642597 \t62\n",
      "Accuracy: 0.0635909 \t63\n",
      "Accuracy: 0.0628669 \t64\n",
      "Accuracy: 0.0620831 \t65\n",
      "Accuracy: 0.0612451 \t66\n",
      "Accuracy: 0.0603516 \t67\n",
      "Accuracy: 0.0594014 \t68\n",
      "Accuracy: 0.0583971 \t69\n",
      "Accuracy: 0.0573421 \t70\n",
      "Accuracy: 0.0562336 \t71\n",
      "Accuracy: 0.0550764 \t72\n",
      "Accuracy: 0.0538712 \t73\n",
      "Accuracy: 0.0526147 \t74\n",
      "Accuracy: 0.0513092 \t75\n",
      "Accuracy: 0.0499598 \t76\n",
      "Accuracy: 0.0485705 \t77\n",
      "Accuracy: 0.0471429 \t78\n",
      "Accuracy: 0.0456824 \t79\n",
      "Accuracy: 0.044195 \t80\n",
      "Accuracy: 0.0426825 \t81\n",
      "Accuracy: 0.0411521 \t82\n",
      "Accuracy: 0.03961 \t83\n",
      "Accuracy: 0.0380601 \t84\n",
      "Accuracy: 0.0365089 \t85\n",
      "Accuracy: 0.0349614 \t86\n",
      "Accuracy: 0.0334258 \t87\n",
      "Accuracy: 0.0319096 \t88\n",
      "Accuracy: 0.0304239 \t89\n",
      "Accuracy: 0.0289759 \t90\n",
      "Accuracy: 0.0275707 \t91\n",
      "Accuracy: 0.0262179 \t92\n",
      "Accuracy: 0.0249269 \t93\n",
      "Accuracy: 0.0237068 \t94\n",
      "Accuracy: 0.0225727 \t95\n",
      "Accuracy: 0.0215702 \t96\n",
      "Accuracy: 0.0207013 \t97\n",
      "Accuracy: 0.01999 \t98\n",
      "Accuracy: 0.0194321 \t99\n",
      "Accuracy: 0.0197967 \t0\n",
      "Accuracy: 0.0200087 \t1\n",
      "Accuracy: 0.020623 \t2\n",
      "Accuracy: 0.0216373 \t3\n",
      "Accuracy: 0.0230579 \t4\n",
      "Accuracy: 0.0248714 \t5\n",
      "Accuracy: 0.0270752 \t6\n",
      "Accuracy: 0.0296616 \t7\n",
      "Accuracy: 0.0326298 \t8\n",
      "Accuracy: 0.0359752 \t9\n",
      "Accuracy: 0.0396949 \t10\n",
      "Accuracy: 0.0437855 \t11\n",
      "Accuracy: 0.0482437 \t12\n",
      "Accuracy: 0.0530655 \t13\n",
      "Accuracy: 0.0582481 \t14\n",
      "Accuracy: 0.0637906 \t15\n",
      "Accuracy: 0.0696913 \t16\n",
      "Accuracy: 0.0758731 \t17\n",
      "Accuracy: 0.082126 \t18\n",
      "Accuracy: 0.0882204 \t19\n",
      "Accuracy: 0.0939695 \t20\n",
      "Accuracy: 0.0994068 \t21\n",
      "Accuracy: 0.104545 \t22\n",
      "Accuracy: 0.109098 \t23\n",
      "Accuracy: 0.113044 \t24\n",
      "Accuracy: 0.116363 \t25\n",
      "Accuracy: 0.118857 \t26\n",
      "Accuracy: 0.120427 \t27\n",
      "Accuracy: 0.121052 \t28\n",
      "Accuracy: 0.120692 \t29\n",
      "Accuracy: 0.119305 \t30\n",
      "Accuracy: 0.116845 \t31\n",
      "Accuracy: 0.114268 \t32\n",
      "Accuracy: 0.111751 \t33\n",
      "Accuracy: 0.109294 \t34\n",
      "Accuracy: 0.106899 \t35\n",
      "Accuracy: 0.10457 \t36\n",
      "Accuracy: 0.102303 \t37\n",
      "Accuracy: 0.100097 \t38\n",
      "Accuracy: 0.0979507 \t39\n",
      "Accuracy: 0.0958652 \t40\n",
      "Accuracy: 0.0938383 \t41\n",
      "Accuracy: 0.0918726 \t42\n",
      "Accuracy: 0.0899592 \t43\n",
      "Accuracy: 0.0881019 \t44\n",
      "Accuracy: 0.0863019 \t45\n",
      "Accuracy: 0.0845596 \t46\n",
      "Accuracy: 0.0828715 \t47\n",
      "Accuracy: 0.0812404 \t48\n",
      "Accuracy: 0.0796652 \t49\n",
      "Accuracy: 0.078145 \t50\n",
      "Accuracy: 0.0766797 \t51\n",
      "Accuracy: 0.0752707 \t52\n",
      "Accuracy: 0.0739106 \t53\n",
      "Accuracy: 0.0725968 \t54\n",
      "Accuracy: 0.0713339 \t55\n",
      "Accuracy: 0.0701248 \t56\n",
      "Accuracy: 0.0689675 \t57\n",
      "Accuracy: 0.0678574 \t58\n",
      "Accuracy: 0.0667919 \t59\n",
      "Accuracy: 0.0657761 \t60\n",
      "Accuracy: 0.0648012 \t61\n",
      "Accuracy: 0.063872 \t62\n",
      "Accuracy: 0.0629861 \t63\n",
      "Accuracy: 0.0621447 \t64\n",
      "Accuracy: 0.0613466 \t65\n",
      "Accuracy: 0.0605929 \t66\n",
      "Accuracy: 0.0598817 \t67\n",
      "Accuracy: 0.0591986 \t68\n",
      "Accuracy: 0.058541 \t69\n",
      "Accuracy: 0.0579086 \t70\n",
      "Accuracy: 0.0573011 \t71\n",
      "Accuracy: 0.0567181 \t72\n",
      "Accuracy: 0.0561595 \t73\n",
      "Accuracy: 0.0556248 \t74\n",
      "Accuracy: 0.0551139 \t75\n",
      "Accuracy: 0.0546264 \t76\n",
      "Accuracy: 0.054162 \t77\n",
      "Accuracy: 0.0537204 \t78\n",
      "Accuracy: 0.0533014 \t79\n",
      "Accuracy: 0.0529046 \t80\n",
      "Accuracy: 0.0525299 \t81\n",
      "Accuracy: 0.0521692 \t82\n",
      "Accuracy: 0.0516918 \t83\n",
      "Accuracy: 0.0511635 \t84\n",
      "Accuracy: 0.0506192 \t85\n",
      "Accuracy: 0.0500452 \t86\n",
      "Accuracy: 0.0494164 \t87\n",
      "Accuracy: 0.0487848 \t88\n",
      "Accuracy: 0.0481636 \t89\n",
      "Accuracy: 0.047524 \t90\n",
      "Accuracy: 0.0468534 \t91\n",
      "Accuracy: 0.0461369 \t92\n",
      "Accuracy: 0.0453858 \t93\n",
      "Accuracy: 0.0446259 \t94\n",
      "Accuracy: 0.0438645 \t95\n",
      "Accuracy: 0.0430671 \t96\n",
      "Accuracy: 0.042321 \t97\n",
      "Accuracy: 0.0415712 \t98\n",
      "Accuracy: 0.0408037 \t99\n",
      "[(0.070476457, 38), (0.12105218, 28)]\n",
      "[1, 0, 3, 2]\n",
      "Accuracy: 0.0407804\n",
      "Accuracy: 0.0334014\n",
      "Accuracy: 0.026179\n",
      "Accuracy: 0.019185\n",
      "Final bead error: 0.019185\n",
      "Accuracy: 0.0249412\n",
      "Accuracy: 0.0200601\n",
      "Final bead error: 0.0200601\n",
      "[True, True, True, True, True]\n",
      "Accuracy: 0.0411401 \t0\n",
      "Accuracy: 0.0411488 \t1\n",
      "Accuracy: 0.0411612 \t2\n",
      "Accuracy: 0.0411768 \t3\n",
      "Accuracy: 0.0411952 \t4\n",
      "Accuracy: 0.0412158 \t5\n",
      "Accuracy: 0.0412382 \t6\n",
      "Accuracy: 0.041262 \t7\n",
      "Accuracy: 0.0412866 \t8\n",
      "Accuracy: 0.0413118 \t9\n",
      "Accuracy: 0.041337 \t10\n",
      "Accuracy: 0.0413618 \t11\n",
      "Accuracy: 0.041386 \t12\n",
      "Accuracy: 0.0414089 \t13\n",
      "Accuracy: 0.0414303 \t14\n",
      "Accuracy: 0.0414499 \t15\n",
      "Accuracy: 0.0414672 \t16\n",
      "Accuracy: 0.0414819 \t17\n",
      "Accuracy: 0.0414937 \t18\n",
      "Accuracy: 0.0415023 \t19\n",
      "Accuracy: 0.0415072 \t20\n",
      "Accuracy: 0.0415084 \t21\n",
      "Accuracy: 0.0415053 \t22\n",
      "Accuracy: 0.0414978 \t23\n",
      "Accuracy: 0.0414857 \t24\n",
      "Accuracy: 0.0414686 \t25\n",
      "Accuracy: 0.0414462 \t26\n",
      "Accuracy: 0.0414185 \t27\n",
      "Accuracy: 0.041385 \t28\n",
      "Accuracy: 0.0413457 \t29\n",
      "Accuracy: 0.0413004 \t30\n",
      "Accuracy: 0.0412488 \t31\n",
      "Accuracy: 0.0411907 \t32\n",
      "Accuracy: 0.0411261 \t33\n",
      "Accuracy: 0.0410547 \t34\n",
      "Accuracy: 0.0409765 \t35\n",
      "Accuracy: 0.0408912 \t36\n",
      "Accuracy: 0.0407989 \t37\n",
      "Accuracy: 0.0406993 \t38\n",
      "Accuracy: 0.0405924 \t39\n",
      "Accuracy: 0.0404782 \t40\n",
      "Accuracy: 0.0403564 \t41\n",
      "Accuracy: 0.0402273 \t42\n",
      "Accuracy: 0.0400906 \t43\n",
      "Accuracy: 0.0399463 \t44\n",
      "Accuracy: 0.0397944 \t45\n",
      "Accuracy: 0.039635 \t46\n",
      "Accuracy: 0.0394681 \t47\n",
      "Accuracy: 0.0392936 \t48\n",
      "Accuracy: 0.0391117 \t49\n",
      "Accuracy: 0.0389223 \t50\n",
      "Accuracy: 0.0387256 \t51\n",
      "Accuracy: 0.0385216 \t52\n",
      "Accuracy: 0.0383104 \t53\n",
      "Accuracy: 0.0380922 \t54\n",
      "Accuracy: 0.0378671 \t55\n",
      "Accuracy: 0.0376351 \t56\n",
      "Accuracy: 0.0373965 \t57\n",
      "Accuracy: 0.0371513 \t58\n",
      "Accuracy: 0.0368999 \t59\n",
      "Accuracy: 0.0366424 \t60\n",
      "Accuracy: 0.036379 \t61\n",
      "Accuracy: 0.0361099 \t62\n",
      "Accuracy: 0.0358353 \t63\n",
      "Accuracy: 0.0355556 \t64\n",
      "Accuracy: 0.0352709 \t65\n",
      "Accuracy: 0.0349816 \t66\n",
      "Accuracy: 0.0346879 \t67\n",
      "Accuracy: 0.0343902 \t68\n",
      "Accuracy: 0.0340887 \t69\n",
      "Accuracy: 0.033784 \t70\n",
      "Accuracy: 0.0334762 \t71\n",
      "Accuracy: 0.0331657 \t72\n",
      "Accuracy: 0.032853 \t73\n",
      "Accuracy: 0.0325384 \t74\n",
      "Accuracy: 0.0322224 \t75\n",
      "Accuracy: 0.0319054 \t76\n",
      "Accuracy: 0.0315879 \t77\n",
      "Accuracy: 0.0312702 \t78\n",
      "Accuracy: 0.030953 \t79\n",
      "Accuracy: 0.0306366 \t80\n",
      "Accuracy: 0.0303216 \t81\n",
      "Accuracy: 0.0300086 \t82\n",
      "Accuracy: 0.0296433 \t83\n",
      "Accuracy: 0.0291349 \t84\n",
      "Accuracy: 0.0285484 \t85\n",
      "Accuracy: 0.0279196 \t86\n",
      "Accuracy: 0.0272737 \t87\n",
      "Accuracy: 0.0266292 \t88\n",
      "Accuracy: 0.0259948 \t89\n",
      "Accuracy: 0.0253775 \t90\n",
      "Accuracy: 0.0247797 \t91\n",
      "Accuracy: 0.0242044 \t92\n",
      "Accuracy: 0.0236508 \t93\n",
      "Accuracy: 0.0231217 \t94\n",
      "Accuracy: 0.0226194 \t95\n",
      "Accuracy: 0.0221354 \t96\n",
      "Accuracy: 0.0216682 \t97\n",
      "Accuracy: 0.0212177 \t98\n",
      "Accuracy: 0.0207859 \t99\n",
      "Accuracy: 0.0192428 \t0\n",
      "Accuracy: 0.0193128 \t1\n",
      "Accuracy: 0.0193863 \t2\n",
      "Accuracy: 0.0194632 \t3\n",
      "Accuracy: 0.0195433 \t4\n",
      "Accuracy: 0.0196264 \t5\n",
      "Accuracy: 0.0197123 \t6\n",
      "Accuracy: 0.0198008 \t7\n",
      "Accuracy: 0.0198918 \t8\n",
      "Accuracy: 0.019985 \t9\n",
      "Accuracy: 0.0200802 \t10\n",
      "Accuracy: 0.0201773 \t11\n",
      "Accuracy: 0.020276 \t12\n",
      "Accuracy: 0.0203761 \t13\n",
      "Accuracy: 0.0204775 \t14\n",
      "Accuracy: 0.0205798 \t15\n",
      "Accuracy: 0.020683 \t16\n",
      "Accuracy: 0.0207868 \t17\n",
      "Accuracy: 0.0208909 \t18\n",
      "Accuracy: 0.0209953 \t19\n",
      "Accuracy: 0.0210995 \t20\n",
      "Accuracy: 0.0212036 \t21\n",
      "Accuracy: 0.0213071 \t22\n",
      "Accuracy: 0.02141 \t23\n",
      "Accuracy: 0.021512 \t24\n",
      "Accuracy: 0.021613 \t25\n",
      "Accuracy: 0.0217126 \t26\n",
      "Accuracy: 0.0218107 \t27\n",
      "Accuracy: 0.0219071 \t28\n",
      "Accuracy: 0.0220015 \t29\n",
      "Accuracy: 0.0220938 \t30\n",
      "Accuracy: 0.0221838 \t31\n",
      "Accuracy: 0.0222713 \t32\n",
      "Accuracy: 0.0223561 \t33\n",
      "Accuracy: 0.0224379 \t34\n",
      "Accuracy: 0.0225166 \t35\n",
      "Accuracy: 0.0225921 \t36\n",
      "Accuracy: 0.0226641 \t37\n",
      "Accuracy: 0.0227325 \t38\n",
      "Accuracy: 0.022797 \t39\n",
      "Accuracy: 0.0228576 \t40\n",
      "Accuracy: 0.022914 \t41\n",
      "Accuracy: 0.0229661 \t42\n",
      "Accuracy: 0.0230138 \t43\n",
      "Accuracy: 0.0230569 \t44\n",
      "Accuracy: 0.0230953 \t45\n",
      "Accuracy: 0.0231288 \t46\n",
      "Accuracy: 0.0231573 \t47\n",
      "Accuracy: 0.0231807 \t48\n",
      "Accuracy: 0.0231989 \t49\n",
      "Accuracy: 0.0232118 \t50\n",
      "Accuracy: 0.0232193 \t51\n",
      "Accuracy: 0.0232214 \t52\n",
      "Accuracy: 0.0232178 \t53\n",
      "Accuracy: 0.0232087 \t54\n",
      "Accuracy: 0.0231939 \t55\n",
      "Accuracy: 0.0231734 \t56\n",
      "Accuracy: 0.0231471 \t57\n",
      "Accuracy: 0.0231151 \t58\n",
      "Accuracy: 0.0230773 \t59\n",
      "Accuracy: 0.0230337 \t60\n",
      "Accuracy: 0.0229843 \t61\n",
      "Accuracy: 0.0229292 \t62\n",
      "Accuracy: 0.0228684 \t63\n",
      "Accuracy: 0.022802 \t64\n",
      "Accuracy: 0.02273 \t65\n",
      "Accuracy: 0.0226524 \t66\n",
      "Accuracy: 0.0225695 \t67\n",
      "Accuracy: 0.0224813 \t68\n",
      "Accuracy: 0.022388 \t69\n",
      "Accuracy: 0.0222896 \t70\n",
      "Accuracy: 0.0221863 \t71\n",
      "Accuracy: 0.0220784 \t72\n",
      "Accuracy: 0.0219659 \t73\n",
      "Accuracy: 0.0218492 \t74\n",
      "Accuracy: 0.0217285 \t75\n",
      "Accuracy: 0.0216039 \t76\n",
      "Accuracy: 0.0214757 \t77\n",
      "Accuracy: 0.0213443 \t78\n",
      "Accuracy: 0.02121 \t79\n",
      "Accuracy: 0.021073 \t80\n",
      "Accuracy: 0.0209338 \t81\n",
      "Accuracy: 0.0207926 \t82\n",
      "Accuracy: 0.0206499 \t83\n",
      "Accuracy: 0.0205061 \t84\n",
      "Accuracy: 0.0203616 \t85\n",
      "Accuracy: 0.0202169 \t86\n",
      "Accuracy: 0.0200724 \t87\n",
      "Accuracy: 0.0199288 \t88\n",
      "Accuracy: 0.0197864 \t89\n",
      "Accuracy: 0.0196459 \t90\n",
      "Accuracy: 0.0195135 \t91\n",
      "Accuracy: 0.0193943 \t92\n",
      "Accuracy: 0.0192877 \t93\n",
      "Accuracy: 0.0191969 \t94\n",
      "Accuracy: 0.0191196 \t95\n",
      "Accuracy: 0.0190581 \t96\n",
      "Accuracy: 0.0190106 \t97\n",
      "Accuracy: 0.0189779 \t98\n",
      "Accuracy: 0.0189572 \t99\n",
      "Accuracy: 0.020243 \t0\n",
      "Accuracy: 0.0202141 \t1\n",
      "Accuracy: 0.020186 \t2\n",
      "Accuracy: 0.0201585 \t3\n",
      "Accuracy: 0.0201317 \t4\n",
      "Accuracy: 0.0201062 \t5\n",
      "Accuracy: 0.0200812 \t6\n",
      "Accuracy: 0.0200578 \t7\n",
      "Accuracy: 0.0200358 \t8\n",
      "Accuracy: 0.0200146 \t9\n",
      "Accuracy: 0.0199938 \t10\n",
      "Accuracy: 0.0199739 \t11\n",
      "Accuracy: 0.0199544 \t12\n",
      "Accuracy: 0.0199357 \t13\n",
      "Accuracy: 0.0199173 \t14\n",
      "Accuracy: 0.0198992 \t15\n",
      "Accuracy: 0.0198819 \t16\n",
      "Accuracy: 0.0198652 \t17\n",
      "Accuracy: 0.0198489 \t18\n",
      "Accuracy: 0.0198335 \t19\n",
      "Accuracy: 0.0198186 \t20\n",
      "Accuracy: 0.0198045 \t21\n",
      "Accuracy: 0.0197918 \t22\n",
      "Accuracy: 0.0197801 \t23\n",
      "Accuracy: 0.0197691 \t24\n",
      "Accuracy: 0.0197585 \t25\n",
      "Accuracy: 0.0197483 \t26\n",
      "Accuracy: 0.0197383 \t27\n",
      "Accuracy: 0.0197289 \t28\n",
      "Accuracy: 0.01972 \t29\n",
      "Accuracy: 0.019712 \t30\n",
      "Accuracy: 0.0197044 \t31\n",
      "Accuracy: 0.0196974 \t32\n",
      "Accuracy: 0.0196912 \t33\n",
      "Accuracy: 0.0196855 \t34\n",
      "Accuracy: 0.0196801 \t35\n",
      "Accuracy: 0.0196758 \t36\n",
      "Accuracy: 0.0196719 \t37\n",
      "Accuracy: 0.019669 \t38\n",
      "Accuracy: 0.0196668 \t39\n",
      "Accuracy: 0.019665 \t40\n",
      "Accuracy: 0.0196641 \t41\n",
      "Accuracy: 0.0196645 \t42\n",
      "Accuracy: 0.0196659 \t43\n",
      "Accuracy: 0.0196682 \t44\n",
      "Accuracy: 0.0196706 \t45\n",
      "Accuracy: 0.019673 \t46\n",
      "Accuracy: 0.0196758 \t47\n",
      "Accuracy: 0.0196794 \t48\n",
      "Accuracy: 0.0196836 \t49\n",
      "Accuracy: 0.0196884 \t50\n",
      "Accuracy: 0.019694 \t51\n",
      "Accuracy: 0.0197002 \t52\n",
      "Accuracy: 0.0197065 \t53\n",
      "Accuracy: 0.0197135 \t54\n",
      "Accuracy: 0.0197212 \t55\n",
      "Accuracy: 0.0197296 \t56\n",
      "Accuracy: 0.0197385 \t57\n",
      "Accuracy: 0.0197477 \t58\n",
      "Accuracy: 0.0197575 \t59\n",
      "Accuracy: 0.0197677 \t60\n",
      "Accuracy: 0.0197782 \t61\n",
      "Accuracy: 0.0197896 \t62\n",
      "Accuracy: 0.0198019 \t63\n",
      "Accuracy: 0.0198146 \t64\n",
      "Accuracy: 0.0198282 \t65\n",
      "Accuracy: 0.0198426 \t66\n",
      "Accuracy: 0.0198575 \t67\n",
      "Accuracy: 0.0198728 \t68\n",
      "Accuracy: 0.0198893 \t69\n",
      "Accuracy: 0.0199069 \t70\n",
      "Accuracy: 0.0199253 \t71\n",
      "Accuracy: 0.0199445 \t72\n",
      "Accuracy: 0.0199643 \t73\n",
      "Accuracy: 0.0199849 \t74\n",
      "Accuracy: 0.0200064 \t75\n",
      "Accuracy: 0.0200287 \t76\n",
      "Accuracy: 0.0200517 \t77\n",
      "Accuracy: 0.0200754 \t78\n",
      "Accuracy: 0.0201004 \t79\n",
      "Accuracy: 0.0201258 \t80\n",
      "Accuracy: 0.0201523 \t81\n",
      "Accuracy: 0.02018 \t82\n",
      "Accuracy: 0.0202086 \t83\n",
      "Accuracy: 0.0202378 \t84\n",
      "Accuracy: 0.0202677 \t85\n",
      "Accuracy: 0.0202984 \t86\n",
      "Accuracy: 0.0203299 \t87\n",
      "Accuracy: 0.0203623 \t88\n",
      "Accuracy: 0.0203956 \t89\n",
      "Accuracy: 0.0204296 \t90\n",
      "Accuracy: 0.0204648 \t91\n",
      "Accuracy: 0.0205005 \t92\n",
      "Accuracy: 0.020537 \t93\n",
      "Accuracy: 0.0205742 \t94\n",
      "Accuracy: 0.0206123 \t95\n",
      "Accuracy: 0.0206512 \t96\n",
      "Accuracy: 0.0206912 \t97\n",
      "Accuracy: 0.0207322 \t98\n",
      "Accuracy: 0.0207743 \t99\n",
      "Accuracy: 0.0216279 \t0\n",
      "Accuracy: 0.023445 \t1\n",
      "Accuracy: 0.0257038 \t2\n",
      "Accuracy: 0.0282211 \t3\n",
      "Accuracy: 0.0309256 \t4\n",
      "Accuracy: 0.033755 \t5\n",
      "Accuracy: 0.0366221 \t6\n",
      "Accuracy: 0.0395311 \t7\n",
      "Accuracy: 0.0424623 \t8\n",
      "Accuracy: 0.0453361 \t9\n",
      "Accuracy: 0.0480925 \t10\n",
      "Accuracy: 0.0506435 \t11\n",
      "Accuracy: 0.0529263 \t12\n",
      "Accuracy: 0.0549498 \t13\n",
      "Accuracy: 0.0566666 \t14\n",
      "Accuracy: 0.0579023 \t15\n",
      "Accuracy: 0.0586454 \t16\n",
      "Accuracy: 0.0588588 \t17\n",
      "Accuracy: 0.0586227 \t18\n",
      "Accuracy: 0.0583065 \t19\n",
      "Accuracy: 0.0579959 \t20\n",
      "Accuracy: 0.0576909 \t21\n",
      "Accuracy: 0.0573913 \t22\n",
      "Accuracy: 0.0570968 \t23\n",
      "Accuracy: 0.0568078 \t24\n",
      "Accuracy: 0.0565245 \t25\n",
      "Accuracy: 0.0562474 \t26\n",
      "Accuracy: 0.0559767 \t27\n",
      "Accuracy: 0.0557123 \t28\n",
      "Accuracy: 0.0554538 \t29\n",
      "Accuracy: 0.0552014 \t30\n",
      "Accuracy: 0.054956 \t31\n",
      "Accuracy: 0.0547173 \t32\n",
      "Accuracy: 0.0544854 \t33\n",
      "Accuracy: 0.0542602 \t34\n",
      "Accuracy: 0.054043 \t35\n",
      "Accuracy: 0.0538327 \t36\n",
      "Accuracy: 0.0536307 \t37\n",
      "Accuracy: 0.0534363 \t38\n",
      "Accuracy: 0.0532488 \t39\n",
      "Accuracy: 0.053068 \t40\n",
      "Accuracy: 0.0528937 \t41\n",
      "Accuracy: 0.052727 \t42\n",
      "Accuracy: 0.0525675 \t43\n",
      "Accuracy: 0.0524138 \t44\n",
      "Accuracy: 0.0522677 \t45\n",
      "Accuracy: 0.0521299 \t46\n",
      "Accuracy: 0.0519991 \t47\n",
      "Accuracy: 0.0518783 \t48\n",
      "Accuracy: 0.0517653 \t49\n",
      "Accuracy: 0.0516621 \t50\n",
      "Accuracy: 0.0515677 \t51\n",
      "Accuracy: 0.0514805 \t52\n",
      "Accuracy: 0.0513996 \t53\n",
      "Accuracy: 0.0513273 \t54\n",
      "Accuracy: 0.0512636 \t55\n",
      "Accuracy: 0.0512084 \t56\n",
      "Accuracy: 0.0511608 \t57\n",
      "Accuracy: 0.0511205 \t58\n",
      "Accuracy: 0.0510858 \t59\n",
      "Accuracy: 0.0510566 \t60\n",
      "Accuracy: 0.0510293 \t61\n",
      "Accuracy: 0.0510036 \t62\n",
      "Accuracy: 0.0509793 \t63\n",
      "Accuracy: 0.0509566 \t64\n",
      "Accuracy: 0.0509354 \t65\n",
      "Accuracy: 0.0509156 \t66\n",
      "Accuracy: 0.0508973 \t67\n",
      "Accuracy: 0.0508805 \t68\n",
      "Accuracy: 0.050865 \t69\n",
      "Accuracy: 0.050851 \t70\n",
      "Accuracy: 0.0508384 \t71\n",
      "Accuracy: 0.0508272 \t72\n",
      "Accuracy: 0.0508173 \t73\n",
      "Accuracy: 0.0508089 \t74\n",
      "Accuracy: 0.050802 \t75\n",
      "Accuracy: 0.0507906 \t76\n",
      "Accuracy: 0.0507231 \t77\n",
      "Accuracy: 0.0505589 \t78\n",
      "Accuracy: 0.0503091 \t79\n",
      "Accuracy: 0.0499933 \t80\n",
      "Accuracy: 0.0496452 \t81\n",
      "Accuracy: 0.0492533 \t82\n",
      "Accuracy: 0.0488411 \t83\n",
      "Accuracy: 0.0484305 \t84\n",
      "Accuracy: 0.0479977 \t85\n",
      "Accuracy: 0.0475396 \t86\n",
      "Accuracy: 0.0470824 \t87\n",
      "Accuracy: 0.0466181 \t88\n",
      "Accuracy: 0.0461474 \t89\n",
      "Accuracy: 0.0456183 \t90\n",
      "Accuracy: 0.0450643 \t91\n",
      "Accuracy: 0.0445212 \t92\n",
      "Accuracy: 0.0439625 \t93\n",
      "Accuracy: 0.0434002 \t94\n",
      "Accuracy: 0.0428441 \t95\n",
      "Accuracy: 0.0422689 \t96\n",
      "Accuracy: 0.0416949 \t97\n",
      "Accuracy: 0.0411297 \t98\n",
      "Accuracy: 0.0405514 \t99\n",
      "[(0.041508354, 21), (0.023221364, 52), (0.020774256, 99), (0.05885876, 17)]\n",
      "[4, 3]\n",
      "Accuracy: 0.0486189\n",
      "Accuracy: 0.0220093\n",
      "Accuracy: 0.0197364\n",
      "Final bead error: 0.0197364\n",
      "[True, True, True, True, True, True]\n",
      "Accuracy: 0.0204262 \t0\n",
      "Accuracy: 0.0204177 \t1\n",
      "Accuracy: 0.0204092 \t2\n",
      "Accuracy: 0.0204009 \t3\n",
      "Accuracy: 0.0203926 \t4\n",
      "Accuracy: 0.0203844 \t5\n",
      "Accuracy: 0.0203763 \t6\n",
      "Accuracy: 0.0203682 \t7\n",
      "Accuracy: 0.0203603 \t8\n",
      "Accuracy: 0.0203524 \t9\n",
      "Accuracy: 0.0203445 \t10\n",
      "Accuracy: 0.0203368 \t11\n",
      "Accuracy: 0.0203291 \t12\n",
      "Accuracy: 0.0203214 \t13\n",
      "Accuracy: 0.0203139 \t14\n",
      "Accuracy: 0.0203064 \t15\n",
      "Accuracy: 0.0202991 \t16\n",
      "Accuracy: 0.0202918 \t17\n",
      "Accuracy: 0.0202846 \t18\n",
      "Accuracy: 0.0202775 \t19\n",
      "Accuracy: 0.0202704 \t20\n",
      "Accuracy: 0.0202634 \t21\n",
      "Accuracy: 0.0202564 \t22\n",
      "Accuracy: 0.0202496 \t23\n",
      "Accuracy: 0.0202428 \t24\n",
      "Accuracy: 0.0202361 \t25\n",
      "Accuracy: 0.0202294 \t26\n",
      "Accuracy: 0.0202228 \t27\n",
      "Accuracy: 0.0202163 \t28\n",
      "Accuracy: 0.0202099 \t29\n",
      "Accuracy: 0.0202035 \t30\n",
      "Accuracy: 0.0201971 \t31\n",
      "Accuracy: 0.0201908 \t32\n",
      "Accuracy: 0.0201846 \t33\n",
      "Accuracy: 0.0201785 \t34\n",
      "Accuracy: 0.0201724 \t35\n",
      "Accuracy: 0.0201663 \t36\n",
      "Accuracy: 0.0201604 \t37\n",
      "Accuracy: 0.0201545 \t38\n",
      "Accuracy: 0.0201486 \t39\n",
      "Accuracy: 0.020143 \t40\n",
      "Accuracy: 0.0201378 \t41\n",
      "Accuracy: 0.0201326 \t42\n",
      "Accuracy: 0.0201275 \t43\n",
      "Accuracy: 0.0201226 \t44\n",
      "Accuracy: 0.0201177 \t45\n",
      "Accuracy: 0.0201129 \t46\n",
      "Accuracy: 0.0201082 \t47\n",
      "Accuracy: 0.0201035 \t48\n",
      "Accuracy: 0.0200989 \t49\n",
      "Accuracy: 0.0200944 \t50\n",
      "Accuracy: 0.0200899 \t51\n",
      "Accuracy: 0.0200855 \t52\n",
      "Accuracy: 0.0200812 \t53\n",
      "Accuracy: 0.020077 \t54\n",
      "Accuracy: 0.0200728 \t55\n",
      "Accuracy: 0.0200687 \t56\n",
      "Accuracy: 0.0200647 \t57\n",
      "Accuracy: 0.0200607 \t58\n",
      "Accuracy: 0.0200567 \t59\n",
      "Accuracy: 0.0200529 \t60\n",
      "Accuracy: 0.0200491 \t61\n",
      "Accuracy: 0.0200454 \t62\n",
      "Accuracy: 0.0200417 \t63\n",
      "Accuracy: 0.0200381 \t64\n",
      "Accuracy: 0.0200346 \t65\n",
      "Accuracy: 0.0200311 \t66\n",
      "Accuracy: 0.0200277 \t67\n",
      "Accuracy: 0.0200243 \t68\n",
      "Accuracy: 0.020021 \t69\n",
      "Accuracy: 0.0200177 \t70\n",
      "Accuracy: 0.0200145 \t71\n",
      "Accuracy: 0.0200113 \t72\n",
      "Accuracy: 0.0200082 \t73\n",
      "Accuracy: 0.0200051 \t74\n",
      "Accuracy: 0.0200021 \t75\n",
      "Accuracy: 0.0199992 \t76\n",
      "Accuracy: 0.0199963 \t77\n",
      "Accuracy: 0.0199935 \t78\n",
      "Accuracy: 0.0199908 \t79\n",
      "Accuracy: 0.0199881 \t80\n",
      "Accuracy: 0.0199855 \t81\n",
      "Accuracy: 0.0199829 \t82\n",
      "Accuracy: 0.0199804 \t83\n",
      "Accuracy: 0.0199779 \t84\n",
      "Accuracy: 0.0199755 \t85\n",
      "Accuracy: 0.0199732 \t86\n",
      "Accuracy: 0.0199709 \t87\n",
      "Accuracy: 0.0199686 \t88\n",
      "Accuracy: 0.0199665 \t89\n",
      "Accuracy: 0.0199643 \t90\n",
      "Accuracy: 0.0199623 \t91\n",
      "Accuracy: 0.0199602 \t92\n",
      "Accuracy: 0.0199583 \t93\n",
      "Accuracy: 0.0199563 \t94\n",
      "Accuracy: 0.0199545 \t95\n",
      "Accuracy: 0.0199526 \t96\n",
      "Accuracy: 0.0199509 \t97\n",
      "Accuracy: 0.0199491 \t98\n",
      "Accuracy: 0.0199474 \t99\n",
      "Accuracy: 0.0180867 \t0\n",
      "Accuracy: 0.0198475 \t1\n",
      "Accuracy: 0.022153 \t2\n",
      "Accuracy: 0.0247582 \t3\n",
      "Accuracy: 0.027622 \t4\n",
      "Accuracy: 0.0306812 \t5\n",
      "Accuracy: 0.0338234 \t6\n",
      "Accuracy: 0.0369779 \t7\n",
      "Accuracy: 0.0400631 \t8\n",
      "Accuracy: 0.0430925 \t9\n",
      "Accuracy: 0.0460233 \t10\n",
      "Accuracy: 0.0487659 \t11\n",
      "Accuracy: 0.0512479 \t12\n",
      "Accuracy: 0.0533827 \t13\n",
      "Accuracy: 0.0551471 \t14\n",
      "Accuracy: 0.0564141 \t15\n",
      "Accuracy: 0.0571515 \t16\n",
      "Accuracy: 0.057423 \t17\n",
      "Accuracy: 0.0572233 \t18\n",
      "Accuracy: 0.0569041 \t19\n",
      "Accuracy: 0.0565896 \t20\n",
      "Accuracy: 0.0562812 \t21\n",
      "Accuracy: 0.0559787 \t22\n",
      "Accuracy: 0.0556821 \t23\n",
      "Accuracy: 0.0553909 \t24\n",
      "Accuracy: 0.0551051 \t25\n",
      "Accuracy: 0.0548243 \t26\n",
      "Accuracy: 0.0545484 \t27\n",
      "Accuracy: 0.0542785 \t28\n",
      "Accuracy: 0.054014 \t29\n",
      "Accuracy: 0.0537547 \t30\n",
      "Accuracy: 0.0535004 \t31\n",
      "Accuracy: 0.053252 \t32\n",
      "Accuracy: 0.0530098 \t33\n",
      "Accuracy: 0.0527738 \t34\n",
      "Accuracy: 0.0525445 \t35\n",
      "Accuracy: 0.0523207 \t36\n",
      "Accuracy: 0.0521037 \t37\n",
      "Accuracy: 0.0518933 \t38\n",
      "Accuracy: 0.0516908 \t39\n",
      "Accuracy: 0.0514947 \t40\n",
      "Accuracy: 0.0513056 \t41\n",
      "Accuracy: 0.0511224 \t42\n",
      "Accuracy: 0.0509466 \t43\n",
      "Accuracy: 0.0507769 \t44\n",
      "Accuracy: 0.0506153 \t45\n",
      "Accuracy: 0.0504611 \t46\n",
      "Accuracy: 0.0503143 \t47\n",
      "Accuracy: 0.0501757 \t48\n",
      "Accuracy: 0.0500456 \t49\n",
      "Accuracy: 0.0499232 \t50\n",
      "Accuracy: 0.0498069 \t51\n",
      "Accuracy: 0.0496977 \t52\n",
      "Accuracy: 0.0495958 \t53\n",
      "Accuracy: 0.0494996 \t54\n",
      "Accuracy: 0.0494098 \t55\n",
      "Accuracy: 0.0493243 \t56\n",
      "Accuracy: 0.0492477 \t57\n",
      "Accuracy: 0.0491784 \t58\n",
      "Accuracy: 0.0491116 \t59\n",
      "Accuracy: 0.0490463 \t60\n",
      "Accuracy: 0.0489824 \t61\n",
      "Accuracy: 0.0489201 \t62\n",
      "Accuracy: 0.0488592 \t63\n",
      "Accuracy: 0.0487998 \t64\n",
      "Accuracy: 0.0487419 \t65\n",
      "Accuracy: 0.0486854 \t66\n",
      "Accuracy: 0.0486303 \t67\n",
      "Accuracy: 0.0485767 \t68\n",
      "Accuracy: 0.0485249 \t69\n",
      "Accuracy: 0.0484751 \t70\n",
      "Accuracy: 0.0484124 \t71\n",
      "Accuracy: 0.0483259 \t72\n",
      "Accuracy: 0.0482018 \t73\n",
      "Accuracy: 0.0480627 \t74\n",
      "Accuracy: 0.0479204 \t75\n",
      "Accuracy: 0.0477589 \t76\n",
      "Accuracy: 0.0475844 \t77\n",
      "Accuracy: 0.0473904 \t78\n",
      "Accuracy: 0.0471761 \t79\n",
      "Accuracy: 0.0469382 \t80\n",
      "Accuracy: 0.0466631 \t81\n",
      "Accuracy: 0.0463692 \t82\n",
      "Accuracy: 0.0460678 \t83\n",
      "Accuracy: 0.0457606 \t84\n",
      "Accuracy: 0.0454464 \t85\n",
      "Accuracy: 0.0451309 \t86\n",
      "Accuracy: 0.0448308 \t87\n",
      "Accuracy: 0.044518 \t88\n",
      "Accuracy: 0.0441982 \t89\n",
      "Accuracy: 0.0438879 \t90\n",
      "Accuracy: 0.0435804 \t91\n",
      "Accuracy: 0.0432674 \t92\n",
      "Accuracy: 0.0429265 \t93\n",
      "Accuracy: 0.0425814 \t94\n",
      "Accuracy: 0.042239 \t95\n",
      "Accuracy: 0.0418846 \t96\n",
      "Accuracy: 0.0415159 \t97\n",
      "Accuracy: 0.0411496 \t98\n",
      "Accuracy: 0.0407783 \t99\n",
      "[(0.020426163, 0), (0.057423033, 17)]\n",
      "[5, 4]\n",
      "Accuracy: 0.0479585\n",
      "Accuracy: 0.0230529\n",
      "Accuracy: 0.0192294\n",
      "Final bead error: 0.0192294\n",
      "[True, True, True, True, True, True, True]\n",
      "Accuracy: 0.0189601 \t0\n",
      "Accuracy: 0.0189481 \t1\n",
      "Accuracy: 0.0189362 \t2\n",
      "Accuracy: 0.0189243 \t3\n",
      "Accuracy: 0.0189123 \t4\n",
      "Accuracy: 0.0189004 \t5\n",
      "Accuracy: 0.0188885 \t6\n",
      "Accuracy: 0.0188766 \t7\n",
      "Accuracy: 0.0188648 \t8\n",
      "Accuracy: 0.0188529 \t9\n",
      "Accuracy: 0.0188411 \t10\n",
      "Accuracy: 0.0188293 \t11\n",
      "Accuracy: 0.0188175 \t12\n",
      "Accuracy: 0.0188057 \t13\n",
      "Accuracy: 0.0187939 \t14\n",
      "Accuracy: 0.0187822 \t15\n",
      "Accuracy: 0.0187719 \t16\n",
      "Accuracy: 0.0187618 \t17\n",
      "Accuracy: 0.0187518 \t18\n",
      "Accuracy: 0.0187418 \t19\n",
      "Accuracy: 0.0187318 \t20\n",
      "Accuracy: 0.0187218 \t21\n",
      "Accuracy: 0.0187118 \t22\n",
      "Accuracy: 0.0187018 \t23\n",
      "Accuracy: 0.0186918 \t24\n",
      "Accuracy: 0.0186819 \t25\n",
      "Accuracy: 0.018672 \t26\n",
      "Accuracy: 0.018662 \t27\n",
      "Accuracy: 0.0186521 \t28\n",
      "Accuracy: 0.0186422 \t29\n",
      "Accuracy: 0.0186323 \t30\n",
      "Accuracy: 0.0186224 \t31\n",
      "Accuracy: 0.0186125 \t32\n",
      "Accuracy: 0.0186027 \t33\n",
      "Accuracy: 0.0185928 \t34\n",
      "Accuracy: 0.018583 \t35\n",
      "Accuracy: 0.0185732 \t36\n",
      "Accuracy: 0.0185634 \t37\n",
      "Accuracy: 0.0185535 \t38\n",
      "Accuracy: 0.0185437 \t39\n",
      "Accuracy: 0.018534 \t40\n",
      "Accuracy: 0.0185242 \t41\n",
      "Accuracy: 0.0185144 \t42\n",
      "Accuracy: 0.0185047 \t43\n",
      "Accuracy: 0.0184949 \t44\n",
      "Accuracy: 0.0184852 \t45\n",
      "Accuracy: 0.0184755 \t46\n",
      "Accuracy: 0.0184658 \t47\n",
      "Accuracy: 0.0184561 \t48\n",
      "Accuracy: 0.0184464 \t49\n",
      "Accuracy: 0.0184368 \t50\n",
      "Accuracy: 0.0184271 \t51\n",
      "Accuracy: 0.0184174 \t52\n",
      "Accuracy: 0.0184078 \t53\n",
      "Accuracy: 0.0183982 \t54\n",
      "Accuracy: 0.0183886 \t55\n",
      "Accuracy: 0.018379 \t56\n",
      "Accuracy: 0.0183694 \t57\n",
      "Accuracy: 0.0183598 \t58\n",
      "Accuracy: 0.0183502 \t59\n",
      "Accuracy: 0.0183406 \t60\n",
      "Accuracy: 0.0183311 \t61\n",
      "Accuracy: 0.0183216 \t62\n",
      "Accuracy: 0.018312 \t63\n",
      "Accuracy: 0.0183025 \t64\n",
      "Accuracy: 0.018293 \t65\n",
      "Accuracy: 0.0182835 \t66\n",
      "Accuracy: 0.018274 \t67\n",
      "Accuracy: 0.0182646 \t68\n",
      "Accuracy: 0.0182551 \t69\n",
      "Accuracy: 0.0182456 \t70\n",
      "Accuracy: 0.0182362 \t71\n",
      "Accuracy: 0.0182268 \t72\n",
      "Accuracy: 0.0182174 \t73\n",
      "Accuracy: 0.018208 \t74\n",
      "Accuracy: 0.0181986 \t75\n",
      "Accuracy: 0.0181892 \t76\n",
      "Accuracy: 0.0181799 \t77\n",
      "Accuracy: 0.0181705 \t78\n",
      "Accuracy: 0.0181612 \t79\n",
      "Accuracy: 0.0181519 \t80\n",
      "Accuracy: 0.0181425 \t81\n",
      "Accuracy: 0.0181332 \t82\n",
      "Accuracy: 0.018124 \t83\n",
      "Accuracy: 0.0181147 \t84\n",
      "Accuracy: 0.0181054 \t85\n",
      "Accuracy: 0.0180961 \t86\n",
      "Accuracy: 0.0180869 \t87\n",
      "Accuracy: 0.0180777 \t88\n",
      "Accuracy: 0.0180684 \t89\n",
      "Accuracy: 0.0180592 \t90\n",
      "Accuracy: 0.01805 \t91\n",
      "Accuracy: 0.0180408 \t92\n",
      "Accuracy: 0.0180316 \t93\n",
      "Accuracy: 0.0180224 \t94\n",
      "Accuracy: 0.0180133 \t95\n",
      "Accuracy: 0.0180041 \t96\n",
      "Accuracy: 0.017995 \t97\n",
      "Accuracy: 0.0179858 \t98\n",
      "Accuracy: 0.0179767 \t99\n",
      "Accuracy: 0.01615 \t0\n",
      "Accuracy: 0.0178266 \t1\n",
      "Accuracy: 0.0200974 \t2\n",
      "Accuracy: 0.0226827 \t3\n",
      "Accuracy: 0.0254772 \t4\n",
      "Accuracy: 0.0284133 \t5\n",
      "Accuracy: 0.0314561 \t6\n",
      "Accuracy: 0.0345471 \t7\n",
      "Accuracy: 0.0375681 \t8\n",
      "Accuracy: 0.0404295 \t9\n",
      "Accuracy: 0.0430665 \t10\n",
      "Accuracy: 0.0454956 \t11\n",
      "Accuracy: 0.0477461 \t12\n",
      "Accuracy: 0.0497094 \t13\n",
      "Accuracy: 0.0512788 \t14\n",
      "Accuracy: 0.0524421 \t15\n",
      "Accuracy: 0.053263 \t16\n",
      "Accuracy: 0.0536616 \t17\n",
      "Accuracy: 0.0535358 \t18\n",
      "Accuracy: 0.0532368 \t19\n",
      "Accuracy: 0.0529422 \t20\n",
      "Accuracy: 0.0526524 \t21\n",
      "Accuracy: 0.0523681 \t22\n",
      "Accuracy: 0.0520891 \t23\n",
      "Accuracy: 0.0518156 \t24\n",
      "Accuracy: 0.0515469 \t25\n",
      "Accuracy: 0.0512834 \t26\n",
      "Accuracy: 0.0510269 \t27\n",
      "Accuracy: 0.0507774 \t28\n",
      "Accuracy: 0.0505338 \t29\n",
      "Accuracy: 0.050296 \t30\n",
      "Accuracy: 0.0500629 \t31\n",
      "Accuracy: 0.0498346 \t32\n",
      "Accuracy: 0.0496135 \t33\n",
      "Accuracy: 0.0493981 \t34\n",
      "Accuracy: 0.0491867 \t35\n",
      "Accuracy: 0.0489802 \t36\n",
      "Accuracy: 0.048779 \t37\n",
      "Accuracy: 0.0485832 \t38\n",
      "Accuracy: 0.0483933 \t39\n",
      "Accuracy: 0.0482098 \t40\n",
      "Accuracy: 0.0480323 \t41\n",
      "Accuracy: 0.0478619 \t42\n",
      "Accuracy: 0.0476979 \t43\n",
      "Accuracy: 0.0475399 \t44\n",
      "Accuracy: 0.0473891 \t45\n",
      "Accuracy: 0.0472437 \t46\n",
      "Accuracy: 0.0471043 \t47\n",
      "Accuracy: 0.0469714 \t48\n",
      "Accuracy: 0.0468434 \t49\n",
      "Accuracy: 0.0467204 \t50\n",
      "Accuracy: 0.0466019 \t51\n",
      "Accuracy: 0.0464895 \t52\n",
      "Accuracy: 0.0463852 \t53\n",
      "Accuracy: 0.0462862 \t54\n",
      "Accuracy: 0.0461939 \t55\n",
      "Accuracy: 0.0461092 \t56\n",
      "Accuracy: 0.0460303 \t57\n",
      "Accuracy: 0.0459558 \t58\n",
      "Accuracy: 0.0458829 \t59\n",
      "Accuracy: 0.0458117 \t60\n",
      "Accuracy: 0.0457421 \t61\n",
      "Accuracy: 0.0456741 \t62\n",
      "Accuracy: 0.0456079 \t63\n",
      "Accuracy: 0.0455439 \t64\n",
      "Accuracy: 0.0454775 \t65\n",
      "Accuracy: 0.0453762 \t66\n",
      "Accuracy: 0.0452523 \t67\n",
      "Accuracy: 0.0451279 \t68\n",
      "Accuracy: 0.0449811 \t69\n",
      "Accuracy: 0.0448031 \t70\n",
      "Accuracy: 0.0446064 \t71\n",
      "Accuracy: 0.0444051 \t72\n",
      "Accuracy: 0.0442105 \t73\n",
      "Accuracy: 0.0440066 \t74\n",
      "Accuracy: 0.043788 \t75\n",
      "Accuracy: 0.0435575 \t76\n",
      "Accuracy: 0.0433105 \t77\n",
      "Accuracy: 0.0430393 \t78\n",
      "Accuracy: 0.0427574 \t79\n",
      "Accuracy: 0.0424637 \t80\n",
      "Accuracy: 0.0421665 \t81\n",
      "Accuracy: 0.0418635 \t82\n",
      "Accuracy: 0.0415567 \t83\n",
      "Accuracy: 0.0412515 \t84\n",
      "Accuracy: 0.0409553 \t85\n",
      "Accuracy: 0.0406627 \t86\n",
      "Accuracy: 0.0403794 \t87\n",
      "Accuracy: 0.0401019 \t88\n",
      "Accuracy: 0.0398233 \t89\n",
      "Accuracy: 0.0395279 \t90\n",
      "Accuracy: 0.039236 \t91\n",
      "Accuracy: 0.0389528 \t92\n",
      "Accuracy: 0.0386699 \t93\n",
      "Accuracy: 0.0383831 \t94\n",
      "Accuracy: 0.0380893 \t95\n",
      "Accuracy: 0.0377936 \t96\n",
      "Accuracy: 0.0374934 \t97\n",
      "Accuracy: 0.037199 \t98\n",
      "Accuracy: 0.0369015 \t99\n",
      "[(0.018960103, 0), (0.053661644, 17)]\n",
      "[6, 5]\n",
      "Accuracy: 0.0442602\n",
      "Accuracy: 0.0178528\n",
      "Final bead error: 0.0178528\n",
      "[True, True, True, True, True, True, True, True]\n",
      "Accuracy: 0.0189386 \t0\n",
      "Accuracy: 0.0189393 \t1\n",
      "Accuracy: 0.01894 \t2\n",
      "Accuracy: 0.0189407 \t3\n",
      "Accuracy: 0.0189414 \t4\n",
      "Accuracy: 0.0189421 \t5\n",
      "Accuracy: 0.0189428 \t6\n",
      "Accuracy: 0.0189435 \t7\n",
      "Accuracy: 0.0189442 \t8\n",
      "Accuracy: 0.0189449 \t9\n",
      "Accuracy: 0.0189456 \t10\n",
      "Accuracy: 0.0189463 \t11\n",
      "Accuracy: 0.018947 \t12\n",
      "Accuracy: 0.0189477 \t13\n",
      "Accuracy: 0.0189484 \t14\n",
      "Accuracy: 0.0189492 \t15\n",
      "Accuracy: 0.0189499 \t16\n",
      "Accuracy: 0.0189508 \t17\n",
      "Accuracy: 0.0189516 \t18\n",
      "Accuracy: 0.0189524 \t19\n",
      "Accuracy: 0.0189532 \t20\n",
      "Accuracy: 0.0189541 \t21\n",
      "Accuracy: 0.0189549 \t22\n",
      "Accuracy: 0.0189557 \t23\n",
      "Accuracy: 0.0189565 \t24\n",
      "Accuracy: 0.0189573 \t25\n",
      "Accuracy: 0.0189582 \t26\n",
      "Accuracy: 0.0189591 \t27\n",
      "Accuracy: 0.01896 \t28\n",
      "Accuracy: 0.018961 \t29\n",
      "Accuracy: 0.0189619 \t30\n",
      "Accuracy: 0.0189628 \t31\n",
      "Accuracy: 0.0189638 \t32\n",
      "Accuracy: 0.0189647 \t33\n",
      "Accuracy: 0.0189657 \t34\n",
      "Accuracy: 0.0189666 \t35\n",
      "Accuracy: 0.0189676 \t36\n",
      "Accuracy: 0.0189685 \t37\n",
      "Accuracy: 0.0189695 \t38\n",
      "Accuracy: 0.0189704 \t39\n",
      "Accuracy: 0.0189713 \t40\n",
      "Accuracy: 0.0189723 \t41\n",
      "Accuracy: 0.0189732 \t42\n",
      "Accuracy: 0.0189742 \t43\n",
      "Accuracy: 0.0189751 \t44\n",
      "Accuracy: 0.0189761 \t45\n",
      "Accuracy: 0.018977 \t46\n",
      "Accuracy: 0.018978 \t47\n",
      "Accuracy: 0.0189789 \t48\n",
      "Accuracy: 0.0189799 \t49\n",
      "Accuracy: 0.0189809 \t50\n",
      "Accuracy: 0.0189819 \t51\n",
      "Accuracy: 0.0189829 \t52\n",
      "Accuracy: 0.0189839 \t53\n",
      "Accuracy: 0.0189848 \t54\n",
      "Accuracy: 0.0189858 \t55\n",
      "Accuracy: 0.0189869 \t56\n",
      "Accuracy: 0.0189879 \t57\n",
      "Accuracy: 0.0189889 \t58\n",
      "Accuracy: 0.0189899 \t59\n",
      "Accuracy: 0.018991 \t60\n",
      "Accuracy: 0.018992 \t61\n",
      "Accuracy: 0.0189931 \t62\n",
      "Accuracy: 0.0189942 \t63\n",
      "Accuracy: 0.0189952 \t64\n",
      "Accuracy: 0.0189963 \t65\n",
      "Accuracy: 0.0189973 \t66\n",
      "Accuracy: 0.0189985 \t67\n",
      "Accuracy: 0.0189996 \t68\n",
      "Accuracy: 0.0190007 \t69\n",
      "Accuracy: 0.0190019 \t70\n",
      "Accuracy: 0.019003 \t71\n",
      "Accuracy: 0.0190041 \t72\n",
      "Accuracy: 0.0190053 \t73\n",
      "Accuracy: 0.0190064 \t74\n",
      "Accuracy: 0.0190076 \t75\n",
      "Accuracy: 0.0190088 \t76\n",
      "Accuracy: 0.0190099 \t77\n",
      "Accuracy: 0.0190111 \t78\n",
      "Accuracy: 0.0190124 \t79\n",
      "Accuracy: 0.0190136 \t80\n",
      "Accuracy: 0.0190148 \t81\n",
      "Accuracy: 0.019016 \t82\n",
      "Accuracy: 0.0190172 \t83\n",
      "Accuracy: 0.0190185 \t84\n",
      "Accuracy: 0.0190197 \t85\n",
      "Accuracy: 0.0190209 \t86\n",
      "Accuracy: 0.0190221 \t87\n",
      "Accuracy: 0.0190234 \t88\n",
      "Accuracy: 0.0190246 \t89\n",
      "Accuracy: 0.0190259 \t90\n",
      "Accuracy: 0.0190272 \t91\n",
      "Accuracy: 0.0190285 \t92\n",
      "Accuracy: 0.0190298 \t93\n",
      "Accuracy: 0.0190311 \t94\n",
      "Accuracy: 0.0190325 \t95\n",
      "Accuracy: 0.0190338 \t96\n",
      "Accuracy: 0.0190352 \t97\n",
      "Accuracy: 0.0190365 \t98\n",
      "Accuracy: 0.0190378 \t99\n",
      "Accuracy: 0.018337 \t0\n",
      "Accuracy: 0.0201817 \t1\n",
      "Accuracy: 0.0225423 \t2\n",
      "Accuracy: 0.025204 \t3\n",
      "Accuracy: 0.0280912 \t4\n",
      "Accuracy: 0.031182 \t5\n",
      "Accuracy: 0.0343541 \t6\n",
      "Accuracy: 0.0375217 \t7\n",
      "Accuracy: 0.0406048 \t8\n",
      "Accuracy: 0.0435379 \t9\n",
      "Accuracy: 0.0462567 \t10\n",
      "Accuracy: 0.0487346 \t11\n",
      "Accuracy: 0.0510204 \t12\n",
      "Accuracy: 0.0529908 \t13\n",
      "Accuracy: 0.0546474 \t14\n",
      "Accuracy: 0.0559264 \t15\n",
      "Accuracy: 0.0566869 \t16\n",
      "Accuracy: 0.0568809 \t17\n",
      "Accuracy: 0.0566665 \t18\n",
      "Accuracy: 0.0563964 \t19\n",
      "Accuracy: 0.0561299 \t20\n",
      "Accuracy: 0.0558684 \t21\n",
      "Accuracy: 0.0556109 \t22\n",
      "Accuracy: 0.0553573 \t23\n",
      "Accuracy: 0.0551089 \t24\n",
      "Accuracy: 0.0548657 \t25\n",
      "Accuracy: 0.0546267 \t26\n",
      "Accuracy: 0.0543919 \t27\n",
      "Accuracy: 0.0541632 \t28\n",
      "Accuracy: 0.0539405 \t29\n",
      "Accuracy: 0.053723 \t30\n",
      "Accuracy: 0.0535098 \t31\n",
      "Accuracy: 0.0533013 \t32\n",
      "Accuracy: 0.0530992 \t33\n",
      "Accuracy: 0.0529021 \t34\n",
      "Accuracy: 0.0527091 \t35\n",
      "Accuracy: 0.0525215 \t36\n",
      "Accuracy: 0.0523376 \t37\n",
      "Accuracy: 0.0521577 \t38\n",
      "Accuracy: 0.0519846 \t39\n",
      "Accuracy: 0.0518175 \t40\n",
      "Accuracy: 0.0516546 \t41\n",
      "Accuracy: 0.0514968 \t42\n",
      "Accuracy: 0.0513446 \t43\n",
      "Accuracy: 0.0511973 \t44\n",
      "Accuracy: 0.0510556 \t45\n",
      "Accuracy: 0.0509189 \t46\n",
      "Accuracy: 0.0507884 \t47\n",
      "Accuracy: 0.0506639 \t48\n",
      "Accuracy: 0.0505451 \t49\n",
      "Accuracy: 0.0504344 \t50\n",
      "Accuracy: 0.0503303 \t51\n",
      "Accuracy: 0.0502318 \t52\n",
      "Accuracy: 0.0501406 \t53\n",
      "Accuracy: 0.0500559 \t54\n",
      "Accuracy: 0.0499775 \t55\n",
      "Accuracy: 0.0499039 \t56\n",
      "Accuracy: 0.0498295 \t57\n",
      "Accuracy: 0.0497378 \t58\n",
      "Accuracy: 0.0496343 \t59\n",
      "Accuracy: 0.0495157 \t60\n",
      "Accuracy: 0.049377 \t61\n",
      "Accuracy: 0.049216 \t62\n",
      "Accuracy: 0.0490435 \t63\n",
      "Accuracy: 0.0488743 \t64\n",
      "Accuracy: 0.0487045 \t65\n",
      "Accuracy: 0.0485366 \t66\n",
      "Accuracy: 0.0483662 \t67\n",
      "Accuracy: 0.0481912 \t68\n",
      "Accuracy: 0.0480109 \t69\n",
      "Accuracy: 0.0478178 \t70\n",
      "Accuracy: 0.0476075 \t71\n",
      "Accuracy: 0.0473925 \t72\n",
      "Accuracy: 0.047169 \t73\n",
      "Accuracy: 0.0469362 \t74\n",
      "Accuracy: 0.0466929 \t75\n",
      "Accuracy: 0.0464417 \t76\n",
      "Accuracy: 0.0461888 \t77\n",
      "Accuracy: 0.0459278 \t78\n",
      "Accuracy: 0.0456597 \t79\n",
      "Accuracy: 0.0453717 \t80\n",
      "Accuracy: 0.0450803 \t81\n",
      "Accuracy: 0.0447891 \t82\n",
      "Accuracy: 0.0444986 \t83\n",
      "Accuracy: 0.0442121 \t84\n",
      "Accuracy: 0.0439309 \t85\n",
      "Accuracy: 0.0436537 \t86\n",
      "Accuracy: 0.0433825 \t87\n",
      "Accuracy: 0.0431158 \t88\n",
      "Accuracy: 0.0428401 \t89\n",
      "Accuracy: 0.042573 \t90\n",
      "Accuracy: 0.0423012 \t91\n",
      "Accuracy: 0.042028 \t92\n",
      "Accuracy: 0.0417601 \t93\n",
      "Accuracy: 0.0414831 \t94\n",
      "Accuracy: 0.0412057 \t95\n",
      "Accuracy: 0.0409206 \t96\n",
      "Accuracy: 0.0406301 \t97\n",
      "Accuracy: 0.040345 \t98\n",
      "Accuracy: 0.0400644 \t99\n",
      "[(0.01903783, 99), (0.05688085, 17)]\n",
      "[7, 6]\n",
      "Accuracy: 0.0510139\n",
      "Accuracy: 0.0274898\n",
      "Accuracy: 0.0205312\n",
      "Final bead error: 0.0205312\n",
      "[True, True, True, True, True, True, True, True, True]\n",
      "Accuracy: 0.017036 \t0\n",
      "Accuracy: 0.0170355 \t1\n",
      "Accuracy: 0.017035 \t2\n",
      "Accuracy: 0.0170345 \t3\n",
      "Accuracy: 0.0170339 \t4\n",
      "Accuracy: 0.0170334 \t5\n",
      "Accuracy: 0.0170328 \t6\n",
      "Accuracy: 0.0170323 \t7\n",
      "Accuracy: 0.0170318 \t8\n",
      "Accuracy: 0.0170312 \t9\n",
      "Accuracy: 0.0170307 \t10\n",
      "Accuracy: 0.0170301 \t11\n",
      "Accuracy: 0.0170296 \t12\n",
      "Accuracy: 0.017029 \t13\n",
      "Accuracy: 0.0170285 \t14\n",
      "Accuracy: 0.0170279 \t15\n",
      "Accuracy: 0.0170273 \t16\n",
      "Accuracy: 0.0170268 \t17\n",
      "Accuracy: 0.0170263 \t18\n",
      "Accuracy: 0.0170258 \t19\n",
      "Accuracy: 0.0170252 \t20\n",
      "Accuracy: 0.0170248 \t21\n",
      "Accuracy: 0.0170243 \t22\n",
      "Accuracy: 0.0170238 \t23\n",
      "Accuracy: 0.0170234 \t24\n",
      "Accuracy: 0.0170229 \t25\n",
      "Accuracy: 0.0170224 \t26\n",
      "Accuracy: 0.017022 \t27\n",
      "Accuracy: 0.0170215 \t28\n",
      "Accuracy: 0.0170211 \t29\n",
      "Accuracy: 0.0170207 \t30\n",
      "Accuracy: 0.0170202 \t31\n",
      "Accuracy: 0.0170198 \t32\n",
      "Accuracy: 0.0170194 \t33\n",
      "Accuracy: 0.017019 \t34\n",
      "Accuracy: 0.0170186 \t35\n",
      "Accuracy: 0.0170181 \t36\n",
      "Accuracy: 0.0170177 \t37\n",
      "Accuracy: 0.0170173 \t38\n",
      "Accuracy: 0.0170168 \t39\n",
      "Accuracy: 0.0170164 \t40\n",
      "Accuracy: 0.0170159 \t41\n",
      "Accuracy: 0.0170155 \t42\n",
      "Accuracy: 0.017015 \t43\n",
      "Accuracy: 0.0170146 \t44\n",
      "Accuracy: 0.0170141 \t45\n",
      "Accuracy: 0.0170137 \t46\n",
      "Accuracy: 0.0170133 \t47\n",
      "Accuracy: 0.0170129 \t48\n",
      "Accuracy: 0.0170124 \t49\n",
      "Accuracy: 0.017012 \t50\n",
      "Accuracy: 0.0170116 \t51\n",
      "Accuracy: 0.0170111 \t52\n",
      "Accuracy: 0.0170107 \t53\n",
      "Accuracy: 0.0170103 \t54\n",
      "Accuracy: 0.0170098 \t55\n",
      "Accuracy: 0.0170094 \t56\n",
      "Accuracy: 0.0170089 \t57\n",
      "Accuracy: 0.0170085 \t58\n",
      "Accuracy: 0.017008 \t59\n",
      "Accuracy: 0.0170076 \t60\n",
      "Accuracy: 0.0170072 \t61\n",
      "Accuracy: 0.0170067 \t62\n",
      "Accuracy: 0.0170063 \t63\n",
      "Accuracy: 0.0170059 \t64\n",
      "Accuracy: 0.0170054 \t65\n",
      "Accuracy: 0.017005 \t66\n",
      "Accuracy: 0.0170045 \t67\n",
      "Accuracy: 0.0170041 \t68\n",
      "Accuracy: 0.0170036 \t69\n",
      "Accuracy: 0.0170032 \t70\n",
      "Accuracy: 0.0170027 \t71\n",
      "Accuracy: 0.0170023 \t72\n",
      "Accuracy: 0.0170018 \t73\n",
      "Accuracy: 0.0170014 \t74\n",
      "Accuracy: 0.0170009 \t75\n",
      "Accuracy: 0.0170004 \t76\n",
      "Accuracy: 0.017 \t77\n",
      "Accuracy: 0.0169995 \t78\n",
      "Accuracy: 0.016999 \t79\n",
      "Accuracy: 0.0169985 \t80\n",
      "Accuracy: 0.016998 \t81\n",
      "Accuracy: 0.0169975 \t82\n",
      "Accuracy: 0.0169971 \t83\n",
      "Accuracy: 0.0169966 \t84\n",
      "Accuracy: 0.0169961 \t85\n",
      "Accuracy: 0.0169956 \t86\n",
      "Accuracy: 0.016995 \t87\n",
      "Accuracy: 0.0169945 \t88\n",
      "Accuracy: 0.016994 \t89\n",
      "Accuracy: 0.0169935 \t90\n",
      "Accuracy: 0.016993 \t91\n",
      "Accuracy: 0.0169925 \t92\n",
      "Accuracy: 0.016992 \t93\n",
      "Accuracy: 0.0169914 \t94\n",
      "Accuracy: 0.0169909 \t95\n",
      "Accuracy: 0.0169904 \t96\n",
      "Accuracy: 0.0169899 \t97\n",
      "Accuracy: 0.0169894 \t98\n",
      "Accuracy: 0.0169889 \t99\n",
      "Accuracy: 0.0181507 \t0\n",
      "Accuracy: 0.0200463 \t1\n",
      "Accuracy: 0.0223601 \t2\n",
      "Accuracy: 0.024976 \t3\n",
      "Accuracy: 0.0278396 \t4\n",
      "Accuracy: 0.0308597 \t5\n",
      "Accuracy: 0.0339306 \t6\n",
      "Accuracy: 0.0369773 \t7\n",
      "Accuracy: 0.0399707 \t8\n",
      "Accuracy: 0.0429011 \t9\n",
      "Accuracy: 0.0457309 \t10\n",
      "Accuracy: 0.0484259 \t11\n",
      "Accuracy: 0.0508219 \t12\n",
      "Accuracy: 0.0528088 \t13\n",
      "Accuracy: 0.0543954 \t14\n",
      "Accuracy: 0.0555964 \t15\n",
      "Accuracy: 0.0563307 \t16\n",
      "Accuracy: 0.0565186 \t17\n",
      "Accuracy: 0.0562902 \t18\n",
      "Accuracy: 0.0560475 \t19\n",
      "Accuracy: 0.0558078 \t20\n",
      "Accuracy: 0.0555719 \t21\n",
      "Accuracy: 0.0553398 \t22\n",
      "Accuracy: 0.0551109 \t23\n",
      "Accuracy: 0.0548858 \t24\n",
      "Accuracy: 0.054664 \t25\n",
      "Accuracy: 0.0544459 \t26\n",
      "Accuracy: 0.0542317 \t27\n",
      "Accuracy: 0.0540206 \t28\n",
      "Accuracy: 0.0538126 \t29\n",
      "Accuracy: 0.0536088 \t30\n",
      "Accuracy: 0.0534085 \t31\n",
      "Accuracy: 0.0532118 \t32\n",
      "Accuracy: 0.0530183 \t33\n",
      "Accuracy: 0.052828 \t34\n",
      "Accuracy: 0.0526424 \t35\n",
      "Accuracy: 0.0524617 \t36\n",
      "Accuracy: 0.0522862 \t37\n",
      "Accuracy: 0.052115 \t38\n",
      "Accuracy: 0.0519491 \t39\n",
      "Accuracy: 0.0517886 \t40\n",
      "Accuracy: 0.051634 \t41\n",
      "Accuracy: 0.0514864 \t42\n",
      "Accuracy: 0.0513431 \t43\n",
      "Accuracy: 0.0512047 \t44\n",
      "Accuracy: 0.0510719 \t45\n",
      "Accuracy: 0.0509439 \t46\n",
      "Accuracy: 0.0508152 \t47\n",
      "Accuracy: 0.0506838 \t48\n",
      "Accuracy: 0.0505365 \t49\n",
      "Accuracy: 0.0503844 \t50\n",
      "Accuracy: 0.0502398 \t51\n",
      "Accuracy: 0.0500835 \t52\n",
      "Accuracy: 0.0499113 \t53\n",
      "Accuracy: 0.0497385 \t54\n",
      "Accuracy: 0.0495642 \t55\n",
      "Accuracy: 0.0493997 \t56\n",
      "Accuracy: 0.0492308 \t57\n",
      "Accuracy: 0.0490593 \t58\n",
      "Accuracy: 0.048881 \t59\n",
      "Accuracy: 0.0486841 \t60\n",
      "Accuracy: 0.0484749 \t61\n",
      "Accuracy: 0.0482626 \t62\n",
      "Accuracy: 0.04805 \t63\n",
      "Accuracy: 0.0478416 \t64\n",
      "Accuracy: 0.047631 \t65\n",
      "Accuracy: 0.0474125 \t66\n",
      "Accuracy: 0.0471871 \t67\n",
      "Accuracy: 0.0469599 \t68\n",
      "Accuracy: 0.0467368 \t69\n",
      "Accuracy: 0.0465145 \t70\n",
      "Accuracy: 0.0462771 \t71\n",
      "Accuracy: 0.0460396 \t72\n",
      "Accuracy: 0.0458039 \t73\n",
      "Accuracy: 0.0455677 \t74\n",
      "Accuracy: 0.0453256 \t75\n",
      "Accuracy: 0.0450873 \t76\n",
      "Accuracy: 0.0448477 \t77\n",
      "Accuracy: 0.0446057 \t78\n",
      "Accuracy: 0.0443649 \t79\n",
      "Accuracy: 0.0441266 \t80\n",
      "Accuracy: 0.0438921 \t81\n",
      "Accuracy: 0.0436552 \t82\n",
      "Accuracy: 0.0434172 \t83\n",
      "Accuracy: 0.0431749 \t84\n",
      "Accuracy: 0.0429262 \t85\n",
      "Accuracy: 0.0426798 \t86\n",
      "Accuracy: 0.0424364 \t87\n",
      "Accuracy: 0.0421897 \t88\n",
      "Accuracy: 0.0419356 \t89\n",
      "Accuracy: 0.0416823 \t90\n",
      "Accuracy: 0.0414265 \t91\n",
      "Accuracy: 0.0411678 \t92\n",
      "Accuracy: 0.0409098 \t93\n",
      "Accuracy: 0.0406549 \t94\n",
      "Accuracy: 0.0404005 \t95\n",
      "Accuracy: 0.0401488 \t96\n",
      "Accuracy: 0.0398975 \t97\n",
      "Accuracy: 0.0396468 \t98\n",
      "Accuracy: 0.0393979 \t99\n",
      "[(0.017036043, 0), (0.056518648, 17)]\n",
      "[8, 7]\n",
      "Accuracy: 0.0466223\n",
      "Accuracy: 0.0267693\n",
      "Accuracy: 0.018095\n",
      "Final bead error: 0.018095\n",
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "Accuracy: 0.0192793 \t0\n",
      "Accuracy: 0.0192828 \t1\n",
      "Accuracy: 0.0192864 \t2\n",
      "Accuracy: 0.0192901 \t3\n",
      "Accuracy: 0.0192937 \t4\n",
      "Accuracy: 0.0192973 \t5\n",
      "Accuracy: 0.019301 \t6\n",
      "Accuracy: 0.0193047 \t7\n",
      "Accuracy: 0.0193084 \t8\n",
      "Accuracy: 0.0193121 \t9\n",
      "Accuracy: 0.0193159 \t10\n",
      "Accuracy: 0.0193197 \t11\n",
      "Accuracy: 0.0193234 \t12\n",
      "Accuracy: 0.0193272 \t13\n",
      "Accuracy: 0.0193309 \t14\n",
      "Accuracy: 0.0193349 \t15\n",
      "Accuracy: 0.0193389 \t16\n",
      "Accuracy: 0.0193431 \t17\n",
      "Accuracy: 0.0193473 \t18\n",
      "Accuracy: 0.0193516 \t19\n",
      "Accuracy: 0.0193559 \t20\n",
      "Accuracy: 0.0193602 \t21\n",
      "Accuracy: 0.0193645 \t22\n",
      "Accuracy: 0.0193687 \t23\n",
      "Accuracy: 0.019373 \t24\n",
      "Accuracy: 0.0193772 \t25\n",
      "Accuracy: 0.0193815 \t26\n",
      "Accuracy: 0.0193857 \t27\n",
      "Accuracy: 0.0193899 \t28\n",
      "Accuracy: 0.0193942 \t29\n",
      "Accuracy: 0.0193984 \t30\n",
      "Accuracy: 0.0194027 \t31\n",
      "Accuracy: 0.019407 \t32\n",
      "Accuracy: 0.0194113 \t33\n",
      "Accuracy: 0.0194157 \t34\n",
      "Accuracy: 0.0194201 \t35\n",
      "Accuracy: 0.0194244 \t36\n",
      "Accuracy: 0.0194288 \t37\n",
      "Accuracy: 0.0194331 \t38\n",
      "Accuracy: 0.0194376 \t39\n",
      "Accuracy: 0.0194421 \t40\n",
      "Accuracy: 0.0194466 \t41\n",
      "Accuracy: 0.0194511 \t42\n",
      "Accuracy: 0.0194557 \t43\n",
      "Accuracy: 0.0194602 \t44\n",
      "Accuracy: 0.0194648 \t45\n",
      "Accuracy: 0.0194694 \t46\n",
      "Accuracy: 0.0194739 \t47\n",
      "Accuracy: 0.0194785 \t48\n",
      "Accuracy: 0.0194831 \t49\n",
      "Accuracy: 0.0194879 \t50\n",
      "Accuracy: 0.0194926 \t51\n",
      "Accuracy: 0.0194973 \t52\n",
      "Accuracy: 0.019502 \t53\n",
      "Accuracy: 0.0195068 \t54\n",
      "Accuracy: 0.0195116 \t55\n",
      "Accuracy: 0.0195165 \t56\n",
      "Accuracy: 0.0195214 \t57\n",
      "Accuracy: 0.0195263 \t58\n",
      "Accuracy: 0.0195312 \t59\n",
      "Accuracy: 0.0195362 \t60\n",
      "Accuracy: 0.0195411 \t61\n",
      "Accuracy: 0.019546 \t62\n",
      "Accuracy: 0.0195509 \t63\n",
      "Accuracy: 0.0195558 \t64\n",
      "Accuracy: 0.0195607 \t65\n",
      "Accuracy: 0.0195656 \t66\n",
      "Accuracy: 0.0195705 \t67\n",
      "Accuracy: 0.0195754 \t68\n",
      "Accuracy: 0.0195802 \t69\n",
      "Accuracy: 0.0195851 \t70\n",
      "Accuracy: 0.0195901 \t71\n",
      "Accuracy: 0.019595 \t72\n",
      "Accuracy: 0.0196 \t73\n",
      "Accuracy: 0.0196051 \t74\n",
      "Accuracy: 0.0196101 \t75\n",
      "Accuracy: 0.0196151 \t76\n",
      "Accuracy: 0.0196201 \t77\n",
      "Accuracy: 0.0196251 \t78\n",
      "Accuracy: 0.0196301 \t79\n",
      "Accuracy: 0.0196351 \t80\n",
      "Accuracy: 0.0196401 \t81\n",
      "Accuracy: 0.0196451 \t82\n",
      "Accuracy: 0.0196501 \t83\n",
      "Accuracy: 0.019655 \t84\n",
      "Accuracy: 0.01966 \t85\n",
      "Accuracy: 0.019665 \t86\n",
      "Accuracy: 0.0196699 \t87\n",
      "Accuracy: 0.0196749 \t88\n",
      "Accuracy: 0.0196799 \t89\n",
      "Accuracy: 0.0196848 \t90\n",
      "Accuracy: 0.0196898 \t91\n",
      "Accuracy: 0.0196947 \t92\n",
      "Accuracy: 0.0196997 \t93\n",
      "Accuracy: 0.0197046 \t94\n",
      "Accuracy: 0.0197096 \t95\n",
      "Accuracy: 0.0197145 \t96\n",
      "Accuracy: 0.0197194 \t97\n",
      "Accuracy: 0.0197244 \t98\n",
      "Accuracy: 0.0197293 \t99\n",
      "Accuracy: 0.0185368 \t0\n",
      "Accuracy: 0.0205025 \t1\n",
      "Accuracy: 0.0228227 \t2\n",
      "Accuracy: 0.0254294 \t3\n",
      "Accuracy: 0.0282424 \t4\n",
      "Accuracy: 0.0311954 \t5\n",
      "Accuracy: 0.0342424 \t6\n",
      "Accuracy: 0.0372955 \t7\n",
      "Accuracy: 0.0402457 \t8\n",
      "Accuracy: 0.0430966 \t9\n",
      "Accuracy: 0.0457634 \t10\n",
      "Accuracy: 0.048194 \t11\n",
      "Accuracy: 0.0502281 \t12\n",
      "Accuracy: 0.0519726 \t13\n",
      "Accuracy: 0.0533482 \t14\n",
      "Accuracy: 0.0543254 \t15\n",
      "Accuracy: 0.054895 \t16\n",
      "Accuracy: 0.0549538 \t17\n",
      "Accuracy: 0.0547424 \t18\n",
      "Accuracy: 0.0545324 \t19\n",
      "Accuracy: 0.054324 \t20\n",
      "Accuracy: 0.0541182 \t21\n",
      "Accuracy: 0.0539147 \t22\n",
      "Accuracy: 0.0537131 \t23\n",
      "Accuracy: 0.0535144 \t24\n",
      "Accuracy: 0.053319 \t25\n",
      "Accuracy: 0.0531267 \t26\n",
      "Accuracy: 0.0529378 \t27\n",
      "Accuracy: 0.052752 \t28\n",
      "Accuracy: 0.0525689 \t29\n",
      "Accuracy: 0.052389 \t30\n",
      "Accuracy: 0.0522119 \t31\n",
      "Accuracy: 0.0520385 \t32\n",
      "Accuracy: 0.0518648 \t33\n",
      "Accuracy: 0.05168 \t34\n",
      "Accuracy: 0.0514956 \t35\n",
      "Accuracy: 0.0513104 \t36\n",
      "Accuracy: 0.0511314 \t37\n",
      "Accuracy: 0.0509535 \t38\n",
      "Accuracy: 0.0507791 \t39\n",
      "Accuracy: 0.0506047 \t40\n",
      "Accuracy: 0.050425 \t41\n",
      "Accuracy: 0.050243 \t42\n",
      "Accuracy: 0.0500581 \t43\n",
      "Accuracy: 0.049866 \t44\n",
      "Accuracy: 0.0496785 \t45\n",
      "Accuracy: 0.049494 \t46\n",
      "Accuracy: 0.0493079 \t47\n",
      "Accuracy: 0.0491216 \t48\n",
      "Accuracy: 0.0489355 \t49\n",
      "Accuracy: 0.0487529 \t50\n",
      "Accuracy: 0.048576 \t51\n",
      "Accuracy: 0.0484006 \t52\n",
      "Accuracy: 0.0482275 \t53\n",
      "Accuracy: 0.048053 \t54\n",
      "Accuracy: 0.0478754 \t55\n",
      "Accuracy: 0.0476985 \t56\n",
      "Accuracy: 0.0475176 \t57\n",
      "Accuracy: 0.0473315 \t58\n",
      "Accuracy: 0.0471472 \t59\n",
      "Accuracy: 0.046967 \t60\n",
      "Accuracy: 0.0467906 \t61\n",
      "Accuracy: 0.0466061 \t62\n",
      "Accuracy: 0.0464205 \t63\n",
      "Accuracy: 0.0462325 \t64\n",
      "Accuracy: 0.0460455 \t65\n",
      "Accuracy: 0.0458613 \t66\n",
      "Accuracy: 0.0456754 \t67\n",
      "Accuracy: 0.0454872 \t68\n",
      "Accuracy: 0.0452986 \t69\n",
      "Accuracy: 0.0451107 \t70\n",
      "Accuracy: 0.0449157 \t71\n",
      "Accuracy: 0.0447153 \t72\n",
      "Accuracy: 0.0445129 \t73\n",
      "Accuracy: 0.0443094 \t74\n",
      "Accuracy: 0.0441035 \t75\n",
      "Accuracy: 0.0438942 \t76\n",
      "Accuracy: 0.0436859 \t77\n",
      "Accuracy: 0.0434666 \t78\n",
      "Accuracy: 0.0432398 \t79\n",
      "Accuracy: 0.0430129 \t80\n",
      "Accuracy: 0.0427854 \t81\n",
      "Accuracy: 0.0425572 \t82\n",
      "Accuracy: 0.0423295 \t83\n",
      "Accuracy: 0.0421039 \t84\n",
      "Accuracy: 0.0418792 \t85\n",
      "Accuracy: 0.0416564 \t86\n",
      "Accuracy: 0.0414365 \t87\n",
      "Accuracy: 0.0412161 \t88\n",
      "Accuracy: 0.040989 \t89\n",
      "Accuracy: 0.0407626 \t90\n",
      "Accuracy: 0.0405405 \t91\n",
      "Accuracy: 0.0403201 \t92\n",
      "Accuracy: 0.0400972 \t93\n",
      "Accuracy: 0.0398765 \t94\n",
      "Accuracy: 0.0396598 \t95\n",
      "Accuracy: 0.0394462 \t96\n",
      "Accuracy: 0.0392359 \t97\n",
      "Accuracy: 0.0390272 \t98\n",
      "Accuracy: 0.0388211 \t99\n",
      "[(0.019729288, 99), (0.054953828, 17)]\n",
      "[9, 8]\n",
      "Accuracy: 0.0497233\n",
      "Accuracy: 0.0495117\n",
      "Accuracy: 0.0492999\n",
      "Accuracy: 0.0491201\n",
      "Accuracy: 0.0489578\n",
      "Accuracy: 0.0487608\n",
      "Accuracy: 0.0485895\n",
      "Accuracy: 0.0484178\n",
      "Accuracy: 0.048205\n",
      "Accuracy: 0.0479871\n",
      "Accuracy: 0.0476775\n",
      "Accuracy: 0.0443743\n",
      "Accuracy: 0.0208035\n",
      "Accuracy: 0.0192714\n",
      "Final bead error: 0.0192714\n",
      "[True, True, True, True, True, True, True, True, True, True, True]\n",
      "Accuracy: 0.018964 \t0\n",
      "Accuracy: 0.0189672 \t1\n",
      "Accuracy: 0.0189702 \t2\n",
      "Accuracy: 0.0189732 \t3\n",
      "Accuracy: 0.0189761 \t4\n",
      "Accuracy: 0.0189789 \t5\n",
      "Accuracy: 0.0189816 \t6\n",
      "Accuracy: 0.0189842 \t7\n",
      "Accuracy: 0.0189868 \t8\n",
      "Accuracy: 0.0189893 \t9\n",
      "Accuracy: 0.0189917 \t10\n",
      "Accuracy: 0.018994 \t11\n",
      "Accuracy: 0.0189962 \t12\n",
      "Accuracy: 0.0189984 \t13\n",
      "Accuracy: 0.0190004 \t14\n",
      "Accuracy: 0.0190024 \t15\n",
      "Accuracy: 0.0190043 \t16\n",
      "Accuracy: 0.0190062 \t17\n",
      "Accuracy: 0.0190079 \t18\n",
      "Accuracy: 0.0190096 \t19\n",
      "Accuracy: 0.0190111 \t20\n",
      "Accuracy: 0.0190126 \t21\n",
      "Accuracy: 0.019014 \t22\n",
      "Accuracy: 0.0190153 \t23\n",
      "Accuracy: 0.0190166 \t24\n",
      "Accuracy: 0.0190177 \t25\n",
      "Accuracy: 0.0190188 \t26\n",
      "Accuracy: 0.0190197 \t27\n",
      "Accuracy: 0.0190206 \t28\n",
      "Accuracy: 0.0190214 \t29\n",
      "Accuracy: 0.0190222 \t30\n",
      "Accuracy: 0.0190228 \t31\n",
      "Accuracy: 0.0190234 \t32\n",
      "Accuracy: 0.0190238 \t33\n",
      "Accuracy: 0.0190242 \t34\n",
      "Accuracy: 0.0190246 \t35\n",
      "Accuracy: 0.0190248 \t36\n",
      "Accuracy: 0.019025 \t37\n",
      "Accuracy: 0.019025 \t38\n",
      "Accuracy: 0.0190251 \t39\n",
      "Accuracy: 0.019025 \t40\n",
      "Accuracy: 0.0190248 \t41\n",
      "Accuracy: 0.0190246 \t42\n",
      "Accuracy: 0.0190242 \t43\n",
      "Accuracy: 0.0190238 \t44\n",
      "Accuracy: 0.0190232 \t45\n",
      "Accuracy: 0.0190226 \t46\n",
      "Accuracy: 0.0190219 \t47\n",
      "Accuracy: 0.0190211 \t48\n",
      "Accuracy: 0.0190202 \t49\n",
      "Accuracy: 0.0190193 \t50\n",
      "Accuracy: 0.0190182 \t51\n",
      "Accuracy: 0.0190171 \t52\n",
      "Accuracy: 0.0190159 \t53\n",
      "Accuracy: 0.0190146 \t54\n",
      "Accuracy: 0.0190132 \t55\n",
      "Accuracy: 0.0190117 \t56\n",
      "Accuracy: 0.0190101 \t57\n",
      "Accuracy: 0.0190085 \t58\n",
      "Accuracy: 0.0190067 \t59\n",
      "Accuracy: 0.0190049 \t60\n",
      "Accuracy: 0.019003 \t61\n",
      "Accuracy: 0.019001 \t62\n",
      "Accuracy: 0.0189989 \t63\n",
      "Accuracy: 0.0189968 \t64\n",
      "Accuracy: 0.0189945 \t65\n",
      "Accuracy: 0.0189922 \t66\n",
      "Accuracy: 0.0189898 \t67\n",
      "Accuracy: 0.0189873 \t68\n",
      "Accuracy: 0.0189847 \t69\n",
      "Accuracy: 0.018982 \t70\n",
      "Accuracy: 0.0189792 \t71\n",
      "Accuracy: 0.0189764 \t72\n",
      "Accuracy: 0.0189735 \t73\n",
      "Accuracy: 0.0189704 \t74\n",
      "Accuracy: 0.0189673 \t75\n",
      "Accuracy: 0.0189642 \t76\n",
      "Accuracy: 0.0189609 \t77\n",
      "Accuracy: 0.0189576 \t78\n",
      "Accuracy: 0.0189542 \t79\n",
      "Accuracy: 0.0189506 \t80\n",
      "Accuracy: 0.0189471 \t81\n",
      "Accuracy: 0.0189434 \t82\n",
      "Accuracy: 0.0189397 \t83\n",
      "Accuracy: 0.0189358 \t84\n",
      "Accuracy: 0.0189319 \t85\n",
      "Accuracy: 0.0189279 \t86\n",
      "Accuracy: 0.0189238 \t87\n",
      "Accuracy: 0.0189197 \t88\n",
      "Accuracy: 0.0189154 \t89\n",
      "Accuracy: 0.0189111 \t90\n",
      "Accuracy: 0.0189067 \t91\n",
      "Accuracy: 0.0189023 \t92\n",
      "Accuracy: 0.0188977 \t93\n",
      "Accuracy: 0.0188931 \t94\n",
      "Accuracy: 0.0188884 \t95\n",
      "Accuracy: 0.0188836 \t96\n",
      "Accuracy: 0.0188787 \t97\n",
      "Accuracy: 0.0188737 \t98\n",
      "Accuracy: 0.0188686 \t99\n",
      "Accuracy: 0.0177282 \t0\n",
      "Accuracy: 0.0196204 \t1\n",
      "Accuracy: 0.0218728 \t2\n",
      "Accuracy: 0.0244151 \t3\n",
      "Accuracy: 0.0271951 \t4\n",
      "Accuracy: 0.0301369 \t5\n",
      "Accuracy: 0.0331351 \t6\n",
      "Accuracy: 0.036182 \t7\n",
      "Accuracy: 0.0391195 \t8\n",
      "Accuracy: 0.0418626 \t9\n",
      "Accuracy: 0.0444025 \t10\n",
      "Accuracy: 0.0467952 \t11\n",
      "Accuracy: 0.0489007 \t12\n",
      "Accuracy: 0.0505907 \t13\n",
      "Accuracy: 0.0519032 \t14\n",
      "Accuracy: 0.0527099 \t15\n",
      "Accuracy: 0.0530213 \t16\n",
      "Accuracy: 0.0530032 \t17\n",
      "Accuracy: 0.0529768 \t18\n",
      "Accuracy: 0.0529459 \t19\n",
      "Accuracy: 0.0529106 \t20\n",
      "Accuracy: 0.0528709 \t21\n",
      "Accuracy: 0.0528269 \t22\n",
      "Accuracy: 0.052778 \t23\n",
      "Accuracy: 0.0527228 \t24\n",
      "Accuracy: 0.0526549 \t25\n",
      "Accuracy: 0.0525788 \t26\n",
      "Accuracy: 0.0524977 \t27\n",
      "Accuracy: 0.0524073 \t28\n",
      "Accuracy: 0.0523007 \t29\n",
      "Accuracy: 0.0521829 \t30\n",
      "Accuracy: 0.0520536 \t31\n",
      "Accuracy: 0.0519228 \t32\n",
      "Accuracy: 0.0517887 \t33\n",
      "Accuracy: 0.0516488 \t34\n",
      "Accuracy: 0.0515056 \t35\n",
      "Accuracy: 0.0513568 \t36\n",
      "Accuracy: 0.0512015 \t37\n",
      "Accuracy: 0.0510401 \t38\n",
      "Accuracy: 0.0508712 \t39\n",
      "Accuracy: 0.0506997 \t40\n",
      "Accuracy: 0.0505272 \t41\n",
      "Accuracy: 0.0503548 \t42\n",
      "Accuracy: 0.0501818 \t43\n",
      "Accuracy: 0.0500082 \t44\n",
      "Accuracy: 0.0498276 \t45\n",
      "Accuracy: 0.0496382 \t46\n",
      "Accuracy: 0.0494449 \t47\n",
      "Accuracy: 0.0492471 \t48\n",
      "Accuracy: 0.0490489 \t49\n",
      "Accuracy: 0.0488512 \t50\n",
      "Accuracy: 0.048657 \t51\n",
      "Accuracy: 0.048465 \t52\n",
      "Accuracy: 0.048268 \t53\n",
      "Accuracy: 0.0480703 \t54\n",
      "Accuracy: 0.0478701 \t55\n",
      "Accuracy: 0.0476661 \t56\n",
      "Accuracy: 0.0474633 \t57\n",
      "Accuracy: 0.047256 \t58\n",
      "Accuracy: 0.0470528 \t59\n",
      "Accuracy: 0.0468541 \t60\n",
      "Accuracy: 0.0466551 \t61\n",
      "Accuracy: 0.0464589 \t62\n",
      "Accuracy: 0.0462658 \t63\n",
      "Accuracy: 0.0460762 \t64\n",
      "Accuracy: 0.0458885 \t65\n",
      "Accuracy: 0.0457038 \t66\n",
      "Accuracy: 0.0455155 \t67\n",
      "Accuracy: 0.0453235 \t68\n",
      "Accuracy: 0.0451309 \t69\n",
      "Accuracy: 0.0449396 \t70\n",
      "Accuracy: 0.0447482 \t71\n",
      "Accuracy: 0.0445603 \t72\n",
      "Accuracy: 0.0443745 \t73\n",
      "Accuracy: 0.0441884 \t74\n",
      "Accuracy: 0.0440034 \t75\n",
      "Accuracy: 0.0438185 \t76\n",
      "Accuracy: 0.0436359 \t77\n",
      "Accuracy: 0.0434505 \t78\n",
      "Accuracy: 0.0432684 \t79\n",
      "Accuracy: 0.0430858 \t80\n",
      "Accuracy: 0.0429035 \t81\n",
      "Accuracy: 0.0427189 \t82\n",
      "Accuracy: 0.042534 \t83\n",
      "Accuracy: 0.0423471 \t84\n",
      "Accuracy: 0.0421603 \t85\n",
      "Accuracy: 0.0419759 \t86\n",
      "Accuracy: 0.0417903 \t87\n",
      "Accuracy: 0.041603 \t88\n",
      "Accuracy: 0.0414161 \t89\n",
      "Accuracy: 0.0412315 \t90\n",
      "Accuracy: 0.0410448 \t91\n",
      "Accuracy: 0.0408574 \t92\n",
      "Accuracy: 0.0406688 \t93\n",
      "Accuracy: 0.0404842 \t94\n",
      "Accuracy: 0.040303 \t95\n",
      "Accuracy: 0.0401224 \t96\n",
      "Accuracy: 0.0399412 \t97\n",
      "Accuracy: 0.0397581 \t98\n",
      "Accuracy: 0.0395763 \t99\n",
      "[(0.019025052, 39), (0.053021256, 16)]\n",
      "[10, 9]\n",
      "Accuracy: 0.0464183\n",
      "Accuracy: 0.0237074\n",
      "Accuracy: 0.0187393\n",
      "Final bead error: 0.0187393\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Accuracy: 0.0181569 \t0\n",
      "Accuracy: 0.0181573 \t1\n",
      "Accuracy: 0.0181577 \t2\n",
      "Accuracy: 0.0181581 \t3\n",
      "Accuracy: 0.0181585 \t4\n",
      "Accuracy: 0.0181589 \t5\n",
      "Accuracy: 0.0181593 \t6\n",
      "Accuracy: 0.0181597 \t7\n",
      "Accuracy: 0.0181601 \t8\n",
      "Accuracy: 0.0181604 \t9\n",
      "Accuracy: 0.0181608 \t10\n",
      "Accuracy: 0.0181612 \t11\n",
      "Accuracy: 0.0181615 \t12\n",
      "Accuracy: 0.0181619 \t13\n",
      "Accuracy: 0.0181622 \t14\n",
      "Accuracy: 0.0181625 \t15\n",
      "Accuracy: 0.0181628 \t16\n",
      "Accuracy: 0.0181632 \t17\n",
      "Accuracy: 0.0181635 \t18\n",
      "Accuracy: 0.0181638 \t19\n",
      "Accuracy: 0.0181641 \t20\n",
      "Accuracy: 0.0181644 \t21\n",
      "Accuracy: 0.0181646 \t22\n",
      "Accuracy: 0.0181649 \t23\n",
      "Accuracy: 0.0181652 \t24\n",
      "Accuracy: 0.0181654 \t25\n",
      "Accuracy: 0.0181657 \t26\n",
      "Accuracy: 0.018166 \t27\n",
      "Accuracy: 0.0181662 \t28\n",
      "Accuracy: 0.0181664 \t29\n",
      "Accuracy: 0.0181667 \t30\n",
      "Accuracy: 0.0181669 \t31\n",
      "Accuracy: 0.0181671 \t32\n",
      "Accuracy: 0.0181673 \t33\n",
      "Accuracy: 0.0181675 \t34\n",
      "Accuracy: 0.0181677 \t35\n",
      "Accuracy: 0.0181679 \t36\n",
      "Accuracy: 0.0181681 \t37\n",
      "Accuracy: 0.0181683 \t38\n",
      "Accuracy: 0.0181684 \t39\n",
      "Accuracy: 0.0181686 \t40\n",
      "Accuracy: 0.0181688 \t41\n",
      "Accuracy: 0.0181689 \t42\n",
      "Accuracy: 0.0181691 \t43\n",
      "Accuracy: 0.0181692 \t44\n",
      "Accuracy: 0.0181693 \t45\n",
      "Accuracy: 0.0181695 \t46\n",
      "Accuracy: 0.0181696 \t47\n",
      "Accuracy: 0.0181697 \t48\n",
      "Accuracy: 0.0181698 \t49\n",
      "Accuracy: 0.0181699 \t50\n",
      "Accuracy: 0.01817 \t51\n",
      "Accuracy: 0.0181701 \t52\n",
      "Accuracy: 0.0181702 \t53\n",
      "Accuracy: 0.0181702 \t54\n",
      "Accuracy: 0.0181703 \t55\n",
      "Accuracy: 0.0181703 \t56\n",
      "Accuracy: 0.0181704 \t57\n",
      "Accuracy: 0.0181704 \t58\n",
      "Accuracy: 0.0181705 \t59\n",
      "Accuracy: 0.0181705 \t60\n",
      "Accuracy: 0.0181706 \t61\n",
      "Accuracy: 0.0181706 \t62\n",
      "Accuracy: 0.0181706 \t63\n",
      "Accuracy: 0.0181706 \t64\n",
      "Accuracy: 0.0181706 \t65\n",
      "Accuracy: 0.0181706 \t66\n",
      "Accuracy: 0.0181706 \t67\n",
      "Accuracy: 0.0181706 \t68\n",
      "Accuracy: 0.0181706 \t69\n",
      "Accuracy: 0.0181705 \t70\n",
      "Accuracy: 0.0181705 \t71\n",
      "Accuracy: 0.0181705 \t72\n",
      "Accuracy: 0.0181704 \t73\n",
      "Accuracy: 0.0181704 \t74\n",
      "Accuracy: 0.0181703 \t75\n",
      "Accuracy: 0.0181702 \t76\n",
      "Accuracy: 0.0181702 \t77\n",
      "Accuracy: 0.0181701 \t78\n",
      "Accuracy: 0.01817 \t79\n",
      "Accuracy: 0.0181699 \t80\n",
      "Accuracy: 0.0181698 \t81\n",
      "Accuracy: 0.0181697 \t82\n",
      "Accuracy: 0.0181696 \t83\n",
      "Accuracy: 0.0181695 \t84\n",
      "Accuracy: 0.0181693 \t85\n",
      "Accuracy: 0.0181692 \t86\n",
      "Accuracy: 0.0181691 \t87\n",
      "Accuracy: 0.0181689 \t88\n",
      "Accuracy: 0.0181688 \t89\n",
      "Accuracy: 0.0181686 \t90\n",
      "Accuracy: 0.0181685 \t91\n",
      "Accuracy: 0.0181683 \t92\n",
      "Accuracy: 0.0181681 \t93\n",
      "Accuracy: 0.018168 \t94\n",
      "Accuracy: 0.0181678 \t95\n",
      "Accuracy: 0.0181676 \t96\n",
      "Accuracy: 0.0181674 \t97\n",
      "Accuracy: 0.0181672 \t98\n",
      "Accuracy: 0.018167 \t99\n",
      "Accuracy: 0.0194753 \t0\n",
      "Accuracy: 0.0214455 \t1\n",
      "Accuracy: 0.0237353 \t2\n",
      "Accuracy: 0.0263074 \t3\n",
      "Accuracy: 0.0291118 \t4\n",
      "Accuracy: 0.0320002 \t5\n",
      "Accuracy: 0.0349129 \t6\n",
      "Accuracy: 0.0378181 \t7\n",
      "Accuracy: 0.040653 \t8\n",
      "Accuracy: 0.0432108 \t9\n",
      "Accuracy: 0.0455065 \t10\n",
      "Accuracy: 0.0475571 \t11\n",
      "Accuracy: 0.04921 \t12\n",
      "Accuracy: 0.0505691 \t13\n",
      "Accuracy: 0.0515938 \t14\n",
      "Accuracy: 0.0522672 \t15\n",
      "Accuracy: 0.0524708 \t16\n",
      "Accuracy: 0.0524532 \t17\n",
      "Accuracy: 0.0524253 \t18\n",
      "Accuracy: 0.0523873 \t19\n",
      "Accuracy: 0.0523381 \t20\n",
      "Accuracy: 0.0522814 \t21\n",
      "Accuracy: 0.0522118 \t22\n",
      "Accuracy: 0.0521305 \t23\n",
      "Accuracy: 0.0520389 \t24\n",
      "Accuracy: 0.0519387 \t25\n",
      "Accuracy: 0.0518323 \t26\n",
      "Accuracy: 0.0517147 \t27\n",
      "Accuracy: 0.051589 \t28\n",
      "Accuracy: 0.0514512 \t29\n",
      "Accuracy: 0.051307 \t30\n",
      "Accuracy: 0.0511584 \t31\n",
      "Accuracy: 0.0510074 \t32\n",
      "Accuracy: 0.0508516 \t33\n",
      "Accuracy: 0.0506914 \t34\n",
      "Accuracy: 0.050526 \t35\n",
      "Accuracy: 0.0503575 \t36\n",
      "Accuracy: 0.0501894 \t37\n",
      "Accuracy: 0.0500151 \t38\n",
      "Accuracy: 0.0498358 \t39\n",
      "Accuracy: 0.0496546 \t40\n",
      "Accuracy: 0.0494678 \t41\n",
      "Accuracy: 0.0492787 \t42\n",
      "Accuracy: 0.0490825 \t43\n",
      "Accuracy: 0.0488863 \t44\n",
      "Accuracy: 0.0486916 \t45\n",
      "Accuracy: 0.0484986 \t46\n",
      "Accuracy: 0.0483016 \t47\n",
      "Accuracy: 0.0481001 \t48\n",
      "Accuracy: 0.0478977 \t49\n",
      "Accuracy: 0.0476958 \t50\n",
      "Accuracy: 0.047495 \t51\n",
      "Accuracy: 0.0472957 \t52\n",
      "Accuracy: 0.0470982 \t53\n",
      "Accuracy: 0.046901 \t54\n",
      "Accuracy: 0.0467041 \t55\n",
      "Accuracy: 0.0465097 \t56\n",
      "Accuracy: 0.0463175 \t57\n",
      "Accuracy: 0.0461265 \t58\n",
      "Accuracy: 0.0459367 \t59\n",
      "Accuracy: 0.0457481 \t60\n",
      "Accuracy: 0.0455587 \t61\n",
      "Accuracy: 0.0453697 \t62\n",
      "Accuracy: 0.0451807 \t63\n",
      "Accuracy: 0.0449924 \t64\n",
      "Accuracy: 0.0448027 \t65\n",
      "Accuracy: 0.0446134 \t66\n",
      "Accuracy: 0.0444217 \t67\n",
      "Accuracy: 0.044228 \t68\n",
      "Accuracy: 0.0440282 \t69\n",
      "Accuracy: 0.0438251 \t70\n",
      "Accuracy: 0.0436229 \t71\n",
      "Accuracy: 0.0434222 \t72\n",
      "Accuracy: 0.0432213 \t73\n",
      "Accuracy: 0.0430218 \t74\n",
      "Accuracy: 0.0428226 \t75\n",
      "Accuracy: 0.0426241 \t76\n",
      "Accuracy: 0.0424269 \t77\n",
      "Accuracy: 0.0422291 \t78\n",
      "Accuracy: 0.0420319 \t79\n",
      "Accuracy: 0.0418361 \t80\n",
      "Accuracy: 0.0416412 \t81\n",
      "Accuracy: 0.0414477 \t82\n",
      "Accuracy: 0.0412567 \t83\n",
      "Accuracy: 0.0410674 \t84\n",
      "Accuracy: 0.040879 \t85\n",
      "Accuracy: 0.0406933 \t86\n",
      "Accuracy: 0.0405094 \t87\n",
      "Accuracy: 0.0403233 \t88\n",
      "Accuracy: 0.0401367 \t89\n",
      "Accuracy: 0.0399503 \t90\n",
      "Accuracy: 0.0397616 \t91\n",
      "Accuracy: 0.0395707 \t92\n",
      "Accuracy: 0.0393801 \t93\n",
      "Accuracy: 0.0391875 \t94\n",
      "Accuracy: 0.0389951 \t95\n",
      "Accuracy: 0.038806 \t96\n",
      "Accuracy: 0.0386193 \t97\n",
      "Accuracy: 0.0384365 \t98\n",
      "Accuracy: 0.0382572 \t99\n",
      "[(0.01817061, 66), (0.052470807, 16)]\n",
      "[11, 10]\n",
      "Accuracy: 0.022526\n",
      "Accuracy: 0.0200148\n",
      "Final bead error: 0.0200148\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Accuracy: 0.0191558 \t0\n",
      "Accuracy: 0.0194272 \t1\n",
      "Accuracy: 0.0197014 \t2\n",
      "Accuracy: 0.0199776 \t3\n",
      "Accuracy: 0.0202563 \t4\n",
      "Accuracy: 0.020537 \t5\n",
      "Accuracy: 0.0208198 \t6\n",
      "Accuracy: 0.0210761 \t7\n",
      "Accuracy: 0.0213115 \t8\n",
      "Accuracy: 0.0215393 \t9\n",
      "Accuracy: 0.021752 \t10\n",
      "Accuracy: 0.0219629 \t11\n",
      "Accuracy: 0.0221623 \t12\n",
      "Accuracy: 0.0223472 \t13\n",
      "Accuracy: 0.0225059 \t14\n",
      "Accuracy: 0.0226437 \t15\n",
      "Accuracy: 0.0227734 \t16\n",
      "Accuracy: 0.0228975 \t17\n",
      "Accuracy: 0.0230103 \t18\n",
      "Accuracy: 0.0231151 \t19\n",
      "Accuracy: 0.0231967 \t20\n",
      "Accuracy: 0.0232661 \t21\n",
      "Accuracy: 0.0233295 \t22\n",
      "Accuracy: 0.0233822 \t23\n",
      "Accuracy: 0.0234209 \t24\n",
      "Accuracy: 0.0234526 \t25\n",
      "Accuracy: 0.0234821 \t26\n",
      "Accuracy: 0.023503 \t27\n",
      "Accuracy: 0.023513 \t28\n",
      "Accuracy: 0.0235224 \t29\n",
      "Accuracy: 0.0235126 \t30\n",
      "Accuracy: 0.0234876 \t31\n",
      "Accuracy: 0.0234441 \t32\n",
      "Accuracy: 0.0233943 \t33\n",
      "Accuracy: 0.0233451 \t34\n",
      "Accuracy: 0.0232996 \t35\n",
      "Accuracy: 0.0232464 \t36\n",
      "Accuracy: 0.0231903 \t37\n",
      "Accuracy: 0.0231286 \t38\n",
      "Accuracy: 0.0230628 \t39\n",
      "Accuracy: 0.0229988 \t40\n",
      "Accuracy: 0.0229275 \t41\n",
      "Accuracy: 0.0228444 \t42\n",
      "Accuracy: 0.0227632 \t43\n",
      "Accuracy: 0.0226851 \t44\n",
      "Accuracy: 0.0226059 \t45\n",
      "Accuracy: 0.022529 \t46\n",
      "Accuracy: 0.0224431 \t47\n",
      "Accuracy: 0.0223586 \t48\n",
      "Accuracy: 0.0222726 \t49\n",
      "Accuracy: 0.0221889 \t50\n",
      "Accuracy: 0.0221066 \t51\n",
      "Accuracy: 0.0220271 \t52\n",
      "Accuracy: 0.0219419 \t53\n",
      "Accuracy: 0.0218575 \t54\n",
      "Accuracy: 0.0217751 \t55\n",
      "Accuracy: 0.0216878 \t56\n",
      "Accuracy: 0.0215901 \t57\n",
      "Accuracy: 0.0214969 \t58\n",
      "Accuracy: 0.0214051 \t59\n",
      "Accuracy: 0.0213158 \t60\n",
      "Accuracy: 0.0212203 \t61\n",
      "Accuracy: 0.0211156 \t62\n",
      "Accuracy: 0.021019 \t63\n",
      "Accuracy: 0.0209335 \t64\n",
      "Accuracy: 0.0208444 \t65\n",
      "Accuracy: 0.0207466 \t66\n",
      "Accuracy: 0.020638 \t67\n",
      "Accuracy: 0.0205208 \t68\n",
      "Accuracy: 0.0204029 \t69\n",
      "Accuracy: 0.0202849 \t70\n",
      "Accuracy: 0.0201758 \t71\n",
      "Accuracy: 0.0200691 \t72\n",
      "Accuracy: 0.0199686 \t73\n",
      "Accuracy: 0.0198727 \t74\n",
      "Accuracy: 0.0197744 \t75\n",
      "Accuracy: 0.0196747 \t76\n",
      "Accuracy: 0.0195735 \t77\n",
      "Accuracy: 0.0194814 \t78\n",
      "Accuracy: 0.0193952 \t79\n",
      "Accuracy: 0.0193108 \t80\n",
      "Accuracy: 0.0192344 \t81\n",
      "Accuracy: 0.0191591 \t82\n",
      "Accuracy: 0.0190824 \t83\n",
      "Accuracy: 0.0190134 \t84\n",
      "Accuracy: 0.018952 \t85\n",
      "Accuracy: 0.018894 \t86\n",
      "Accuracy: 0.0188423 \t87\n",
      "Accuracy: 0.018805 \t88\n",
      "Accuracy: 0.0187812 \t89\n",
      "Accuracy: 0.0187638 \t90\n",
      "Accuracy: 0.0187603 \t91\n",
      "Accuracy: 0.0187686 \t92\n",
      "Accuracy: 0.0187927 \t93\n",
      "Accuracy: 0.0188334 \t94\n",
      "Accuracy: 0.0188848 \t95\n",
      "Accuracy: 0.0189461 \t96\n",
      "Accuracy: 0.0190135 \t97\n",
      "Accuracy: 0.0190993 \t98\n",
      "Accuracy: 0.0192016 \t99\n",
      "Accuracy: 0.0192149 \t0\n",
      "Accuracy: 0.0200096 \t1\n",
      "Accuracy: 0.0207131 \t2\n",
      "Accuracy: 0.0212813 \t3\n",
      "Accuracy: 0.0216854 \t4\n",
      "Accuracy: 0.0218927 \t5\n",
      "Accuracy: 0.0220752 \t6\n",
      "Accuracy: 0.0222652 \t7\n",
      "Accuracy: 0.0224574 \t8\n",
      "Accuracy: 0.0226553 \t9\n",
      "Accuracy: 0.0228497 \t10\n",
      "Accuracy: 0.0230452 \t11\n",
      "Accuracy: 0.023236 \t12\n",
      "Accuracy: 0.0234219 \t13\n",
      "Accuracy: 0.0236032 \t14\n",
      "Accuracy: 0.0237798 \t15\n",
      "Accuracy: 0.0239529 \t16\n",
      "Accuracy: 0.0241229 \t17\n",
      "Accuracy: 0.0242901 \t18\n",
      "Accuracy: 0.0244539 \t19\n",
      "Accuracy: 0.02461 \t20\n",
      "Accuracy: 0.0247633 \t21\n",
      "Accuracy: 0.0249128 \t22\n",
      "Accuracy: 0.0250567 \t23\n",
      "Accuracy: 0.0251925 \t24\n",
      "Accuracy: 0.025325 \t25\n",
      "Accuracy: 0.0254566 \t26\n",
      "Accuracy: 0.0255858 \t27\n",
      "Accuracy: 0.0257144 \t28\n",
      "Accuracy: 0.0258444 \t29\n",
      "Accuracy: 0.0259709 \t30\n",
      "Accuracy: 0.026094 \t31\n",
      "Accuracy: 0.026216 \t32\n",
      "Accuracy: 0.0263348 \t33\n",
      "Accuracy: 0.0264476 \t34\n",
      "Accuracy: 0.0265591 \t35\n",
      "Accuracy: 0.0266723 \t36\n",
      "Accuracy: 0.0267862 \t37\n",
      "Accuracy: 0.026904 \t38\n",
      "Accuracy: 0.0270246 \t39\n",
      "Accuracy: 0.0271429 \t40\n",
      "Accuracy: 0.0272666 \t41\n",
      "Accuracy: 0.0273937 \t42\n",
      "Accuracy: 0.0275196 \t43\n",
      "Accuracy: 0.027652 \t44\n",
      "Accuracy: 0.0277873 \t45\n",
      "Accuracy: 0.027925 \t46\n",
      "Accuracy: 0.0280547 \t47\n",
      "Accuracy: 0.028184 \t48\n",
      "Accuracy: 0.0283141 \t49\n",
      "Accuracy: 0.0284467 \t50\n",
      "Accuracy: 0.0285817 \t51\n",
      "Accuracy: 0.0287204 \t52\n",
      "Accuracy: 0.0288626 \t53\n",
      "Accuracy: 0.0290106 \t54\n",
      "Accuracy: 0.0291616 \t55\n",
      "Accuracy: 0.0293151 \t56\n",
      "Accuracy: 0.0294733 \t57\n",
      "Accuracy: 0.0296364 \t58\n",
      "Accuracy: 0.0298001 \t59\n",
      "Accuracy: 0.0299587 \t60\n",
      "Accuracy: 0.0301162 \t61\n",
      "Accuracy: 0.0302722 \t62\n",
      "Accuracy: 0.0304275 \t63\n",
      "Accuracy: 0.0305816 \t64\n",
      "Accuracy: 0.0307434 \t65\n",
      "Accuracy: 0.030913 \t66\n",
      "Accuracy: 0.0310869 \t67\n",
      "Accuracy: 0.0312633 \t68\n",
      "Accuracy: 0.031444 \t69\n",
      "Accuracy: 0.03163 \t70\n",
      "Accuracy: 0.031812 \t71\n",
      "Accuracy: 0.0319936 \t72\n",
      "Accuracy: 0.0321752 \t73\n",
      "Accuracy: 0.0323581 \t74\n",
      "Accuracy: 0.0325451 \t75\n",
      "Accuracy: 0.0327363 \t76\n",
      "Accuracy: 0.0329286 \t77\n",
      "Accuracy: 0.0331253 \t78\n",
      "Accuracy: 0.0333305 \t79\n",
      "Accuracy: 0.0335399 \t80\n",
      "Accuracy: 0.0337531 \t81\n",
      "Accuracy: 0.0339706 \t82\n",
      "Accuracy: 0.0341939 \t83\n",
      "Accuracy: 0.0344211 \t84\n",
      "Accuracy: 0.0346566 \t85\n",
      "Accuracy: 0.0348988 \t86\n",
      "Accuracy: 0.0351448 \t87\n",
      "Accuracy: 0.0353998 \t88\n",
      "Accuracy: 0.035659 \t89\n",
      "Accuracy: 0.0359239 \t90\n",
      "Accuracy: 0.0361913 \t91\n",
      "Accuracy: 0.0364475 \t92\n",
      "Accuracy: 0.0367064 \t93\n",
      "Accuracy: 0.0369727 \t94\n",
      "Accuracy: 0.0372367 \t95\n",
      "Accuracy: 0.0375039 \t96\n",
      "Accuracy: 0.0377767 \t97\n",
      "Accuracy: 0.0380567 \t98\n",
      "Accuracy: 0.0383339 \t99\n",
      "[(0.023522442, 29), (0.038333878, 99)]\n",
      "1\n",
      "2\n",
      "Thresh: 0.04\n",
      "Comps: 1\n",
      "***\n",
      "15.5982346591\n",
      "0.0689477678388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Used for softening the training criteria.  There's some fuzz required due to the difference in \n",
    "#training error between test and training\n",
    "thresh_multiplier = 1.1\n",
    "\n",
    "    \n",
    "results = []\n",
    "\n",
    "connecteddict = {}\n",
    "for i1 in xrange(len(models)):\n",
    "    connecteddict[i1] = 'not connected'\n",
    "\n",
    "\n",
    "for i1 in xrange(len(models)):\n",
    "    print i1\n",
    "    for i2 in xrange(len(models)):\n",
    "        \n",
    "        if i2 > i1 and ((connecteddict[i1] != connecteddict[i2]) or (connecteddict[i1] == 'not connected' or connecteddict[i2] == 'not connected')) :\n",
    "            #print \"slow1?\"\n",
    "            #print i1,i2\n",
    "            #print models[0]\n",
    "            #print models[1]\n",
    "            #print models[0].params\n",
    "            #print models[1].params\n",
    "            test = WeightString(models[i1].params[0],models[i1].params[1],models[i2].params[0],models[i2].params[1],1,1)\n",
    "\n",
    "            training_threshold = thresh\n",
    "\n",
    "            depth = 0\n",
    "            d_max = 10\n",
    "\n",
    "            #Check error between beads\n",
    "            #Alg: for each bead at depth i, SGD until converged.\n",
    "            #For beads with max error along path too large, add another bead between them, repeat\n",
    "\n",
    "            \n",
    "            #Keeps track of which indices to check the interpbeaderror between\n",
    "            newindices = [0,1]\n",
    "            \n",
    "            while (depth < d_max):\n",
    "                print newindices\n",
    "                #print \"slow2?\"\n",
    "                #X, y = GenTest(X,y)\n",
    "                counter = 0\n",
    "\n",
    "                for i,c in enumerate(test.ConvergedList):\n",
    "                    if c == False:\n",
    "                        #print \"slow3?\"\n",
    "                        error = test.SGDBead(i, .5*training_threshold, 20)\n",
    "                        #print \"slow4?\"\n",
    "                            #if counter%5000==0:\n",
    "                            #    print counter\n",
    "                            #    print error\n",
    "                        test.ConvergedList[i] = True\n",
    "\n",
    "                print test.ConvergedList\n",
    "\n",
    "                interperrors = []\n",
    "                interp_bead_indices = []\n",
    "                for b in xrange(len(test.AllBeads)-1):\n",
    "                    if b in newindices:\n",
    "                        e = InterpBeadError(test.AllBeads[b][0],test.AllBeads[b][1], test.AllBeads[b+1][0], test.AllBeads[b+1][1])\n",
    "\n",
    "                        interperrors.append(e)\n",
    "                        interp_bead_indices.append(b)\n",
    "                print interperrors\n",
    "\n",
    "                if max([ee[0] for ee in interperrors]) < thresh_multiplier*training_threshold:\n",
    "                    depth = 2*d_max\n",
    "                    #print test.ConvergedList\n",
    "                    #print test.SpringNorm(2)\n",
    "                    #print \"Done!\"\n",
    "\n",
    "                else:\n",
    "                    del newindices[:]\n",
    "                    #Interperrors stores the maximum error on the path between beads\n",
    "                    #shift index to account for added beads\n",
    "                    shift = 0\n",
    "                    for i, ie in enumerate(interperrors):\n",
    "                        if ie[0] > thresh_multiplier*training_threshold:\n",
    "                            k = interp_bead_indices[i]\n",
    "                            \n",
    "                            ws,bs = model_interpolate(test.AllBeads[k+shift][0],test.AllBeads[k+shift][1],\\\n",
    "                                                      test.AllBeads[k+shift+1][0],test.AllBeads[k+shift+1][1],\\\n",
    "                                                      ie[1]/100.)\n",
    "                            \n",
    "                            test.AllBeads.insert(k+shift+1,[ws,bs])\n",
    "                            test.ConvergedList.insert(k+shift+1, False)\n",
    "                            newindices.append(k+shift+1)\n",
    "                            newindices.append(k+shift)\n",
    "                            shift+=1\n",
    "                            #print test.ConvergedList\n",
    "                            #print test.SpringNorm(2)\n",
    "\n",
    "\n",
    "                    #print d_max\n",
    "                    depth += 1\n",
    "            if depth == 2*d_max:\n",
    "                results.append([i1,i2,test.SpringNorm(2),\"Connected\"])\n",
    "                if connecteddict[i1] == 'not connected' and connecteddict[i2] == 'not connected':\n",
    "                    connecteddict[i1] = i1\n",
    "                    connecteddict[i2] = i1\n",
    "\n",
    "                if connecteddict[i1] == 'not connected':\n",
    "                    connecteddict[i1] = connecteddict[i2]\n",
    "                else:\n",
    "                    if connecteddict[i2] == 'not connected':\n",
    "                        connecteddict[i2] = connecteddict[i1]\n",
    "                    else:\n",
    "                        if connecteddict[i1] != 'not connected' and connecteddict[i2] != 'not connected':\n",
    "                            hold = connecteddict[i2]\n",
    "                            connecteddict[i2] = connecteddict[i1]\n",
    "                            for h in xrange(len(models)):\n",
    "                                if connecteddict[h] == hold:\n",
    "                                    connecteddict[h] = connecteddict[i1]\n",
    "                    \n",
    "            else:\n",
    "                results.append([i1,i2,test.SpringNorm(2),\"Disconnected\"])\n",
    "            #print results[-1]\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\n",
    "uniquecomps = []\n",
    "totalcomps = 0\n",
    "for i in xrange(len(models)):\n",
    "    if not (connecteddict[i] in uniquecomps):\n",
    "        uniquecomps.append(connecteddict[i])\n",
    "    \n",
    "    if connecteddict[i] == 'not connected':\n",
    "        totalcomps += 1\n",
    "        \n",
    "    #print i,connecteddict[i]\n",
    "\n",
    "notconoffset = 0\n",
    "\n",
    "if 'not connected' in uniquecomps:\n",
    "    notconoffset = -1\n",
    "    \n",
    "print \"Thresh: \" + str(thresh)\n",
    "print \"Comps: \" + str(len(uniquecomps) + notconoffset + totalcomps)\n",
    "\n",
    "\n",
    "\n",
    "#for i in xrange(len(synapses)):\n",
    "#    print connecteddict[i]\n",
    "\n",
    "connsum = []\n",
    "for r in results:\n",
    "    if r[3] == \"Connected\":\n",
    "        connsum.append(r[2])\n",
    "        #print r[2]\n",
    "        \n",
    "print \"***\"\n",
    "print np.average(connsum)\n",
    "print np.std(connsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not connected'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connecteddict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[-1.4998107 , -0.23091862,  0.1918727 ,  0.20560187]], dtype=float32),\n",
       "  array([[-1.17867899,  0.78579026, -0.89930582, -1.24247086],\n",
       "         [-0.86114722, -0.1602481 , -0.86737674, -1.13779354],\n",
       "         [ 0.72442532, -0.57136399,  0.78054297, -0.60431153],\n",
       "         [-0.08741271, -0.26723036,  0.54410225,  0.19356832]], dtype=float32),\n",
       "  array([[ 0.61557311],\n",
       "         [ 0.35026345],\n",
       "         [ 0.14537683],\n",
       "         [-0.67227656]], dtype=float32)],\n",
       " [array([-0.07781173,  0.05719442, -2.12461662, -1.48168921], dtype=float32),\n",
       "  array([-1.76856709, -0.59562606,  3.31021976, -1.39359915], dtype=float32),\n",
       "  array([-0.24812987], dtype=float32)])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[-1.00434339,  0.85087883, -0.4409067 , -0.07457321]], dtype=float32),\n",
       "  array([[ 0.75883377, -0.20007004,  0.41354284,  0.29582518],\n",
       "         [ 0.00292377,  1.28025246,  0.57766509,  1.37554467],\n",
       "         [-1.0842402 ,  1.0441618 , -0.43416622,  0.22762316],\n",
       "         [-2.10338879, -1.36171842, -0.22631967,  0.58631068]], dtype=float32),\n",
       "  array([[-0.11565071],\n",
       "         [ 0.33658049],\n",
       "         [-1.14827919],\n",
       "         [ 0.68962663]], dtype=float32)],\n",
       " [array([ 0.73745489, -0.77259034, -1.86909771,  1.6973933 ], dtype=float32),\n",
       "  array([ 1.3530215 , -1.10458112,  0.21234779, -1.07358325], dtype=float32),\n",
       "  array([ 0.20686743], dtype=float32)])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[1].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.AllBeads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0417373 \t0\n",
      "Accuracy: 0.0417649 \t1\n",
      "Accuracy: 0.0417958 \t2\n",
      "Accuracy: 0.0418295 \t3\n",
      "Accuracy: 0.0418655 \t4\n",
      "Accuracy: 0.0419032 \t5\n",
      "Accuracy: 0.0419423 \t6\n",
      "Accuracy: 0.0419823 \t7\n",
      "Accuracy: 0.0420228 \t8\n",
      "Accuracy: 0.0420632 \t9\n",
      "Accuracy: 0.0421033 \t10\n",
      "Accuracy: 0.0421425 \t11\n",
      "Accuracy: 0.0421806 \t12\n",
      "Accuracy: 0.042217 \t13\n",
      "Accuracy: 0.0422514 \t14\n",
      "Accuracy: 0.0422835 \t15\n",
      "Accuracy: 0.0423129 \t16\n",
      "Accuracy: 0.0423392 \t17\n",
      "Accuracy: 0.0423621 \t18\n",
      "Accuracy: 0.0423813 \t19\n",
      "Accuracy: 0.0423964 \t20\n",
      "Accuracy: 0.0424072 \t21\n",
      "Accuracy: 0.0424134 \t22\n",
      "Accuracy: 0.0424146 \t23\n",
      "Accuracy: 0.0424107 \t24\n",
      "Accuracy: 0.0424014 \t25\n",
      "Accuracy: 0.0423864 \t26\n",
      "Accuracy: 0.0423654 \t27\n",
      "Accuracy: 0.0423383 \t28\n",
      "Accuracy: 0.0423049 \t29\n",
      "Accuracy: 0.0422649 \t30\n",
      "Accuracy: 0.0422182 \t31\n",
      "Accuracy: 0.0421646 \t32\n",
      "Accuracy: 0.042104 \t33\n",
      "Accuracy: 0.0420361 \t34\n",
      "Accuracy: 0.0419609 \t35\n",
      "Accuracy: 0.0418782 \t36\n",
      "Accuracy: 0.0417879 \t37\n",
      "Accuracy: 0.0416899 \t38\n",
      "Accuracy: 0.0415841 \t39\n",
      "Accuracy: 0.0414705 \t40\n",
      "Accuracy: 0.041349 \t41\n",
      "Accuracy: 0.0412195 \t42\n",
      "Accuracy: 0.041082 \t43\n",
      "Accuracy: 0.0409365 \t44\n",
      "Accuracy: 0.040783 \t45\n",
      "Accuracy: 0.0406215 \t46\n",
      "Accuracy: 0.040452 \t47\n",
      "Accuracy: 0.0402744 \t48\n",
      "Accuracy: 0.040089 \t49\n",
      "Accuracy: 0.0398957 \t50\n",
      "Accuracy: 0.0396946 \t51\n",
      "Accuracy: 0.0394857 \t52\n",
      "Accuracy: 0.0392693 \t53\n",
      "Accuracy: 0.0390453 \t54\n",
      "Accuracy: 0.038814 \t55\n",
      "Accuracy: 0.0385754 \t56\n",
      "Accuracy: 0.0383298 \t57\n",
      "Accuracy: 0.0380772 \t58\n",
      "Accuracy: 0.0378179 \t59\n",
      "Accuracy: 0.037552 \t60\n",
      "Accuracy: 0.0372799 \t61\n",
      "Accuracy: 0.0370016 \t62\n",
      "Accuracy: 0.0367175 \t63\n",
      "Accuracy: 0.0364278 \t64\n",
      "Accuracy: 0.0361328 \t65\n",
      "Accuracy: 0.0358327 \t66\n",
      "Accuracy: 0.0355279 \t67\n",
      "Accuracy: 0.0352187 \t68\n",
      "Accuracy: 0.0349053 \t69\n",
      "Accuracy: 0.0345882 \t70\n",
      "Accuracy: 0.0342678 \t71\n",
      "Accuracy: 0.0339443 \t72\n",
      "Accuracy: 0.0336182 \t73\n",
      "Accuracy: 0.0332899 \t74\n",
      "Accuracy: 0.0329598 \t75\n",
      "Accuracy: 0.0326283 \t76\n",
      "Accuracy: 0.032296 \t77\n",
      "Accuracy: 0.0319632 \t78\n",
      "Accuracy: 0.0316305 \t79\n",
      "Accuracy: 0.0312983 \t80\n",
      "Accuracy: 0.0309673 \t81\n",
      "Accuracy: 0.0306378 \t82\n",
      "Accuracy: 0.0302642 \t83\n",
      "Accuracy: 0.0297479 \t84\n",
      "Accuracy: 0.0291486 \t85\n",
      "Accuracy: 0.0285088 \t86\n",
      "Accuracy: 0.0278548 \t87\n",
      "Accuracy: 0.0272019 \t88\n",
      "Accuracy: 0.0265603 \t89\n",
      "Accuracy: 0.0259373 \t90\n",
      "Accuracy: 0.0253339 \t91\n",
      "Accuracy: 0.0247518 \t92\n",
      "Accuracy: 0.0241901 \t93\n",
      "Accuracy: 0.0236501 \t94\n",
      "Accuracy: 0.0231299 \t95\n",
      "Accuracy: 0.022629 \t96\n",
      "Accuracy: 0.0221506 \t97\n",
      "Accuracy: 0.0216911 \t98\n",
      "Accuracy: 0.0212481 \t99\n",
      "Accuracy: 0.0208234 \t0\n",
      "Accuracy: 0.020939 \t1\n",
      "Accuracy: 0.0210577 \t2\n",
      "Accuracy: 0.0211794 \t3\n",
      "Accuracy: 0.0213038 \t4\n",
      "Accuracy: 0.0214307 \t5\n",
      "Accuracy: 0.0215601 \t6\n",
      "Accuracy: 0.0216915 \t7\n",
      "Accuracy: 0.0218248 \t8\n",
      "Accuracy: 0.0219598 \t9\n",
      "Accuracy: 0.0220963 \t10\n",
      "Accuracy: 0.0222341 \t11\n",
      "Accuracy: 0.0223729 \t12\n",
      "Accuracy: 0.0225125 \t13\n",
      "Accuracy: 0.0226527 \t14\n",
      "Accuracy: 0.0227932 \t15\n",
      "Accuracy: 0.0229339 \t16\n",
      "Accuracy: 0.0230746 \t17\n",
      "Accuracy: 0.0232149 \t18\n",
      "Accuracy: 0.0233546 \t19\n",
      "Accuracy: 0.0234936 \t20\n",
      "Accuracy: 0.0236315 \t21\n",
      "Accuracy: 0.0237682 \t22\n",
      "Accuracy: 0.0239035 \t23\n",
      "Accuracy: 0.0240371 \t24\n",
      "Accuracy: 0.0241687 \t25\n",
      "Accuracy: 0.0242982 \t26\n",
      "Accuracy: 0.0244253 \t27\n",
      "Accuracy: 0.0245499 \t28\n",
      "Accuracy: 0.0246716 \t29\n",
      "Accuracy: 0.0247902 \t30\n",
      "Accuracy: 0.0249057 \t31\n",
      "Accuracy: 0.0250176 \t32\n",
      "Accuracy: 0.0251259 \t33\n",
      "Accuracy: 0.0252302 \t34\n",
      "Accuracy: 0.0253305 \t35\n",
      "Accuracy: 0.0254265 \t36\n",
      "Accuracy: 0.0255181 \t37\n",
      "Accuracy: 0.0256049 \t38\n",
      "Accuracy: 0.0256869 \t39\n",
      "Accuracy: 0.0257639 \t40\n",
      "Accuracy: 0.0258356 \t41\n",
      "Accuracy: 0.0259019 \t42\n",
      "Accuracy: 0.0259627 \t43\n",
      "Accuracy: 0.0260178 \t44\n",
      "Accuracy: 0.026067 \t45\n",
      "Accuracy: 0.0261103 \t46\n",
      "Accuracy: 0.0261473 \t47\n",
      "Accuracy: 0.0261781 \t48\n",
      "Accuracy: 0.0262025 \t49\n",
      "Accuracy: 0.0262205 \t50\n",
      "Accuracy: 0.0262318 \t51\n",
      "Accuracy: 0.0262364 \t52\n",
      "Accuracy: 0.0262342 \t53\n",
      "Accuracy: 0.0262252 \t54\n",
      "Accuracy: 0.0262092 \t55\n",
      "Accuracy: 0.0261863 \t56\n",
      "Accuracy: 0.0261564 \t57\n",
      "Accuracy: 0.0261194 \t58\n",
      "Accuracy: 0.0260753 \t59\n",
      "Accuracy: 0.0260241 \t60\n",
      "Accuracy: 0.0259659 \t61\n",
      "Accuracy: 0.0259006 \t62\n",
      "Accuracy: 0.0258283 \t63\n",
      "Accuracy: 0.025749 \t64\n",
      "Accuracy: 0.0256628 \t65\n",
      "Accuracy: 0.0255697 \t66\n",
      "Accuracy: 0.0254699 \t67\n",
      "Accuracy: 0.0253634 \t68\n",
      "Accuracy: 0.0252504 \t69\n",
      "Accuracy: 0.025131 \t70\n",
      "Accuracy: 0.0250053 \t71\n",
      "Accuracy: 0.0248736 \t72\n",
      "Accuracy: 0.0247359 \t73\n",
      "Accuracy: 0.0245926 \t74\n",
      "Accuracy: 0.0244438 \t75\n",
      "Accuracy: 0.0242898 \t76\n",
      "Accuracy: 0.0241308 \t77\n",
      "Accuracy: 0.0239672 \t78\n",
      "Accuracy: 0.0237991 \t79\n",
      "Accuracy: 0.023627 \t80\n",
      "Accuracy: 0.0234513 \t81\n",
      "Accuracy: 0.0232721 \t82\n",
      "Accuracy: 0.02309 \t83\n",
      "Accuracy: 0.0229054 \t84\n",
      "Accuracy: 0.0227186 \t85\n",
      "Accuracy: 0.0225302 \t86\n",
      "Accuracy: 0.0223406 \t87\n",
      "Accuracy: 0.0221504 \t88\n",
      "Accuracy: 0.02196 \t89\n",
      "Accuracy: 0.0217703 \t90\n",
      "Accuracy: 0.021593 \t91\n",
      "Accuracy: 0.0214287 \t92\n",
      "Accuracy: 0.0212835 \t93\n",
      "Accuracy: 0.0211502 \t94\n",
      "Accuracy: 0.021031 \t95\n",
      "Accuracy: 0.0209271 \t96\n",
      "Accuracy: 0.0208377 \t97\n",
      "Accuracy: 0.0207599 \t98\n",
      "Accuracy: 0.0206996 \t99\n",
      "Accuracy: 0.0206582 \t0\n",
      "Accuracy: 0.0206239 \t1\n",
      "Accuracy: 0.0205912 \t2\n",
      "Accuracy: 0.0205598 \t3\n",
      "Accuracy: 0.0205292 \t4\n",
      "Accuracy: 0.0205007 \t5\n",
      "Accuracy: 0.0204735 \t6\n",
      "Accuracy: 0.0204476 \t7\n",
      "Accuracy: 0.020423 \t8\n",
      "Accuracy: 0.0203993 \t9\n",
      "Accuracy: 0.020376 \t10\n",
      "Accuracy: 0.0203534 \t11\n",
      "Accuracy: 0.0203323 \t12\n",
      "Accuracy: 0.0203127 \t13\n",
      "Accuracy: 0.0202942 \t14\n",
      "Accuracy: 0.0202767 \t15\n",
      "Accuracy: 0.0202596 \t16\n",
      "Accuracy: 0.0202437 \t17\n",
      "Accuracy: 0.0202286 \t18\n",
      "Accuracy: 0.0202139 \t19\n",
      "Accuracy: 0.0201999 \t20\n",
      "Accuracy: 0.0201867 \t21\n",
      "Accuracy: 0.020174 \t22\n",
      "Accuracy: 0.0201625 \t23\n",
      "Accuracy: 0.0201516 \t24\n",
      "Accuracy: 0.0201414 \t25\n",
      "Accuracy: 0.0201316 \t26\n",
      "Accuracy: 0.0201221 \t27\n",
      "Accuracy: 0.0201133 \t28\n",
      "Accuracy: 0.0201053 \t29\n",
      "Accuracy: 0.0200981 \t30\n",
      "Accuracy: 0.0200914 \t31\n",
      "Accuracy: 0.0200852 \t32\n",
      "Accuracy: 0.0200802 \t33\n",
      "Accuracy: 0.0200761 \t34\n",
      "Accuracy: 0.0200729 \t35\n",
      "Accuracy: 0.0200708 \t36\n",
      "Accuracy: 0.0200697 \t37\n",
      "Accuracy: 0.0200691 \t38\n",
      "Accuracy: 0.0200686 \t39\n",
      "Accuracy: 0.0200684 \t40\n",
      "Accuracy: 0.0200691 \t41\n",
      "Accuracy: 0.0200709 \t42\n",
      "Accuracy: 0.0200735 \t43\n",
      "Accuracy: 0.0200769 \t44\n",
      "Accuracy: 0.020081 \t45\n",
      "Accuracy: 0.0200853 \t46\n",
      "Accuracy: 0.0200909 \t47\n",
      "Accuracy: 0.0200969 \t48\n",
      "Accuracy: 0.0201037 \t49\n",
      "Accuracy: 0.0201112 \t50\n",
      "Accuracy: 0.0201191 \t51\n",
      "Accuracy: 0.0201276 \t52\n",
      "Accuracy: 0.0201365 \t53\n",
      "Accuracy: 0.0201459 \t54\n",
      "Accuracy: 0.020156 \t55\n",
      "Accuracy: 0.0201665 \t56\n",
      "Accuracy: 0.0201776 \t57\n",
      "Accuracy: 0.0201894 \t58\n",
      "Accuracy: 0.0202019 \t59\n",
      "Accuracy: 0.0202151 \t60\n",
      "Accuracy: 0.020229 \t61\n",
      "Accuracy: 0.0202436 \t62\n",
      "Accuracy: 0.0202588 \t63\n",
      "Accuracy: 0.0202745 \t64\n",
      "Accuracy: 0.0202906 \t65\n",
      "Accuracy: 0.0203071 \t66\n",
      "Accuracy: 0.0203245 \t67\n",
      "Accuracy: 0.0203427 \t68\n",
      "Accuracy: 0.020362 \t69\n",
      "Accuracy: 0.0203819 \t70\n",
      "Accuracy: 0.0204024 \t71\n",
      "Accuracy: 0.020424 \t72\n",
      "Accuracy: 0.020446 \t73\n",
      "Accuracy: 0.0204685 \t74\n",
      "Accuracy: 0.0204918 \t75\n",
      "Accuracy: 0.0205157 \t76\n",
      "Accuracy: 0.0205405 \t77\n",
      "Accuracy: 0.0205659 \t78\n",
      "Accuracy: 0.0205918 \t79\n",
      "Accuracy: 0.0206183 \t80\n",
      "Accuracy: 0.0206458 \t81\n",
      "Accuracy: 0.0206741 \t82\n",
      "Accuracy: 0.0207032 \t83\n",
      "Accuracy: 0.020733 \t84\n",
      "Accuracy: 0.0207636 \t85\n",
      "Accuracy: 0.0207949 \t86\n",
      "Accuracy: 0.0208273 \t87\n",
      "Accuracy: 0.0208607 \t88\n",
      "Accuracy: 0.020895 \t89\n",
      "Accuracy: 0.02093 \t90\n",
      "Accuracy: 0.0209657 \t91\n",
      "Accuracy: 0.0210022 \t92\n",
      "Accuracy: 0.0210394 \t93\n",
      "Accuracy: 0.0210774 \t94\n",
      "Accuracy: 0.0211161 \t95\n",
      "Accuracy: 0.0211555 \t96\n",
      "Accuracy: 0.0211959 \t97\n",
      "Accuracy: 0.0212372 \t98\n",
      "Accuracy: 0.0212796 \t99\n",
      "Accuracy: 0.0213422 \t0\n",
      "Accuracy: 0.0213334 \t1\n",
      "Accuracy: 0.0213246 \t2\n",
      "Accuracy: 0.0213159 \t3\n",
      "Accuracy: 0.0213073 \t4\n",
      "Accuracy: 0.0212988 \t5\n",
      "Accuracy: 0.0212903 \t6\n",
      "Accuracy: 0.021282 \t7\n",
      "Accuracy: 0.0212736 \t8\n",
      "Accuracy: 0.0212654 \t9\n",
      "Accuracy: 0.0212572 \t10\n",
      "Accuracy: 0.0212491 \t11\n",
      "Accuracy: 0.0212411 \t12\n",
      "Accuracy: 0.0212331 \t13\n",
      "Accuracy: 0.0212252 \t14\n",
      "Accuracy: 0.0212174 \t15\n",
      "Accuracy: 0.0212096 \t16\n",
      "Accuracy: 0.021202 \t17\n",
      "Accuracy: 0.0211943 \t18\n",
      "Accuracy: 0.0211868 \t19\n",
      "Accuracy: 0.0211794 \t20\n",
      "Accuracy: 0.021172 \t21\n",
      "Accuracy: 0.0211647 \t22\n",
      "Accuracy: 0.0211575 \t23\n",
      "Accuracy: 0.0211503 \t24\n",
      "Accuracy: 0.0211432 \t25\n",
      "Accuracy: 0.0211361 \t26\n",
      "Accuracy: 0.0211291 \t27\n",
      "Accuracy: 0.0211222 \t28\n",
      "Accuracy: 0.0211153 \t29\n",
      "Accuracy: 0.0211085 \t30\n",
      "Accuracy: 0.0211018 \t31\n",
      "Accuracy: 0.0210951 \t32\n",
      "Accuracy: 0.0210885 \t33\n",
      "Accuracy: 0.021082 \t34\n",
      "Accuracy: 0.0210755 \t35\n",
      "Accuracy: 0.0210691 \t36\n",
      "Accuracy: 0.0210628 \t37\n",
      "Accuracy: 0.0210566 \t38\n",
      "Accuracy: 0.0210504 \t39\n",
      "Accuracy: 0.0210444 \t40\n",
      "Accuracy: 0.0210388 \t41\n",
      "Accuracy: 0.0210334 \t42\n",
      "Accuracy: 0.021028 \t43\n",
      "Accuracy: 0.0210227 \t44\n",
      "Accuracy: 0.0210176 \t45\n",
      "Accuracy: 0.0210125 \t46\n",
      "Accuracy: 0.0210075 \t47\n",
      "Accuracy: 0.0210025 \t48\n",
      "Accuracy: 0.0209976 \t49\n",
      "Accuracy: 0.0209928 \t50\n",
      "Accuracy: 0.020988 \t51\n",
      "Accuracy: 0.0209833 \t52\n",
      "Accuracy: 0.0209787 \t53\n",
      "Accuracy: 0.0209742 \t54\n",
      "Accuracy: 0.0209697 \t55\n",
      "Accuracy: 0.0209652 \t56\n",
      "Accuracy: 0.0209608 \t57\n",
      "Accuracy: 0.0209565 \t58\n",
      "Accuracy: 0.0209522 \t59\n",
      "Accuracy: 0.020948 \t60\n",
      "Accuracy: 0.0209438 \t61\n",
      "Accuracy: 0.0209397 \t62\n",
      "Accuracy: 0.0209357 \t63\n",
      "Accuracy: 0.0209317 \t64\n",
      "Accuracy: 0.0209278 \t65\n",
      "Accuracy: 0.0209239 \t66\n",
      "Accuracy: 0.0209201 \t67\n",
      "Accuracy: 0.0209163 \t68\n",
      "Accuracy: 0.0209126 \t69\n",
      "Accuracy: 0.0209089 \t70\n",
      "Accuracy: 0.0209054 \t71\n",
      "Accuracy: 0.0209018 \t72\n",
      "Accuracy: 0.0208983 \t73\n",
      "Accuracy: 0.0208949 \t74\n",
      "Accuracy: 0.0208916 \t75\n",
      "Accuracy: 0.0208883 \t76\n",
      "Accuracy: 0.0208851 \t77\n",
      "Accuracy: 0.0208819 \t78\n",
      "Accuracy: 0.0208788 \t79\n",
      "Accuracy: 0.0208758 \t80\n",
      "Accuracy: 0.0208727 \t81\n",
      "Accuracy: 0.0208698 \t82\n",
      "Accuracy: 0.0208669 \t83\n",
      "Accuracy: 0.0208641 \t84\n",
      "Accuracy: 0.0208613 \t85\n",
      "Accuracy: 0.0208586 \t86\n",
      "Accuracy: 0.0208559 \t87\n",
      "Accuracy: 0.0208533 \t88\n",
      "Accuracy: 0.0208508 \t89\n",
      "Accuracy: 0.0208483 \t90\n",
      "Accuracy: 0.0208459 \t91\n",
      "Accuracy: 0.0208435 \t92\n",
      "Accuracy: 0.0208412 \t93\n",
      "Accuracy: 0.0208389 \t94\n",
      "Accuracy: 0.0208367 \t95\n",
      "Accuracy: 0.0208345 \t96\n",
      "Accuracy: 0.0208324 \t97\n",
      "Accuracy: 0.0208304 \t98\n",
      "Accuracy: 0.0208284 \t99\n",
      "Accuracy: 0.0208265 \t0\n",
      "Accuracy: 0.0208142 \t1\n",
      "Accuracy: 0.0208018 \t2\n",
      "Accuracy: 0.0207895 \t3\n",
      "Accuracy: 0.0207772 \t4\n",
      "Accuracy: 0.020765 \t5\n",
      "Accuracy: 0.0207527 \t6\n",
      "Accuracy: 0.0207405 \t7\n",
      "Accuracy: 0.0207283 \t8\n",
      "Accuracy: 0.0207161 \t9\n",
      "Accuracy: 0.0207039 \t10\n",
      "Accuracy: 0.0206917 \t11\n",
      "Accuracy: 0.0206795 \t12\n",
      "Accuracy: 0.0206673 \t13\n",
      "Accuracy: 0.0206552 \t14\n",
      "Accuracy: 0.020643 \t15\n",
      "Accuracy: 0.0206325 \t16\n",
      "Accuracy: 0.0206222 \t17\n",
      "Accuracy: 0.0206119 \t18\n",
      "Accuracy: 0.0206017 \t19\n",
      "Accuracy: 0.0205915 \t20\n",
      "Accuracy: 0.0205812 \t21\n",
      "Accuracy: 0.020571 \t22\n",
      "Accuracy: 0.0205608 \t23\n",
      "Accuracy: 0.0205506 \t24\n",
      "Accuracy: 0.0205404 \t25\n",
      "Accuracy: 0.0205302 \t26\n",
      "Accuracy: 0.02052 \t27\n",
      "Accuracy: 0.0205098 \t28\n",
      "Accuracy: 0.0204997 \t29\n",
      "Accuracy: 0.0204895 \t30\n",
      "Accuracy: 0.0204794 \t31\n",
      "Accuracy: 0.0204692 \t32\n",
      "Accuracy: 0.0204591 \t33\n",
      "Accuracy: 0.020449 \t34\n",
      "Accuracy: 0.0204389 \t35\n",
      "Accuracy: 0.0204287 \t36\n",
      "Accuracy: 0.0204186 \t37\n",
      "Accuracy: 0.0204086 \t38\n",
      "Accuracy: 0.0203985 \t39\n",
      "Accuracy: 0.0203884 \t40\n",
      "Accuracy: 0.0203783 \t41\n",
      "Accuracy: 0.0203683 \t42\n",
      "Accuracy: 0.0203582 \t43\n",
      "Accuracy: 0.0203481 \t44\n",
      "Accuracy: 0.0203381 \t45\n",
      "Accuracy: 0.0203281 \t46\n",
      "Accuracy: 0.020318 \t47\n",
      "Accuracy: 0.020308 \t48\n",
      "Accuracy: 0.020298 \t49\n",
      "Accuracy: 0.020288 \t50\n",
      "Accuracy: 0.020278 \t51\n",
      "Accuracy: 0.020268 \t52\n",
      "Accuracy: 0.0202581 \t53\n",
      "Accuracy: 0.0202481 \t54\n",
      "Accuracy: 0.0202382 \t55\n",
      "Accuracy: 0.0202282 \t56\n",
      "Accuracy: 0.0202183 \t57\n",
      "Accuracy: 0.0202084 \t58\n",
      "Accuracy: 0.0201985 \t59\n",
      "Accuracy: 0.0201886 \t60\n",
      "Accuracy: 0.0201787 \t61\n",
      "Accuracy: 0.0201688 \t62\n",
      "Accuracy: 0.0201589 \t63\n",
      "Accuracy: 0.020149 \t64\n",
      "Accuracy: 0.0201391 \t65\n",
      "Accuracy: 0.0201293 \t66\n",
      "Accuracy: 0.0201194 \t67\n",
      "Accuracy: 0.0201096 \t68\n",
      "Accuracy: 0.0200997 \t69\n",
      "Accuracy: 0.0200899 \t70\n",
      "Accuracy: 0.02008 \t71\n",
      "Accuracy: 0.0200702 \t72\n",
      "Accuracy: 0.0200604 \t73\n",
      "Accuracy: 0.0200506 \t74\n",
      "Accuracy: 0.0200408 \t75\n",
      "Accuracy: 0.020031 \t76\n",
      "Accuracy: 0.0200212 \t77\n",
      "Accuracy: 0.0200115 \t78\n",
      "Accuracy: 0.0200017 \t79\n",
      "Accuracy: 0.0199919 \t80\n",
      "Accuracy: 0.0199822 \t81\n",
      "Accuracy: 0.0199725 \t82\n",
      "Accuracy: 0.0199627 \t83\n",
      "Accuracy: 0.019953 \t84\n",
      "Accuracy: 0.0199433 \t85\n",
      "Accuracy: 0.0199336 \t86\n",
      "Accuracy: 0.0199239 \t87\n",
      "Accuracy: 0.0199143 \t88\n",
      "Accuracy: 0.0199046 \t89\n",
      "Accuracy: 0.0198949 \t90\n",
      "Accuracy: 0.0198853 \t91\n",
      "Accuracy: 0.0198756 \t92\n",
      "Accuracy: 0.019866 \t93\n",
      "Accuracy: 0.0198563 \t94\n",
      "Accuracy: 0.0198467 \t95\n",
      "Accuracy: 0.0198371 \t96\n",
      "Accuracy: 0.0198275 \t97\n",
      "Accuracy: 0.0198179 \t98\n",
      "Accuracy: 0.0198083 \t99\n",
      "Accuracy: 0.0197987 \t0\n",
      "Accuracy: 0.0197993 \t1\n",
      "Accuracy: 0.0197999 \t2\n",
      "Accuracy: 0.0198005 \t3\n",
      "Accuracy: 0.0198011 \t4\n",
      "Accuracy: 0.0198017 \t5\n",
      "Accuracy: 0.0198024 \t6\n",
      "Accuracy: 0.019803 \t7\n",
      "Accuracy: 0.0198036 \t8\n",
      "Accuracy: 0.0198043 \t9\n",
      "Accuracy: 0.0198049 \t10\n",
      "Accuracy: 0.0198056 \t11\n",
      "Accuracy: 0.0198062 \t12\n",
      "Accuracy: 0.0198068 \t13\n",
      "Accuracy: 0.0198075 \t14\n",
      "Accuracy: 0.0198081 \t15\n",
      "Accuracy: 0.0198088 \t16\n",
      "Accuracy: 0.0198094 \t17\n",
      "Accuracy: 0.0198101 \t18\n",
      "Accuracy: 0.0198107 \t19\n",
      "Accuracy: 0.0198114 \t20\n",
      "Accuracy: 0.019812 \t21\n",
      "Accuracy: 0.0198127 \t22\n",
      "Accuracy: 0.0198134 \t23\n",
      "Accuracy: 0.019814 \t24\n",
      "Accuracy: 0.0198147 \t25\n",
      "Accuracy: 0.0198153 \t26\n",
      "Accuracy: 0.019816 \t27\n",
      "Accuracy: 0.0198166 \t28\n",
      "Accuracy: 0.0198173 \t29\n",
      "Accuracy: 0.0198179 \t30\n",
      "Accuracy: 0.0198186 \t31\n",
      "Accuracy: 0.0198193 \t32\n",
      "Accuracy: 0.01982 \t33\n",
      "Accuracy: 0.0198207 \t34\n",
      "Accuracy: 0.0198213 \t35\n",
      "Accuracy: 0.019822 \t36\n",
      "Accuracy: 0.0198227 \t37\n",
      "Accuracy: 0.0198233 \t38\n",
      "Accuracy: 0.019824 \t39\n",
      "Accuracy: 0.0198247 \t40\n",
      "Accuracy: 0.0198255 \t41\n",
      "Accuracy: 0.0198262 \t42\n",
      "Accuracy: 0.019827 \t43\n",
      "Accuracy: 0.0198278 \t44\n",
      "Accuracy: 0.0198286 \t45\n",
      "Accuracy: 0.0198294 \t46\n",
      "Accuracy: 0.0198302 \t47\n",
      "Accuracy: 0.019831 \t48\n",
      "Accuracy: 0.0198319 \t49\n",
      "Accuracy: 0.0198327 \t50\n",
      "Accuracy: 0.0198336 \t51\n",
      "Accuracy: 0.0198344 \t52\n",
      "Accuracy: 0.0198353 \t53\n",
      "Accuracy: 0.0198361 \t54\n",
      "Accuracy: 0.019837 \t55\n",
      "Accuracy: 0.0198379 \t56\n",
      "Accuracy: 0.0198388 \t57\n",
      "Accuracy: 0.0198397 \t58\n",
      "Accuracy: 0.0198406 \t59\n",
      "Accuracy: 0.0198415 \t60\n",
      "Accuracy: 0.0198424 \t61\n",
      "Accuracy: 0.0198433 \t62\n",
      "Accuracy: 0.0198443 \t63\n",
      "Accuracy: 0.0198452 \t64\n",
      "Accuracy: 0.0198461 \t65\n",
      "Accuracy: 0.019847 \t66\n",
      "Accuracy: 0.0198479 \t67\n",
      "Accuracy: 0.0198489 \t68\n",
      "Accuracy: 0.0198498 \t69\n",
      "Accuracy: 0.0198508 \t70\n",
      "Accuracy: 0.0198517 \t71\n",
      "Accuracy: 0.0198527 \t72\n",
      "Accuracy: 0.0198536 \t73\n",
      "Accuracy: 0.0198546 \t74\n",
      "Accuracy: 0.0198555 \t75\n",
      "Accuracy: 0.0198564 \t76\n",
      "Accuracy: 0.0198574 \t77\n",
      "Accuracy: 0.0198583 \t78\n",
      "Accuracy: 0.0198593 \t79\n",
      "Accuracy: 0.0198602 \t80\n",
      "Accuracy: 0.0198611 \t81\n",
      "Accuracy: 0.0198621 \t82\n",
      "Accuracy: 0.019863 \t83\n",
      "Accuracy: 0.0198639 \t84\n",
      "Accuracy: 0.0198648 \t85\n",
      "Accuracy: 0.0198658 \t86\n",
      "Accuracy: 0.0198667 \t87\n",
      "Accuracy: 0.0198676 \t88\n",
      "Accuracy: 0.0198685 \t89\n",
      "Accuracy: 0.0198694 \t90\n",
      "Accuracy: 0.0198704 \t91\n",
      "Accuracy: 0.0198713 \t92\n",
      "Accuracy: 0.0198723 \t93\n",
      "Accuracy: 0.0198732 \t94\n",
      "Accuracy: 0.0198742 \t95\n",
      "Accuracy: 0.0198752 \t96\n",
      "Accuracy: 0.0198763 \t97\n",
      "Accuracy: 0.0198774 \t98\n",
      "Accuracy: 0.0198785 \t99\n",
      "Accuracy: 0.0198797 \t0\n",
      "Accuracy: 0.0198789 \t1\n",
      "Accuracy: 0.019878 \t2\n",
      "Accuracy: 0.0198772 \t3\n",
      "Accuracy: 0.0198764 \t4\n",
      "Accuracy: 0.0198756 \t5\n",
      "Accuracy: 0.0198747 \t6\n",
      "Accuracy: 0.0198739 \t7\n",
      "Accuracy: 0.0198731 \t8\n",
      "Accuracy: 0.0198723 \t9\n",
      "Accuracy: 0.0198715 \t10\n",
      "Accuracy: 0.0198707 \t11\n",
      "Accuracy: 0.0198699 \t12\n",
      "Accuracy: 0.019869 \t13\n",
      "Accuracy: 0.0198682 \t14\n",
      "Accuracy: 0.0198674 \t15\n",
      "Accuracy: 0.0198665 \t16\n",
      "Accuracy: 0.0198657 \t17\n",
      "Accuracy: 0.0198648 \t18\n",
      "Accuracy: 0.019864 \t19\n",
      "Accuracy: 0.0198631 \t20\n",
      "Accuracy: 0.0198622 \t21\n",
      "Accuracy: 0.0198614 \t22\n",
      "Accuracy: 0.0198605 \t23\n",
      "Accuracy: 0.0198596 \t24\n",
      "Accuracy: 0.0198588 \t25\n",
      "Accuracy: 0.0198579 \t26\n",
      "Accuracy: 0.019857 \t27\n",
      "Accuracy: 0.0198561 \t28\n",
      "Accuracy: 0.0198552 \t29\n",
      "Accuracy: 0.0198544 \t30\n",
      "Accuracy: 0.0198535 \t31\n",
      "Accuracy: 0.0198526 \t32\n",
      "Accuracy: 0.0198517 \t33\n",
      "Accuracy: 0.0198509 \t34\n",
      "Accuracy: 0.01985 \t35\n",
      "Accuracy: 0.0198491 \t36\n",
      "Accuracy: 0.0198482 \t37\n",
      "Accuracy: 0.0198473 \t38\n",
      "Accuracy: 0.0198464 \t39\n",
      "Accuracy: 0.0198455 \t40\n",
      "Accuracy: 0.0198446 \t41\n",
      "Accuracy: 0.0198437 \t42\n",
      "Accuracy: 0.0198428 \t43\n",
      "Accuracy: 0.0198419 \t44\n",
      "Accuracy: 0.0198409 \t45\n",
      "Accuracy: 0.01984 \t46\n",
      "Accuracy: 0.0198391 \t47\n",
      "Accuracy: 0.0198381 \t48\n",
      "Accuracy: 0.0198372 \t49\n",
      "Accuracy: 0.0198363 \t50\n",
      "Accuracy: 0.0198353 \t51\n",
      "Accuracy: 0.0198344 \t52\n",
      "Accuracy: 0.0198334 \t53\n",
      "Accuracy: 0.0198325 \t54\n",
      "Accuracy: 0.0198316 \t55\n",
      "Accuracy: 0.0198307 \t56\n",
      "Accuracy: 0.0198298 \t57\n",
      "Accuracy: 0.0198289 \t58\n",
      "Accuracy: 0.019828 \t59\n",
      "Accuracy: 0.0198271 \t60\n",
      "Accuracy: 0.0198262 \t61\n",
      "Accuracy: 0.0198252 \t62\n",
      "Accuracy: 0.0198243 \t63\n",
      "Accuracy: 0.0198234 \t64\n",
      "Accuracy: 0.0198225 \t65\n",
      "Accuracy: 0.0198216 \t66\n",
      "Accuracy: 0.0198207 \t67\n",
      "Accuracy: 0.0198199 \t68\n",
      "Accuracy: 0.019819 \t69\n",
      "Accuracy: 0.0198181 \t70\n",
      "Accuracy: 0.0198172 \t71\n",
      "Accuracy: 0.0198164 \t72\n",
      "Accuracy: 0.0198155 \t73\n",
      "Accuracy: 0.0198146 \t74\n",
      "Accuracy: 0.0198137 \t75\n",
      "Accuracy: 0.0198128 \t76\n",
      "Accuracy: 0.0198119 \t77\n",
      "Accuracy: 0.0198109 \t78\n",
      "Accuracy: 0.01981 \t79\n",
      "Accuracy: 0.0198091 \t80\n",
      "Accuracy: 0.0198082 \t81\n",
      "Accuracy: 0.0198072 \t82\n",
      "Accuracy: 0.0198063 \t83\n",
      "Accuracy: 0.0198053 \t84\n",
      "Accuracy: 0.0198044 \t85\n",
      "Accuracy: 0.0198034 \t86\n",
      "Accuracy: 0.0198025 \t87\n",
      "Accuracy: 0.0198016 \t88\n",
      "Accuracy: 0.0198006 \t89\n",
      "Accuracy: 0.0197997 \t90\n",
      "Accuracy: 0.0197988 \t91\n",
      "Accuracy: 0.0197979 \t92\n",
      "Accuracy: 0.019797 \t93\n",
      "Accuracy: 0.0197961 \t94\n",
      "Accuracy: 0.0197952 \t95\n",
      "Accuracy: 0.0197942 \t96\n",
      "Accuracy: 0.0197933 \t97\n",
      "Accuracy: 0.0197924 \t98\n",
      "Accuracy: 0.0197916 \t99\n",
      "Accuracy: 0.0197907 \t0\n",
      "Accuracy: 0.0197939 \t1\n",
      "Accuracy: 0.0197972 \t2\n",
      "Accuracy: 0.0198005 \t3\n",
      "Accuracy: 0.0198039 \t4\n",
      "Accuracy: 0.0198075 \t5\n",
      "Accuracy: 0.019811 \t6\n",
      "Accuracy: 0.0198146 \t7\n",
      "Accuracy: 0.0198182 \t8\n",
      "Accuracy: 0.0198217 \t9\n",
      "Accuracy: 0.0198252 \t10\n",
      "Accuracy: 0.0198288 \t11\n",
      "Accuracy: 0.0198323 \t12\n",
      "Accuracy: 0.0198359 \t13\n",
      "Accuracy: 0.0198395 \t14\n",
      "Accuracy: 0.019843 \t15\n",
      "Accuracy: 0.0198466 \t16\n",
      "Accuracy: 0.0198503 \t17\n",
      "Accuracy: 0.0198541 \t18\n",
      "Accuracy: 0.019858 \t19\n",
      "Accuracy: 0.0198619 \t20\n",
      "Accuracy: 0.0198658 \t21\n",
      "Accuracy: 0.0198697 \t22\n",
      "Accuracy: 0.0198736 \t23\n",
      "Accuracy: 0.0198776 \t24\n",
      "Accuracy: 0.0198815 \t25\n",
      "Accuracy: 0.0198854 \t26\n",
      "Accuracy: 0.0198893 \t27\n",
      "Accuracy: 0.0198932 \t28\n",
      "Accuracy: 0.0198972 \t29\n",
      "Accuracy: 0.0199011 \t30\n",
      "Accuracy: 0.0199051 \t31\n",
      "Accuracy: 0.0199092 \t32\n",
      "Accuracy: 0.0199134 \t33\n",
      "Accuracy: 0.0199175 \t34\n",
      "Accuracy: 0.0199217 \t35\n",
      "Accuracy: 0.0199258 \t36\n",
      "Accuracy: 0.0199299 \t37\n",
      "Accuracy: 0.019934 \t38\n",
      "Accuracy: 0.0199382 \t39\n",
      "Accuracy: 0.0199423 \t40\n",
      "Accuracy: 0.0199465 \t41\n",
      "Accuracy: 0.0199507 \t42\n",
      "Accuracy: 0.0199548 \t43\n",
      "Accuracy: 0.019959 \t44\n",
      "Accuracy: 0.0199632 \t45\n",
      "Accuracy: 0.0199673 \t46\n",
      "Accuracy: 0.0199715 \t47\n",
      "Accuracy: 0.0199756 \t48\n",
      "Accuracy: 0.0199797 \t49\n",
      "Accuracy: 0.0199839 \t50\n",
      "Accuracy: 0.019988 \t51\n",
      "Accuracy: 0.0199922 \t52\n",
      "Accuracy: 0.0199966 \t53\n",
      "Accuracy: 0.0200009 \t54\n",
      "Accuracy: 0.0200053 \t55\n",
      "Accuracy: 0.0200096 \t56\n",
      "Accuracy: 0.020014 \t57\n",
      "Accuracy: 0.0200184 \t58\n",
      "Accuracy: 0.0200228 \t59\n",
      "Accuracy: 0.0200272 \t60\n",
      "Accuracy: 0.0200317 \t61\n",
      "Accuracy: 0.0200362 \t62\n",
      "Accuracy: 0.0200407 \t63\n",
      "Accuracy: 0.0200453 \t64\n",
      "Accuracy: 0.0200501 \t65\n",
      "Accuracy: 0.0200549 \t66\n",
      "Accuracy: 0.0200597 \t67\n",
      "Accuracy: 0.0200645 \t68\n",
      "Accuracy: 0.0200693 \t69\n",
      "Accuracy: 0.0200741 \t70\n",
      "Accuracy: 0.020079 \t71\n",
      "Accuracy: 0.0200839 \t72\n",
      "Accuracy: 0.0200888 \t73\n",
      "Accuracy: 0.0200937 \t74\n",
      "Accuracy: 0.0200986 \t75\n",
      "Accuracy: 0.0201036 \t76\n",
      "Accuracy: 0.0201085 \t77\n",
      "Accuracy: 0.0201134 \t78\n",
      "Accuracy: 0.0201183 \t79\n",
      "Accuracy: 0.0201233 \t80\n",
      "Accuracy: 0.0201282 \t81\n",
      "Accuracy: 0.0201331 \t82\n",
      "Accuracy: 0.020138 \t83\n",
      "Accuracy: 0.0201429 \t84\n",
      "Accuracy: 0.0201478 \t85\n",
      "Accuracy: 0.0201527 \t86\n",
      "Accuracy: 0.0201576 \t87\n",
      "Accuracy: 0.0201625 \t88\n",
      "Accuracy: 0.0201674 \t89\n",
      "Accuracy: 0.0201723 \t90\n",
      "Accuracy: 0.0201771 \t91\n",
      "Accuracy: 0.020182 \t92\n",
      "Accuracy: 0.0201869 \t93\n",
      "Accuracy: 0.0201917 \t94\n",
      "Accuracy: 0.0201966 \t95\n",
      "Accuracy: 0.0202014 \t96\n",
      "Accuracy: 0.0202063 \t97\n",
      "Accuracy: 0.0202111 \t98\n",
      "Accuracy: 0.0202159 \t99\n",
      "Accuracy: 0.0202208 \t0\n",
      "Accuracy: 0.0202238 \t1\n",
      "Accuracy: 0.0202267 \t2\n",
      "Accuracy: 0.0202295 \t3\n",
      "Accuracy: 0.0202322 \t4\n",
      "Accuracy: 0.0202348 \t5\n",
      "Accuracy: 0.0202373 \t6\n",
      "Accuracy: 0.0202397 \t7\n",
      "Accuracy: 0.0202421 \t8\n",
      "Accuracy: 0.0202444 \t9\n",
      "Accuracy: 0.0202465 \t10\n",
      "Accuracy: 0.0202486 \t11\n",
      "Accuracy: 0.0202506 \t12\n",
      "Accuracy: 0.0202526 \t13\n",
      "Accuracy: 0.0202544 \t14\n",
      "Accuracy: 0.0202561 \t15\n",
      "Accuracy: 0.0202578 \t16\n",
      "Accuracy: 0.0202593 \t17\n",
      "Accuracy: 0.0202608 \t18\n",
      "Accuracy: 0.0202622 \t19\n",
      "Accuracy: 0.0202635 \t20\n",
      "Accuracy: 0.0202647 \t21\n",
      "Accuracy: 0.0202658 \t22\n",
      "Accuracy: 0.0202668 \t23\n",
      "Accuracy: 0.0202677 \t24\n",
      "Accuracy: 0.0202686 \t25\n",
      "Accuracy: 0.0202693 \t26\n",
      "Accuracy: 0.02027 \t27\n",
      "Accuracy: 0.0202706 \t28\n",
      "Accuracy: 0.0202711 \t29\n",
      "Accuracy: 0.0202715 \t30\n",
      "Accuracy: 0.0202718 \t31\n",
      "Accuracy: 0.020272 \t32\n",
      "Accuracy: 0.0202722 \t33\n",
      "Accuracy: 0.0202723 \t34\n",
      "Accuracy: 0.0202722 \t35\n",
      "Accuracy: 0.0202721 \t36\n",
      "Accuracy: 0.0202719 \t37\n",
      "Accuracy: 0.0202716 \t38\n",
      "Accuracy: 0.0202713 \t39\n",
      "Accuracy: 0.0202708 \t40\n",
      "Accuracy: 0.0202703 \t41\n",
      "Accuracy: 0.0202696 \t42\n",
      "Accuracy: 0.0202689 \t43\n",
      "Accuracy: 0.020268 \t44\n",
      "Accuracy: 0.020267 \t45\n",
      "Accuracy: 0.0202659 \t46\n",
      "Accuracy: 0.0202648 \t47\n",
      "Accuracy: 0.0202635 \t48\n",
      "Accuracy: 0.0202622 \t49\n",
      "Accuracy: 0.0202607 \t50\n",
      "Accuracy: 0.0202592 \t51\n",
      "Accuracy: 0.0202576 \t52\n",
      "Accuracy: 0.0202559 \t53\n",
      "Accuracy: 0.0202541 \t54\n",
      "Accuracy: 0.0202523 \t55\n",
      "Accuracy: 0.0202503 \t56\n",
      "Accuracy: 0.0202483 \t57\n",
      "Accuracy: 0.0202461 \t58\n",
      "Accuracy: 0.0202439 \t59\n",
      "Accuracy: 0.0202416 \t60\n",
      "Accuracy: 0.0202392 \t61\n",
      "Accuracy: 0.0202367 \t62\n",
      "Accuracy: 0.0202342 \t63\n",
      "Accuracy: 0.0202315 \t64\n",
      "Accuracy: 0.0202287 \t65\n",
      "Accuracy: 0.0202259 \t66\n",
      "Accuracy: 0.020223 \t67\n",
      "Accuracy: 0.02022 \t68\n",
      "Accuracy: 0.0202168 \t69\n",
      "Accuracy: 0.0202136 \t70\n",
      "Accuracy: 0.0202103 \t71\n",
      "Accuracy: 0.0202069 \t72\n",
      "Accuracy: 0.0202034 \t73\n",
      "Accuracy: 0.0201998 \t74\n",
      "Accuracy: 0.0201961 \t75\n",
      "Accuracy: 0.0201924 \t76\n",
      "Accuracy: 0.0201885 \t77\n",
      "Accuracy: 0.0201846 \t78\n",
      "Accuracy: 0.0201805 \t79\n",
      "Accuracy: 0.0201764 \t80\n",
      "Accuracy: 0.0201722 \t81\n",
      "Accuracy: 0.0201679 \t82\n",
      "Accuracy: 0.0201635 \t83\n",
      "Accuracy: 0.020159 \t84\n",
      "Accuracy: 0.0201544 \t85\n",
      "Accuracy: 0.0201497 \t86\n",
      "Accuracy: 0.020145 \t87\n",
      "Accuracy: 0.0201402 \t88\n",
      "Accuracy: 0.0201352 \t89\n",
      "Accuracy: 0.0201302 \t90\n",
      "Accuracy: 0.0201251 \t91\n",
      "Accuracy: 0.02012 \t92\n",
      "Accuracy: 0.0201146 \t93\n",
      "Accuracy: 0.0201093 \t94\n",
      "Accuracy: 0.0201038 \t95\n",
      "Accuracy: 0.0200982 \t96\n",
      "Accuracy: 0.0200926 \t97\n",
      "Accuracy: 0.0200868 \t98\n",
      "Accuracy: 0.0200809 \t99\n",
      "Accuracy: 0.020075 \t0\n",
      "Accuracy: 0.0200753 \t1\n",
      "Accuracy: 0.0200757 \t2\n",
      "Accuracy: 0.020076 \t3\n",
      "Accuracy: 0.0200763 \t4\n",
      "Accuracy: 0.0200767 \t5\n",
      "Accuracy: 0.020077 \t6\n",
      "Accuracy: 0.0200773 \t7\n",
      "Accuracy: 0.0200776 \t8\n",
      "Accuracy: 0.0200779 \t9\n",
      "Accuracy: 0.0200782 \t10\n",
      "Accuracy: 0.0200784 \t11\n",
      "Accuracy: 0.0200787 \t12\n",
      "Accuracy: 0.020079 \t13\n",
      "Accuracy: 0.0200793 \t14\n",
      "Accuracy: 0.0200795 \t15\n",
      "Accuracy: 0.0200798 \t16\n",
      "Accuracy: 0.02008 \t17\n",
      "Accuracy: 0.0200803 \t18\n",
      "Accuracy: 0.0200805 \t19\n",
      "Accuracy: 0.0200807 \t20\n",
      "Accuracy: 0.0200809 \t21\n",
      "Accuracy: 0.0200812 \t22\n",
      "Accuracy: 0.0200814 \t23\n",
      "Accuracy: 0.0200816 \t24\n",
      "Accuracy: 0.0200818 \t25\n",
      "Accuracy: 0.020082 \t26\n",
      "Accuracy: 0.0200822 \t27\n",
      "Accuracy: 0.0200823 \t28\n",
      "Accuracy: 0.0200825 \t29\n",
      "Accuracy: 0.0200827 \t30\n",
      "Accuracy: 0.0200829 \t31\n",
      "Accuracy: 0.020083 \t32\n",
      "Accuracy: 0.0200832 \t33\n",
      "Accuracy: 0.0200833 \t34\n",
      "Accuracy: 0.0200835 \t35\n",
      "Accuracy: 0.0200836 \t36\n",
      "Accuracy: 0.0200837 \t37\n",
      "Accuracy: 0.0200839 \t38\n",
      "Accuracy: 0.020084 \t39\n",
      "Accuracy: 0.0200841 \t40\n",
      "Accuracy: 0.0200842 \t41\n",
      "Accuracy: 0.0200843 \t42\n",
      "Accuracy: 0.0200844 \t43\n",
      "Accuracy: 0.0200845 \t44\n",
      "Accuracy: 0.0200846 \t45\n",
      "Accuracy: 0.0200847 \t46\n",
      "Accuracy: 0.0200848 \t47\n",
      "Accuracy: 0.0200848 \t48\n",
      "Accuracy: 0.0200849 \t49\n",
      "Accuracy: 0.020085 \t50\n",
      "Accuracy: 0.020085 \t51\n",
      "Accuracy: 0.0200851 \t52\n",
      "Accuracy: 0.0200851 \t53\n",
      "Accuracy: 0.0200851 \t54\n",
      "Accuracy: 0.0200852 \t55\n",
      "Accuracy: 0.0200852 \t56\n",
      "Accuracy: 0.0200852 \t57\n",
      "Accuracy: 0.0200852 \t58\n",
      "Accuracy: 0.0200852 \t59\n",
      "Accuracy: 0.0200853 \t60\n",
      "Accuracy: 0.0200853 \t61\n",
      "Accuracy: 0.0200852 \t62\n",
      "Accuracy: 0.0200852 \t63\n",
      "Accuracy: 0.0200852 \t64\n",
      "Accuracy: 0.0200852 \t65\n",
      "Accuracy: 0.0200852 \t66\n",
      "Accuracy: 0.0200851 \t67\n",
      "Accuracy: 0.0200851 \t68\n",
      "Accuracy: 0.020085 \t69\n",
      "Accuracy: 0.020085 \t70\n",
      "Accuracy: 0.0200849 \t71\n",
      "Accuracy: 0.0200849 \t72\n",
      "Accuracy: 0.0200848 \t73\n",
      "Accuracy: 0.0200847 \t74\n",
      "Accuracy: 0.0200847 \t75\n",
      "Accuracy: 0.0200846 \t76\n",
      "Accuracy: 0.0200845 \t77\n",
      "Accuracy: 0.0200844 \t78\n",
      "Accuracy: 0.0200843 \t79\n",
      "Accuracy: 0.0200842 \t80\n",
      "Accuracy: 0.0200841 \t81\n",
      "Accuracy: 0.0200839 \t82\n",
      "Accuracy: 0.0200838 \t83\n",
      "Accuracy: 0.0200837 \t84\n",
      "Accuracy: 0.0200835 \t85\n",
      "Accuracy: 0.0200834 \t86\n",
      "Accuracy: 0.0200833 \t87\n",
      "Accuracy: 0.0200831 \t88\n",
      "Accuracy: 0.0200829 \t89\n",
      "Accuracy: 0.0200828 \t90\n",
      "Accuracy: 0.0200826 \t91\n",
      "Accuracy: 0.0200824 \t92\n",
      "Accuracy: 0.0200822 \t93\n",
      "Accuracy: 0.0200821 \t94\n",
      "Accuracy: 0.0200819 \t95\n",
      "Accuracy: 0.0200817 \t96\n",
      "Accuracy: 0.0200815 \t97\n",
      "Accuracy: 0.0200813 \t98\n",
      "Accuracy: 0.020081 \t99\n",
      "Accuracy: 0.0200808 \t0\n",
      "Accuracy: 0.0203569 \t1\n",
      "Accuracy: 0.0206356 \t2\n",
      "Accuracy: 0.0209164 \t3\n",
      "Accuracy: 0.0211997 \t4\n",
      "Accuracy: 0.0214849 \t5\n",
      "Accuracy: 0.0217685 \t6\n",
      "Accuracy: 0.0220339 \t7\n",
      "Accuracy: 0.0222709 \t8\n",
      "Accuracy: 0.0224911 \t9\n",
      "Accuracy: 0.022706 \t10\n",
      "Accuracy: 0.0229052 \t11\n",
      "Accuracy: 0.0230859 \t12\n",
      "Accuracy: 0.0232484 \t13\n",
      "Accuracy: 0.0233763 \t14\n",
      "Accuracy: 0.0234959 \t15\n",
      "Accuracy: 0.0236023 \t16\n",
      "Accuracy: 0.023705 \t17\n",
      "Accuracy: 0.0238066 \t18\n",
      "Accuracy: 0.0238928 \t19\n",
      "Accuracy: 0.0239619 \t20\n",
      "Accuracy: 0.0240316 \t21\n",
      "Accuracy: 0.0240893 \t22\n",
      "Accuracy: 0.024125 \t23\n",
      "Accuracy: 0.0241582 \t24\n",
      "Accuracy: 0.0241862 \t25\n",
      "Accuracy: 0.024201 \t26\n",
      "Accuracy: 0.0242114 \t27\n",
      "Accuracy: 0.0242118 \t28\n",
      "Accuracy: 0.0242116 \t29\n",
      "Accuracy: 0.0242131 \t30\n",
      "Accuracy: 0.0242146 \t31\n",
      "Accuracy: 0.0241987 \t32\n",
      "Accuracy: 0.024165 \t33\n",
      "Accuracy: 0.0241322 \t34\n",
      "Accuracy: 0.0240934 \t35\n",
      "Accuracy: 0.0240355 \t36\n",
      "Accuracy: 0.0239618 \t37\n",
      "Accuracy: 0.0238864 \t38\n",
      "Accuracy: 0.0238169 \t39\n",
      "Accuracy: 0.023738 \t40\n",
      "Accuracy: 0.0236492 \t41\n",
      "Accuracy: 0.0235559 \t42\n",
      "Accuracy: 0.0234521 \t43\n",
      "Accuracy: 0.0233298 \t44\n",
      "Accuracy: 0.0231992 \t45\n",
      "Accuracy: 0.0230626 \t46\n",
      "Accuracy: 0.0229212 \t47\n",
      "Accuracy: 0.0227835 \t48\n",
      "Accuracy: 0.0226523 \t49\n",
      "Accuracy: 0.0225167 \t50\n",
      "Accuracy: 0.0223724 \t51\n",
      "Accuracy: 0.0222229 \t52\n",
      "Accuracy: 0.0220651 \t53\n",
      "Accuracy: 0.0219167 \t54\n",
      "Accuracy: 0.0217686 \t55\n",
      "Accuracy: 0.0216151 \t56\n",
      "Accuracy: 0.0214671 \t57\n",
      "Accuracy: 0.021325 \t58\n",
      "Accuracy: 0.0211846 \t59\n",
      "Accuracy: 0.0210485 \t60\n",
      "Accuracy: 0.0209184 \t61\n",
      "Accuracy: 0.0207854 \t62\n",
      "Accuracy: 0.0206561 \t63\n",
      "Accuracy: 0.0205331 \t64\n",
      "Accuracy: 0.0204177 \t65\n",
      "Accuracy: 0.020314 \t66\n",
      "Accuracy: 0.0202136 \t67\n",
      "Accuracy: 0.0201161 \t68\n",
      "Accuracy: 0.0200194 \t69\n",
      "Accuracy: 0.0199337 \t70\n",
      "Accuracy: 0.0198545 \t71\n",
      "Accuracy: 0.0197815 \t72\n",
      "Accuracy: 0.0197047 \t73\n",
      "Accuracy: 0.0196339 \t74\n",
      "Accuracy: 0.0195597 \t75\n",
      "Accuracy: 0.0194849 \t76\n",
      "Accuracy: 0.0194102 \t77\n",
      "Accuracy: 0.0193459 \t78\n",
      "Accuracy: 0.0192963 \t79\n",
      "Accuracy: 0.0192578 \t80\n",
      "Accuracy: 0.0192233 \t81\n",
      "Accuracy: 0.0191901 \t82\n",
      "Accuracy: 0.0191636 \t83\n",
      "Accuracy: 0.0191395 \t84\n",
      "Accuracy: 0.0191188 \t85\n",
      "Accuracy: 0.0191088 \t86\n",
      "Accuracy: 0.0191169 \t87\n",
      "Accuracy: 0.0191236 \t88\n",
      "Accuracy: 0.0191427 \t89\n",
      "Accuracy: 0.0191686 \t90\n",
      "Accuracy: 0.0192007 \t91\n",
      "Accuracy: 0.0192401 \t92\n",
      "Accuracy: 0.0192949 \t93\n",
      "Accuracy: 0.0193592 \t94\n",
      "Accuracy: 0.0194373 \t95\n",
      "Accuracy: 0.0195292 \t96\n",
      "Accuracy: 0.0196291 \t97\n",
      "Accuracy: 0.0197447 \t98\n",
      "Accuracy: 0.0198762 \t99\n",
      "Accuracy: 0.0200153 \t0\n",
      "Accuracy: 0.0207024 \t1\n",
      "Accuracy: 0.0213059 \t2\n",
      "Accuracy: 0.0217698 \t3\n",
      "Accuracy: 0.0220383 \t4\n",
      "Accuracy: 0.0222046 \t5\n",
      "Accuracy: 0.0223598 \t6\n",
      "Accuracy: 0.0225158 \t7\n",
      "Accuracy: 0.0226773 \t8\n",
      "Accuracy: 0.0228451 \t9\n",
      "Accuracy: 0.0230167 \t10\n",
      "Accuracy: 0.0231897 \t11\n",
      "Accuracy: 0.0233637 \t12\n",
      "Accuracy: 0.0235379 \t13\n",
      "Accuracy: 0.0237143 \t14\n",
      "Accuracy: 0.0238869 \t15\n",
      "Accuracy: 0.0240567 \t16\n",
      "Accuracy: 0.0242232 \t17\n",
      "Accuracy: 0.0243852 \t18\n",
      "Accuracy: 0.0245452 \t19\n",
      "Accuracy: 0.0247036 \t20\n",
      "Accuracy: 0.0248531 \t21\n",
      "Accuracy: 0.0249961 \t22\n",
      "Accuracy: 0.0251396 \t23\n",
      "Accuracy: 0.0252832 \t24\n",
      "Accuracy: 0.0254231 \t25\n",
      "Accuracy: 0.025559 \t26\n",
      "Accuracy: 0.0256893 \t27\n",
      "Accuracy: 0.0258134 \t28\n",
      "Accuracy: 0.0259358 \t29\n",
      "Accuracy: 0.026053 \t30\n",
      "Accuracy: 0.0261646 \t31\n",
      "Accuracy: 0.026273 \t32\n",
      "Accuracy: 0.0263786 \t33\n",
      "Accuracy: 0.0264854 \t34\n",
      "Accuracy: 0.0265961 \t35\n",
      "Accuracy: 0.0267127 \t36\n",
      "Accuracy: 0.0268361 \t37\n",
      "Accuracy: 0.0269602 \t38\n",
      "Accuracy: 0.0270798 \t39\n",
      "Accuracy: 0.0271998 \t40\n",
      "Accuracy: 0.0273172 \t41\n",
      "Accuracy: 0.0274336 \t42\n",
      "Accuracy: 0.0275477 \t43\n",
      "Accuracy: 0.0276664 \t44\n",
      "Accuracy: 0.0277849 \t45\n",
      "Accuracy: 0.0278976 \t46\n",
      "Accuracy: 0.02801 \t47\n",
      "Accuracy: 0.0281245 \t48\n",
      "Accuracy: 0.0282444 \t49\n",
      "Accuracy: 0.0283646 \t50\n",
      "Accuracy: 0.0284921 \t51\n",
      "Accuracy: 0.0286216 \t52\n",
      "Accuracy: 0.0287513 \t53\n",
      "Accuracy: 0.0288851 \t54\n",
      "Accuracy: 0.0290199 \t55\n",
      "Accuracy: 0.0291578 \t56\n",
      "Accuracy: 0.0292997 \t57\n",
      "Accuracy: 0.0294495 \t58\n",
      "Accuracy: 0.0296061 \t59\n",
      "Accuracy: 0.0297685 \t60\n",
      "Accuracy: 0.0299371 \t61\n",
      "Accuracy: 0.0301057 \t62\n",
      "Accuracy: 0.0302801 \t63\n",
      "Accuracy: 0.0304595 \t64\n",
      "Accuracy: 0.0306355 \t65\n",
      "Accuracy: 0.0308164 \t66\n",
      "Accuracy: 0.031005 \t67\n",
      "Accuracy: 0.0311976 \t68\n",
      "Accuracy: 0.031395 \t69\n",
      "Accuracy: 0.0315953 \t70\n",
      "Accuracy: 0.0318017 \t71\n",
      "Accuracy: 0.0320168 \t72\n",
      "Accuracy: 0.0322377 \t73\n",
      "Accuracy: 0.0324648 \t74\n",
      "Accuracy: 0.0326934 \t75\n",
      "Accuracy: 0.0329222 \t76\n",
      "Accuracy: 0.0331553 \t77\n",
      "Accuracy: 0.0333987 \t78\n",
      "Accuracy: 0.0336513 \t79\n",
      "Accuracy: 0.0339044 \t80\n",
      "Accuracy: 0.0341596 \t81\n",
      "Accuracy: 0.0344184 \t82\n",
      "Accuracy: 0.0346755 \t83\n",
      "Accuracy: 0.0349338 \t84\n",
      "Accuracy: 0.0351925 \t85\n",
      "Accuracy: 0.0354541 \t86\n",
      "Accuracy: 0.0357217 \t87\n",
      "Accuracy: 0.0359995 \t88\n",
      "Accuracy: 0.0362857 \t89\n",
      "Accuracy: 0.0365801 \t90\n",
      "Accuracy: 0.0368773 \t91\n",
      "Accuracy: 0.0371775 \t92\n",
      "Accuracy: 0.0374828 \t93\n",
      "Accuracy: 0.0377883 \t94\n",
      "Accuracy: 0.0380957 \t95\n",
      "Accuracy: 0.03841 \t96\n",
      "Accuracy: 0.0387152 \t97\n",
      "Accuracy: 0.039016 \t98\n",
      "Accuracy: 0.0393312 \t99\n"
     ]
    }
   ],
   "source": [
    "#for b in xrange(len(test.AllBeads)-1):\n",
    "#    e = InterpBeadError(test.AllBeads[b][0],test.AllBeads[b][1], test.AllBeads[b+1][0], test.AllBeads[b+1][1])\n",
    "#for b in xrange(len(test.AllBeads)):\n",
    "#    print test.AllBeads[b][0]\n",
    "\n",
    "xdat,ydat = generatecandidate4(.5, .25, .1, 1000)\n",
    "xdat = np.array(xdat)\n",
    "ydat = np.array(ydat)\n",
    "\n",
    "def InterpBeadError_SameSet(w1,b1, w2,b2,xdat, ydat, write = False, name = \"00\"):\n",
    "    errors = []\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for tt in xrange(100):\n",
    "        #print tt\n",
    "        #accuracy = 0.\n",
    "        t = tt/100.\n",
    "        thiserror = 0\n",
    "\n",
    "        #x0 = tf.placeholder(\"float\", [None, n_input])\n",
    "        #y0 = tf.placeholder(\"float\", [None, n_classes])\n",
    "        weights, biases = model_interpolate(w1,b1,w2,b2, t)\n",
    "        interp_model = multilayer_perceptron(w=weights, b=biases)\n",
    "        \n",
    "        with interp_model.g.as_default():\n",
    "            \n",
    "            #interp_model.UpdateWeights(weights, biases)\n",
    "\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, n_input])\n",
    "            y = tf.placeholder(\"float\", [None, n_classes])\n",
    "            pred = interp_model.predict(x)\n",
    "            init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                correct_prediction = tf.reduce_mean(tf.square(pred-y))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print \"Accuracy:\", accuracy.eval({x: xdat, y: ydat}),\"\\t\",tt\n",
    "                thiserror = accuracy.eval({x: xdat, y: ydat})\n",
    "\n",
    "\n",
    "        errors.append(thiserror)\n",
    "\n",
    "    if write == True:\n",
    "        with open(\"f\" + str(name) + \".out\",'w+') as f:\n",
    "            for e in errors:\n",
    "                f.write(str(e) + \"\\n\")\n",
    "    \n",
    "    return max(errors), np.argmax(errors)\n",
    "\n",
    "\n",
    "for b in xrange(len(test.AllBeads)-1):\n",
    "    e = InterpBeadError_SameSet(test.AllBeads[b][0],test.AllBeads[b][1], test.AllBeads[b+1][0], test.AllBeads[b+1][1], xdat, ydat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
